{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from data import *\n",
    "from minibatch import *\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "# for auto-reloading external modules\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of images in dataset5:  65774\n"
     ]
    }
   ],
   "source": [
    "dataset = get_dataset(type=\"64\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data size:  60040\n",
      "Train label size:  60040\n",
      "Validation data size:  2500\n",
      "Validation label size:  2500\n",
      "Test data size:  3234\n",
      "Test label size:  3234\n"
     ]
    }
   ],
   "source": [
    "train_data, train_label_onehot, validation_data, validation_label_onehot, test_data, test_label_onehot \\\n",
    "    = split_train_validation_test(dataset)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_validation, y_validation = get_data(validation_data, validation_label_onehot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train, y_train = get_data(train_data, train_label_onehot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def shuffle_in_unison(a, b):\n",
    "    rng_state = np.random.get_state()\n",
    "    np.random.shuffle(a)\n",
    "    np.random.set_state(rng_state)\n",
    "    np.random.shuffle(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "470\n"
     ]
    }
   ],
   "source": [
    "def get_mini_batches(data, label, batch_size = 128):\n",
    "    shuffle_in_unison(data, label)\n",
    "    n = len(data)\n",
    "    mini_batches = [(data[k:k+batch_size], label[k:k+batch_size]) for k in range(0, n, batch_size)]\n",
    "    return mini_batches\n",
    "\n",
    "mini_batches = get_mini_batches(X_train, y_train)\n",
    "print(len(mini_batches))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(WIDTH, HEIGHT)= (64, 64)\n"
     ]
    }
   ],
   "source": [
    "from minibatch import *\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "print(\"(WIDTH, HEIGHT)=\",(WIDTH,HEIGHT))\n",
    "C = 24\n",
    "\n",
    "X = tf.placeholder(tf.float32, [None, WIDTH, HEIGHT, 3])\n",
    "Y_ = tf.placeholder(tf.float32, [None, C])\n",
    "\n",
    "# variable learning rate\n",
    "learning_rate = tf.placeholder(tf.float32)\n",
    "\n",
    "D = WIDTH*HEIGHT*3 #120*120*3\n",
    "\n",
    "\n",
    "# Weights initialised with small random values\n",
    "\n",
    "W1 = tf.Variable(tf.zeros((D, C)))  # 784 = 28 * 28\n",
    "#W1 = tf.Variable(tf.zeros((D,C)))\n",
    "B1 = tf.Variable(tf.zeros([C]))\n",
    "\n",
    "XX = tf.reshape(X, [-1, D])\n",
    "\n",
    "Ylogits = tf.matmul(XX, W1) + B1\n",
    "\n",
    "\n",
    "# cross-entropy loss function (= -sum(Y_i * log(Yi)) ), normalised for batches of 100  images\n",
    "# TensorFlow provides the softmax_cross_entropy_with_logits function to avoid numerical stability\n",
    "# problems with log(0) which is NaN\n",
    "softmax = tf.nn.softmax_cross_entropy_with_logits(logits=Ylogits, labels=Y_)\n",
    "cross_entropy = tf.reduce_mean(softmax)\n",
    "\n",
    "gamma = 0.5\n",
    "reg = gamma*tf.nn.l2_loss(W1)\n",
    "\n",
    "\n",
    "total_loss = cross_entropy + reg\n",
    "\n",
    "# accuracy of the trained model, between 0 (worst) and 1 (best)\n",
    "correct_prediction = tf.equal(tf.argmax(Ylogits, 1), tf.argmax(Y_, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "# training step, the learning rate is a placeholder\n",
    "train_step = tf.train.AdamOptimizer(learning_rate = learning_rate).minimize(total_loss)\n",
    "\n",
    "saver =tf.train.Saver()\n",
    "\n",
    "#Initialisation\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check for loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.17807\n",
      "check =  3.17805383035\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(init)   \n",
    "    \n",
    "    #check initial loss\n",
    "    cross_entropy_loss =  sess.run(cross_entropy, feed_dict={X: X_validation, Y_: y_validation})\n",
    "    print(cross_entropy_loss)\n",
    "    \n",
    "print(\"check = \", -np.log(1/24))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import time\n",
    "BATCH_SIZE = 128\n",
    "EPOCHS = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 :  learning rate =  0.029699999999999997\n",
      "--------------------------\n",
      "0 :  cross entropy loss =  1550.29 ; reg =  65.0372\n",
      "100 :  cross entropy loss =  1347.35 ; reg =  763.609\n",
      "200 :  cross entropy loss =  1366.51 ; reg =  852.551\n",
      "300 :  cross entropy loss =  1793.46 ; reg =  876.357\n",
      "400 :  cross entropy loss =  1146.48 ; reg =  830.008\n",
      "0 : train_accuracy:  0.4748 ; validation_accuracy:  0.4968\n",
      "1 :  learning rate =  0.029403\n",
      "--------------------------\n",
      "0 :  cross entropy loss =  2497.04 ; reg =  874.136\n",
      "100 :  cross entropy loss =  1542.17 ; reg =  872.065\n",
      "200 :  cross entropy loss =  1831.58 ; reg =  861.999\n",
      "300 :  cross entropy loss =  1222.23 ; reg =  862.628\n",
      "400 :  cross entropy loss =  1385.35 ; reg =  904.383\n",
      "1 : train_accuracy:  0.5116 ; validation_accuracy:  0.5348\n",
      "2 :  learning rate =  0.029108969999999998\n",
      "--------------------------\n",
      "0 :  cross entropy loss =  2205.33 ; reg =  861.474\n",
      "100 :  cross entropy loss =  2173.63 ; reg =  919.343\n",
      "200 :  cross entropy loss =  1757.84 ; reg =  887.938\n",
      "300 :  cross entropy loss =  1203.72 ; reg =  789.299\n",
      "400 :  cross entropy loss =  1644.41 ; reg =  861.863\n",
      "2 : train_accuracy:  0.4266 ; validation_accuracy:  0.454\n",
      "3 :  learning rate =  0.028817880299999998\n",
      "--------------------------\n",
      "0 :  cross entropy loss =  2711.43 ; reg =  909.79\n",
      "100 :  cross entropy loss =  1457.46 ; reg =  900.978\n",
      "200 :  cross entropy loss =  1332.07 ; reg =  803.415\n",
      "300 :  cross entropy loss =  1252.91 ; reg =  867.904\n",
      "400 :  cross entropy loss =  1320.5 ; reg =  838.209\n",
      "3 : train_accuracy:  0.4028 ; validation_accuracy:  0.408\n",
      "4 :  learning rate =  0.028529701496999998\n",
      "--------------------------\n",
      "0 :  cross entropy loss =  2848.05 ; reg =  817.358\n",
      "100 :  cross entropy loss =  1375.87 ; reg =  835.921\n",
      "200 :  cross entropy loss =  1273.69 ; reg =  836.94\n",
      "300 :  cross entropy loss =  1388.23 ; reg =  865.22\n",
      "400 :  cross entropy loss =  1463.04 ; reg =  798.427\n",
      "4 : train_accuracy:  0.4076 ; validation_accuracy:  0.4228\n",
      "5 :  learning rate =  0.028244404482029997\n",
      "--------------------------\n",
      "0 :  cross entropy loss =  2719.38 ; reg =  794.844\n",
      "100 :  cross entropy loss =  1601.76 ; reg =  841.879\n",
      "200 :  cross entropy loss =  1383.73 ; reg =  825.202\n",
      "300 :  cross entropy loss =  1516.21 ; reg =  834.097\n",
      "400 :  cross entropy loss =  1345.61 ; reg =  849.363\n",
      "5 : train_accuracy:  0.441 ; validation_accuracy:  0.4492\n",
      "6 :  learning rate =  0.027961960437209697\n",
      "--------------------------\n",
      "0 :  cross entropy loss =  2329.6 ; reg =  822.848\n",
      "100 :  cross entropy loss =  1061.84 ; reg =  822.147\n",
      "200 :  cross entropy loss =  1378.35 ; reg =  813.299\n",
      "300 :  cross entropy loss =  1758.66 ; reg =  781.43\n",
      "400 :  cross entropy loss =  1484.93 ; reg =  805.661\n",
      "6 : train_accuracy:  0.4818 ; validation_accuracy:  0.4976\n",
      "7 :  learning rate =  0.0276823408328376\n",
      "--------------------------\n",
      "0 :  cross entropy loss =  1996.53 ; reg =  821.368\n",
      "100 :  cross entropy loss =  1577.88 ; reg =  833.175\n",
      "200 :  cross entropy loss =  1101.14 ; reg =  816.019\n",
      "300 :  cross entropy loss =  2012.97 ; reg =  840.112\n",
      "400 :  cross entropy loss =  1908.76 ; reg =  818.379\n",
      "7 : train_accuracy:  0.4294 ; validation_accuracy:  0.3984\n",
      "8 :  learning rate =  0.027405517424509224\n",
      "--------------------------\n",
      "0 :  cross entropy loss =  2216.54 ; reg =  815.735\n",
      "100 :  cross entropy loss =  1835.42 ; reg =  815.632\n",
      "200 :  cross entropy loss =  1601.65 ; reg =  781.856\n",
      "300 :  cross entropy loss =  1247.98 ; reg =  761.114\n",
      "400 :  cross entropy loss =  1530.0 ; reg =  813.63\n",
      "8 : train_accuracy:  0.4258 ; validation_accuracy:  0.4188\n",
      "9 :  learning rate =  0.02713146225026413\n",
      "--------------------------\n",
      "0 :  cross entropy loss =  2811.54 ; reg =  825.768\n",
      "100 :  cross entropy loss =  1619.91 ; reg =  784.16\n",
      "200 :  cross entropy loss =  1419.59 ; reg =  777.103\n",
      "300 :  cross entropy loss =  2039.96 ; reg =  813.356\n",
      "400 :  cross entropy loss =  1155.32 ; reg =  806.407\n",
      "9 : train_accuracy:  0.3774 ; validation_accuracy:  0.3832\n",
      "10 :  learning rate =  0.02686014762776149\n",
      "--------------------------\n",
      "0 :  cross entropy loss =  4114.28 ; reg =  782.03\n",
      "100 :  cross entropy loss =  1990.4 ; reg =  805.53\n",
      "200 :  cross entropy loss =  1440.13 ; reg =  808.918\n",
      "300 :  cross entropy loss =  2014.63 ; reg =  782.388\n",
      "400 :  cross entropy loss =  1650.51 ; reg =  774.78\n",
      "10 : train_accuracy:  0.5162 ; validation_accuracy:  0.5312\n",
      "11 :  learning rate =  0.026591546151483875\n",
      "--------------------------\n",
      "0 :  cross entropy loss =  2479.69 ; reg =  788.932\n",
      "100 :  cross entropy loss =  1364.01 ; reg =  770.336\n",
      "200 :  cross entropy loss =  1346.43 ; reg =  769.472\n",
      "300 :  cross entropy loss =  1074.46 ; reg =  729.504\n",
      "400 :  cross entropy loss =  954.116 ; reg =  785.644\n",
      "11 : train_accuracy:  0.4896 ; validation_accuracy:  0.4784\n",
      "12 :  learning rate =  0.026325630689969036\n",
      "--------------------------\n",
      "0 :  cross entropy loss =  2153.24 ; reg =  780.021\n",
      "100 :  cross entropy loss =  1492.47 ; reg =  770.699\n",
      "200 :  cross entropy loss =  987.657 ; reg =  768.086\n",
      "300 :  cross entropy loss =  1270.44 ; reg =  744.205\n",
      "400 :  cross entropy loss =  1323.67 ; reg =  716.048\n",
      "12 : train_accuracy:  0.459 ; validation_accuracy:  0.4748\n",
      "13 :  learning rate =  0.026062374383069346\n",
      "--------------------------\n",
      "0 :  cross entropy loss =  1946.34 ; reg =  773.958\n",
      "100 :  cross entropy loss =  1234.28 ; reg =  795.869\n",
      "200 :  cross entropy loss =  1015.52 ; reg =  713.546\n",
      "300 :  cross entropy loss =  1213.49 ; reg =  717.494\n",
      "400 :  cross entropy loss =  1393.88 ; reg =  784.786\n",
      "13 : train_accuracy:  0.4904 ; validation_accuracy:  0.4868\n",
      "14 :  learning rate =  0.025801750639238653\n",
      "--------------------------\n",
      "0 :  cross entropy loss =  2407.71 ; reg =  797.323\n",
      "100 :  cross entropy loss =  1470.17 ; reg =  757.951\n",
      "200 :  cross entropy loss =  1891.58 ; reg =  745.753\n",
      "300 :  cross entropy loss =  976.353 ; reg =  712.849\n",
      "400 :  cross entropy loss =  1390.91 ; reg =  707.506\n",
      "14 : train_accuracy:  0.4562 ; validation_accuracy:  0.4648\n",
      "15 :  learning rate =  0.025543733132846268\n",
      "--------------------------\n",
      "0 :  cross entropy loss =  1794.82 ; reg =  750.98\n",
      "100 :  cross entropy loss =  1196.95 ; reg =  711.282\n",
      "200 :  cross entropy loss =  1633.26 ; reg =  693.963\n",
      "300 :  cross entropy loss =  2073.56 ; reg =  764.589\n",
      "400 :  cross entropy loss =  1419.99 ; reg =  742.065\n",
      "15 : train_accuracy:  0.4794 ; validation_accuracy:  0.498\n",
      "16 :  learning rate =  0.025288295801517806\n",
      "--------------------------\n",
      "0 :  cross entropy loss =  2203.88 ; reg =  716.787\n",
      "100 :  cross entropy loss =  1702.32 ; reg =  742.0\n",
      "200 :  cross entropy loss =  860.956 ; reg =  718.752\n",
      "300 :  cross entropy loss =  1022.39 ; reg =  720.542\n",
      "400 :  cross entropy loss =  1549.45 ; reg =  728.0\n",
      "16 : train_accuracy:  0.4818 ; validation_accuracy:  0.492\n",
      "17 :  learning rate =  0.02503541284350263\n",
      "--------------------------\n",
      "0 :  cross entropy loss =  1806.69 ; reg =  708.737\n",
      "100 :  cross entropy loss =  1668.66 ; reg =  732.562\n",
      "200 :  cross entropy loss =  875.106 ; reg =  676.698\n",
      "300 :  cross entropy loss =  1600.57 ; reg =  672.051\n",
      "400 :  cross entropy loss =  1244.76 ; reg =  744.287\n",
      "17 : train_accuracy:  0.4584 ; validation_accuracy:  0.454\n",
      "18 :  learning rate =  0.0247850587150676\n",
      "--------------------------\n",
      "0 :  cross entropy loss =  3300.93 ; reg =  771.518\n",
      "100 :  cross entropy loss =  1294.53 ; reg =  727.436\n",
      "200 :  cross entropy loss =  1271.0 ; reg =  714.679\n",
      "300 :  cross entropy loss =  1439.81 ; reg =  717.867\n",
      "400 :  cross entropy loss =  1457.71 ; reg =  728.291\n",
      "18 : train_accuracy:  0.476 ; validation_accuracy:  0.4908\n",
      "19 :  learning rate =  0.024537208127916925\n",
      "--------------------------\n",
      "0 :  cross entropy loss =  3000.43 ; reg =  705.223\n",
      "100 :  cross entropy loss =  1253.37 ; reg =  736.954\n",
      "200 :  cross entropy loss =  1418.17 ; reg =  717.688\n",
      "300 :  cross entropy loss =  1192.74 ; reg =  706.537\n",
      "400 :  cross entropy loss =  1359.89 ; reg =  685.085\n",
      "19 : train_accuracy:  0.4908 ; validation_accuracy:  0.5084\n",
      "20 :  learning rate =  0.024291836046637757\n",
      "--------------------------\n",
      "0 :  cross entropy loss =  1857.88 ; reg =  703.472\n",
      "100 :  cross entropy loss =  1303.75 ; reg =  707.349\n",
      "200 :  cross entropy loss =  1302.89 ; reg =  696.582\n",
      "300 :  cross entropy loss =  1224.44 ; reg =  723.238\n",
      "400 :  cross entropy loss =  1563.34 ; reg =  704.657\n",
      "20 : train_accuracy:  0.493 ; validation_accuracy:  0.5132\n",
      "21 :  learning rate =  0.02404891768617138\n",
      "--------------------------\n",
      "0 :  cross entropy loss =  1827.69 ; reg =  713.36\n",
      "100 :  cross entropy loss =  1188.04 ; reg =  706.703\n",
      "200 :  cross entropy loss =  1512.72 ; reg =  706.835\n",
      "300 :  cross entropy loss =  1418.69 ; reg =  679.594\n",
      "400 :  cross entropy loss =  1585.65 ; reg =  668.404\n",
      "21 : train_accuracy:  0.4756 ; validation_accuracy:  0.4904\n",
      "22 :  learning rate =  0.023808428509309667\n",
      "--------------------------\n",
      "0 :  cross entropy loss =  1990.45 ; reg =  673.713\n",
      "100 :  cross entropy loss =  1043.94 ; reg =  664.486\n",
      "200 :  cross entropy loss =  1411.84 ; reg =  654.214\n",
      "300 :  cross entropy loss =  1014.46 ; reg =  685.614\n",
      "400 :  cross entropy loss =  1182.51 ; reg =  667.694\n",
      "22 : train_accuracy:  0.5064 ; validation_accuracy:  0.5064\n",
      "23 :  learning rate =  0.02357034422421657\n",
      "--------------------------\n",
      "0 :  cross entropy loss =  1349.68 ; reg =  679.463\n",
      "100 :  cross entropy loss =  1487.14 ; reg =  676.609\n",
      "200 :  cross entropy loss =  1024.19 ; reg =  676.46\n",
      "300 :  cross entropy loss =  1463.36 ; reg =  673.043\n",
      "400 :  cross entropy loss =  1420.81 ; reg =  707.275\n",
      "23 : train_accuracy:  0.4762 ; validation_accuracy:  0.4888\n",
      "24 :  learning rate =  0.023334640781974402\n",
      "--------------------------\n",
      "0 :  cross entropy loss =  3083.75 ; reg =  714.0\n",
      "100 :  cross entropy loss =  1190.61 ; reg =  707.235\n",
      "200 :  cross entropy loss =  1068.13 ; reg =  648.443\n",
      "300 :  cross entropy loss =  1342.07 ; reg =  663.085\n",
      "400 :  cross entropy loss =  1227.23 ; reg =  623.326\n",
      "24 : train_accuracy:  0.505 ; validation_accuracy:  0.5116\n",
      "25 :  learning rate =  0.023101294374154657\n",
      "--------------------------\n",
      "0 :  cross entropy loss =  2360.19 ; reg =  687.135\n",
      "100 :  cross entropy loss =  1161.75 ; reg =  628.76\n",
      "200 :  cross entropy loss =  1199.15 ; reg =  682.505\n",
      "300 :  cross entropy loss =  1285.0 ; reg =  677.44\n",
      "400 :  cross entropy loss =  1326.41 ; reg =  673.031\n",
      "25 : train_accuracy:  0.4806 ; validation_accuracy:  0.4832\n",
      "26 :  learning rate =  0.02287028143041311\n",
      "--------------------------\n",
      "0 :  cross entropy loss =  2297.79 ; reg =  661.291\n",
      "100 :  cross entropy loss =  1213.08 ; reg =  659.068\n",
      "200 :  cross entropy loss =  1348.64 ; reg =  663.03\n",
      "300 :  cross entropy loss =  1316.97 ; reg =  660.869\n",
      "400 :  cross entropy loss =  1391.03 ; reg =  670.736\n",
      "26 : train_accuracy:  0.464 ; validation_accuracy:  0.468\n",
      "27 :  learning rate =  0.02264157861610898\n",
      "--------------------------\n",
      "0 :  cross entropy loss =  2512.92 ; reg =  657.023\n",
      "100 :  cross entropy loss =  1089.17 ; reg =  687.244\n",
      "200 :  cross entropy loss =  1351.85 ; reg =  627.962\n",
      "300 :  cross entropy loss =  1539.77 ; reg =  632.21\n",
      "400 :  cross entropy loss =  1207.96 ; reg =  622.176\n",
      "27 : train_accuracy:  0.501 ; validation_accuracy:  0.512\n",
      "28 :  learning rate =  0.02241516282994789\n",
      "--------------------------\n",
      "0 :  cross entropy loss =  1548.27 ; reg =  673.285\n",
      "100 :  cross entropy loss =  1160.63 ; reg =  661.932\n",
      "200 :  cross entropy loss =  1037.25 ; reg =  660.341\n",
      "300 :  cross entropy loss =  1191.2 ; reg =  632.307\n",
      "400 :  cross entropy loss =  1436.37 ; reg =  654.479\n",
      "28 : train_accuracy:  0.463 ; validation_accuracy:  0.4732\n",
      "29 :  learning rate =  0.02219101120164841\n",
      "--------------------------\n",
      "0 :  cross entropy loss =  1639.96 ; reg =  663.209\n",
      "100 :  cross entropy loss =  1085.08 ; reg =  649.896\n",
      "200 :  cross entropy loss =  971.327 ; reg =  624.801\n",
      "300 :  cross entropy loss =  975.211 ; reg =  605.907\n",
      "400 :  cross entropy loss =  1424.49 ; reg =  663.894\n",
      "29 : train_accuracy:  0.4528 ; validation_accuracy:  0.444\n",
      "30 :  learning rate =  0.021969101089631925\n",
      "--------------------------\n",
      "0 :  cross entropy loss =  2324.25 ; reg =  635.404\n",
      "100 :  cross entropy loss =  1060.8 ; reg =  666.179\n",
      "200 :  cross entropy loss =  957.842 ; reg =  637.035\n",
      "300 :  cross entropy loss =  1286.67 ; reg =  643.208\n",
      "400 :  cross entropy loss =  1124.16 ; reg =  593.364\n",
      "30 : train_accuracy:  0.4276 ; validation_accuracy:  0.4428\n",
      "31 :  learning rate =  0.021749410078735605\n",
      "--------------------------\n",
      "0 :  cross entropy loss =  1893.47 ; reg =  661.535\n",
      "100 :  cross entropy loss =  1167.33 ; reg =  652.046\n",
      "200 :  cross entropy loss =  1246.8 ; reg =  583.61\n",
      "300 :  cross entropy loss =  941.836 ; reg =  621.443\n",
      "400 :  cross entropy loss =  1214.38 ; reg =  624.085\n",
      "31 : train_accuracy:  0.4934 ; validation_accuracy:  0.4824\n",
      "32 :  learning rate =  0.02153191597794825\n",
      "--------------------------\n",
      "0 :  cross entropy loss =  1516.28 ; reg =  628.987\n",
      "100 :  cross entropy loss =  1072.09 ; reg =  605.453\n",
      "200 :  cross entropy loss =  1213.2 ; reg =  608.697\n",
      "300 :  cross entropy loss =  1019.05 ; reg =  639.066\n",
      "400 :  cross entropy loss =  751.08 ; reg =  596.579\n",
      "32 : train_accuracy:  0.4576 ; validation_accuracy:  0.4596\n",
      "33 :  learning rate =  0.021316596818168766\n",
      "--------------------------\n",
      "0 :  cross entropy loss =  2282.52 ; reg =  616.324\n",
      "100 :  cross entropy loss =  1000.0 ; reg =  641.332\n",
      "200 :  cross entropy loss =  1105.15 ; reg =  585.904\n",
      "300 :  cross entropy loss =  1145.04 ; reg =  581.912\n",
      "400 :  cross entropy loss =  831.76 ; reg =  592.682\n",
      "33 : train_accuracy:  0.4956 ; validation_accuracy:  0.502\n",
      "34 :  learning rate =  0.021103430849987077\n",
      "--------------------------\n",
      "0 :  cross entropy loss =  1525.61 ; reg =  590.919\n",
      "100 :  cross entropy loss =  1122.15 ; reg =  589.415\n",
      "200 :  cross entropy loss =  1352.77 ; reg =  608.155\n",
      "300 :  cross entropy loss =  978.255 ; reg =  577.163\n",
      "400 :  cross entropy loss =  1131.64 ; reg =  613.581\n",
      "34 : train_accuracy:  0.4434 ; validation_accuracy:  0.458\n",
      "35 :  learning rate =  0.020892396541487206\n",
      "--------------------------\n",
      "0 :  cross entropy loss =  1837.57 ; reg =  635.047\n",
      "100 :  cross entropy loss =  734.231 ; reg =  621.888\n",
      "200 :  cross entropy loss =  955.088 ; reg =  606.45\n",
      "300 :  cross entropy loss =  899.107 ; reg =  622.178\n",
      "400 :  cross entropy loss =  952.401 ; reg =  607.956\n",
      "35 : train_accuracy:  0.4338 ; validation_accuracy:  0.426\n",
      "36 :  learning rate =  0.020683472576072334\n",
      "--------------------------\n",
      "0 :  cross entropy loss =  2284.75 ; reg =  591.639\n",
      "100 :  cross entropy loss =  997.183 ; reg =  640.886\n",
      "200 :  cross entropy loss =  1064.74 ; reg =  599.381\n",
      "300 :  cross entropy loss =  936.807 ; reg =  620.919\n",
      "400 :  cross entropy loss =  900.984 ; reg =  564.046\n",
      "36 : train_accuracy:  0.508 ; validation_accuracy:  0.5272\n",
      "37 :  learning rate =  0.02047663785031161\n",
      "--------------------------\n",
      "0 :  cross entropy loss =  1279.82 ; reg =  577.093\n",
      "100 :  cross entropy loss =  1220.4 ; reg =  603.101\n",
      "200 :  cross entropy loss =  1056.19 ; reg =  593.052\n",
      "300 :  cross entropy loss =  1418.61 ; reg =  603.304\n",
      "400 :  cross entropy loss =  1431.2 ; reg =  603.002\n",
      "37 : train_accuracy:  0.5162 ; validation_accuracy:  0.5244\n",
      "38 :  learning rate =  0.020271871471808495\n",
      "--------------------------\n",
      "0 :  cross entropy loss =  1678.0 ; reg =  597.088\n",
      "100 :  cross entropy loss =  1393.97 ; reg =  594.29\n",
      "200 :  cross entropy loss =  1120.68 ; reg =  604.427\n",
      "300 :  cross entropy loss =  995.277 ; reg =  562.764\n",
      "400 :  cross entropy loss =  809.311 ; reg =  550.788\n",
      "38 : train_accuracy:  0.489 ; validation_accuracy:  0.5016\n",
      "39 :  learning rate =  0.02006915275709041\n",
      "--------------------------\n",
      "0 :  cross entropy loss =  1512.86 ; reg =  583.277\n",
      "100 :  cross entropy loss =  1176.38 ; reg =  587.671\n",
      "200 :  cross entropy loss =  725.472 ; reg =  524.248\n",
      "300 :  cross entropy loss =  948.203 ; reg =  552.441\n",
      "400 :  cross entropy loss =  999.589 ; reg =  531.348\n",
      "39 : train_accuracy:  0.5072 ; validation_accuracy:  0.5104\n",
      "40 :  learning rate =  0.019868461229519508\n",
      "--------------------------\n",
      "0 :  cross entropy loss =  1840.83 ; reg =  564.184\n",
      "100 :  cross entropy loss =  1176.1 ; reg =  570.636\n",
      "200 :  cross entropy loss =  1342.1 ; reg =  556.866\n",
      "300 :  cross entropy loss =  1081.82 ; reg =  554.919\n",
      "400 :  cross entropy loss =  966.386 ; reg =  558.844\n",
      "40 : train_accuracy:  0.5078 ; validation_accuracy:  0.5196\n",
      "41 :  learning rate =  0.019669776617224313\n",
      "--------------------------\n",
      "0 :  cross entropy loss =  1604.31 ; reg =  604.842\n",
      "100 :  cross entropy loss =  829.148 ; reg =  575.327\n",
      "200 :  cross entropy loss =  1518.85 ; reg =  545.817\n",
      "300 :  cross entropy loss =  997.859 ; reg =  570.221\n",
      "400 :  cross entropy loss =  1024.63 ; reg =  553.105\n",
      "41 : train_accuracy:  0.4862 ; validation_accuracy:  0.4884\n",
      "42 :  learning rate =  0.01947307885105207\n",
      "--------------------------\n",
      "0 :  cross entropy loss =  1363.95 ; reg =  555.965\n",
      "100 :  cross entropy loss =  1094.57 ; reg =  583.677\n",
      "200 :  cross entropy loss =  813.334 ; reg =  579.65\n",
      "300 :  cross entropy loss =  1103.75 ; reg =  528.426\n",
      "400 :  cross entropy loss =  1189.81 ; reg =  548.421\n",
      "42 : train_accuracy:  0.5208 ; validation_accuracy:  0.5216\n",
      "43 :  learning rate =  0.01927834806254155\n",
      "--------------------------\n",
      "0 :  cross entropy loss =  1421.4 ; reg =  536.161\n",
      "100 :  cross entropy loss =  944.271 ; reg =  548.373\n",
      "200 :  cross entropy loss =  1347.83 ; reg =  548.826\n",
      "300 :  cross entropy loss =  755.599 ; reg =  561.935\n",
      "400 :  cross entropy loss =  1028.24 ; reg =  559.749\n",
      "43 : train_accuracy:  0.4864 ; validation_accuracy:  0.4884\n",
      "44 :  learning rate =  0.019085564581916133\n",
      "--------------------------\n",
      "0 :  cross entropy loss =  1438.74 ; reg =  560.976\n",
      "100 :  cross entropy loss =  868.063 ; reg =  540.599\n",
      "200 :  cross entropy loss =  1123.77 ; reg =  552.743\n",
      "300 :  cross entropy loss =  846.011 ; reg =  510.588\n",
      "400 :  cross entropy loss =  1060.03 ; reg =  554.017\n",
      "44 : train_accuracy:  0.4972 ; validation_accuracy:  0.5112\n",
      "45 :  learning rate =  0.01889470893609697\n",
      "--------------------------\n",
      "0 :  cross entropy loss =  1665.63 ; reg =  515.604\n",
      "100 :  cross entropy loss =  849.388 ; reg =  576.59\n",
      "200 :  cross entropy loss =  965.637 ; reg =  531.975\n",
      "300 :  cross entropy loss =  684.174 ; reg =  556.694\n",
      "400 :  cross entropy loss =  764.838 ; reg =  521.718\n",
      "45 : train_accuracy:  0.4732 ; validation_accuracy:  0.4688\n",
      "46 :  learning rate =  0.018705761846736002\n",
      "--------------------------\n",
      "0 :  cross entropy loss =  1835.2 ; reg =  533.817\n",
      "100 :  cross entropy loss =  795.267 ; reg =  546.632\n",
      "200 :  cross entropy loss =  1038.91 ; reg =  530.066\n",
      "300 :  cross entropy loss =  882.037 ; reg =  519.087\n",
      "400 :  cross entropy loss =  916.536 ; reg =  542.518\n",
      "46 : train_accuracy:  0.5022 ; validation_accuracy:  0.4896\n",
      "47 :  learning rate =  0.01851870422826864\n",
      "--------------------------\n",
      "0 :  cross entropy loss =  1825.23 ; reg =  508.248\n",
      "100 :  cross entropy loss =  1111.48 ; reg =  519.01\n",
      "200 :  cross entropy loss =  1100.52 ; reg =  514.22\n",
      "300 :  cross entropy loss =  809.809 ; reg =  511.008\n",
      "400 :  cross entropy loss =  1176.28 ; reg =  514.409\n",
      "47 : train_accuracy:  0.4642 ; validation_accuracy:  0.4688\n",
      "48 :  learning rate =  0.018333517185985953\n",
      "--------------------------\n",
      "0 :  cross entropy loss =  2314.61 ; reg =  521.709\n",
      "100 :  cross entropy loss =  1009.79 ; reg =  544.26\n",
      "200 :  cross entropy loss =  686.208 ; reg =  492.063\n",
      "300 :  cross entropy loss =  896.272 ; reg =  523.546\n",
      "400 :  cross entropy loss =  1194.97 ; reg =  536.464\n",
      "48 : train_accuracy:  0.4324 ; validation_accuracy:  0.4408\n",
      "49 :  learning rate =  0.018150182014126093\n",
      "--------------------------\n",
      "0 :  cross entropy loss =  1590.92 ; reg =  515.084\n",
      "100 :  cross entropy loss =  1430.02 ; reg =  528.84\n",
      "200 :  cross entropy loss =  978.546 ; reg =  489.406\n",
      "300 :  cross entropy loss =  998.392 ; reg =  516.111\n",
      "400 :  cross entropy loss =  803.361 ; reg =  498.13\n",
      "49 : train_accuracy:  0.4872 ; validation_accuracy:  0.494\n",
      "50 :  learning rate =  0.017968680193984832\n",
      "--------------------------\n",
      "0 :  cross entropy loss =  1094.96 ; reg =  512.722\n",
      "100 :  cross entropy loss =  1067.63 ; reg =  541.378\n",
      "200 :  cross entropy loss =  1051.18 ; reg =  518.604\n",
      "300 :  cross entropy loss =  798.392 ; reg =  494.973\n",
      "400 :  cross entropy loss =  745.132 ; reg =  514.449\n",
      "50 : train_accuracy:  0.5428 ; validation_accuracy:  0.55\n",
      "51 :  learning rate =  0.017788993392044983\n",
      "--------------------------\n",
      "0 :  cross entropy loss =  1493.55 ; reg =  491.98\n",
      "100 :  cross entropy loss =  834.526 ; reg =  511.757\n",
      "200 :  cross entropy loss =  1071.67 ; reg =  509.536\n",
      "300 :  cross entropy loss =  1235.41 ; reg =  524.178\n",
      "400 :  cross entropy loss =  918.307 ; reg =  498.804\n",
      "51 : train_accuracy:  0.4696 ; validation_accuracy:  0.4912\n",
      "52 :  learning rate =  0.017611103458124534\n",
      "--------------------------\n",
      "0 :  cross entropy loss =  1825.71 ; reg =  504.809\n",
      "100 :  cross entropy loss =  1157.84 ; reg =  505.857\n",
      "200 :  cross entropy loss =  935.781 ; reg =  478.457\n",
      "300 :  cross entropy loss =  1318.36 ; reg =  521.387\n",
      "400 :  cross entropy loss =  713.171 ; reg =  491.858\n",
      "52 : train_accuracy:  0.4906 ; validation_accuracy:  0.4908\n",
      "53 :  learning rate =  0.017434992423543287\n",
      "--------------------------\n",
      "0 :  cross entropy loss =  1374.57 ; reg =  483.008\n",
      "100 :  cross entropy loss =  744.811 ; reg =  499.654\n",
      "200 :  cross entropy loss =  910.486 ; reg =  487.024\n",
      "300 :  cross entropy loss =  1190.39 ; reg =  486.375\n",
      "400 :  cross entropy loss =  1233.7 ; reg =  486.543\n",
      "53 : train_accuracy:  0.496 ; validation_accuracy:  0.5076\n",
      "54 :  learning rate =  0.017260642499307855\n",
      "--------------------------\n",
      "0 :  cross entropy loss =  1644.19 ; reg =  491.795\n",
      "100 :  cross entropy loss =  1100.55 ; reg =  511.921\n",
      "200 :  cross entropy loss =  798.504 ; reg =  490.145\n",
      "300 :  cross entropy loss =  845.801 ; reg =  485.737\n",
      "400 :  cross entropy loss =  532.941 ; reg =  475.142\n",
      "54 : train_accuracy:  0.5214 ; validation_accuracy:  0.5256\n",
      "55 :  learning rate =  0.017088036074314777\n",
      "--------------------------\n",
      "0 :  cross entropy loss =  1663.76 ; reg =  484.367\n",
      "100 :  cross entropy loss =  697.017 ; reg =  481.868\n",
      "200 :  cross entropy loss =  935.105 ; reg =  453.237\n",
      "300 :  cross entropy loss =  1084.17 ; reg =  481.436\n",
      "400 :  cross entropy loss =  811.206 ; reg =  441.819\n",
      "55 : train_accuracy:  0.5554 ; validation_accuracy:  0.5608\n",
      "56 :  learning rate =  0.01691715571357163\n",
      "--------------------------\n",
      "0 :  cross entropy loss =  1044.95 ; reg =  474.796\n",
      "100 :  cross entropy loss =  1057.51 ; reg =  491.079\n",
      "200 :  cross entropy loss =  726.931 ; reg =  458.309\n",
      "300 :  cross entropy loss =  896.166 ; reg =  474.518\n",
      "400 :  cross entropy loss =  1069.33 ; reg =  489.076\n",
      "56 : train_accuracy:  0.5438 ; validation_accuracy:  0.5396\n",
      "57 :  learning rate =  0.016747984156435913\n",
      "--------------------------\n",
      "0 :  cross entropy loss =  881.04 ; reg =  486.734\n",
      "100 :  cross entropy loss =  1034.29 ; reg =  477.296\n",
      "200 :  cross entropy loss =  1321.64 ; reg =  488.764\n",
      "300 :  cross entropy loss =  656.961 ; reg =  453.629\n",
      "400 :  cross entropy loss =  854.272 ; reg =  466.709\n",
      "57 : train_accuracy:  0.4848 ; validation_accuracy:  0.4876\n",
      "58 :  learning rate =  0.016580504314871555\n",
      "--------------------------\n",
      "0 :  cross entropy loss =  2158.95 ; reg =  480.214\n",
      "100 :  cross entropy loss =  624.746 ; reg =  476.994\n",
      "200 :  cross entropy loss =  577.673 ; reg =  458.179\n",
      "300 :  cross entropy loss =  1015.73 ; reg =  448.9\n",
      "400 :  cross entropy loss =  753.717 ; reg =  453.691\n",
      "58 : train_accuracy:  0.4864 ; validation_accuracy:  0.4704\n",
      "59 :  learning rate =  0.01641469927172284\n",
      "--------------------------\n",
      "0 :  cross entropy loss =  1988.05 ; reg =  464.801\n",
      "100 :  cross entropy loss =  1126.23 ; reg =  466.489\n",
      "200 :  cross entropy loss =  1331.93 ; reg =  465.456\n",
      "300 :  cross entropy loss =  879.584 ; reg =  439.053\n",
      "400 :  cross entropy loss =  683.693 ; reg =  447.76\n",
      "59 : train_accuracy:  0.4874 ; validation_accuracy:  0.5004\n",
      "60 :  learning rate =  0.01625055227900561\n",
      "--------------------------\n",
      "0 :  cross entropy loss =  1826.71 ; reg =  459.96\n",
      "100 :  cross entropy loss =  660.858 ; reg =  494.041\n",
      "200 :  cross entropy loss =  731.98 ; reg =  443.883\n",
      "300 :  cross entropy loss =  1156.87 ; reg =  448.148\n",
      "400 :  cross entropy loss =  652.871 ; reg =  449.194\n",
      "60 : train_accuracy:  0.5158 ; validation_accuracy:  0.5168\n",
      "61 :  learning rate =  0.016088046756215557\n",
      "--------------------------\n",
      "0 :  cross entropy loss =  1116.04 ; reg =  448.566\n",
      "100 :  cross entropy loss =  882.735 ; reg =  475.115\n",
      "200 :  cross entropy loss =  726.38 ; reg =  449.451\n",
      "300 :  cross entropy loss =  1095.6 ; reg =  458.584\n",
      "400 :  cross entropy loss =  700.517 ; reg =  441.804\n",
      "61 : train_accuracy:  0.497 ; validation_accuracy:  0.4868\n",
      "62 :  learning rate =  0.015927166288653403\n",
      "--------------------------\n",
      "0 :  cross entropy loss =  1723.5 ; reg =  457.199\n",
      "100 :  cross entropy loss =  863.131 ; reg =  459.627\n",
      "200 :  cross entropy loss =  677.022 ; reg =  453.12\n",
      "300 :  cross entropy loss =  927.206 ; reg =  446.6\n",
      "400 :  cross entropy loss =  837.078 ; reg =  422.087\n",
      "62 : train_accuracy:  0.5724 ; validation_accuracy:  0.5744\n",
      "63 :  learning rate =  0.01576789462576687\n",
      "--------------------------\n",
      "0 :  cross entropy loss =  1217.71 ; reg =  456.194\n",
      "100 :  cross entropy loss =  953.695 ; reg =  462.292\n",
      "200 :  cross entropy loss =  849.112 ; reg =  431.965\n",
      "300 :  cross entropy loss =  747.78 ; reg =  426.913\n",
      "400 :  cross entropy loss =  939.309 ; reg =  429.767\n",
      "63 : train_accuracy:  0.475 ; validation_accuracy:  0.498\n",
      "64 :  learning rate =  0.0156102156795092\n",
      "--------------------------\n",
      "0 :  cross entropy loss =  1276.86 ; reg =  439.476\n",
      "100 :  cross entropy loss =  568.951 ; reg =  411.785\n",
      "200 :  cross entropy loss =  778.447 ; reg =  424.528\n",
      "300 :  cross entropy loss =  854.978 ; reg =  431.937\n",
      "400 :  cross entropy loss =  629.152 ; reg =  425.063\n",
      "64 : train_accuracy:  0.5274 ; validation_accuracy:  0.5236\n",
      "65 :  learning rate =  0.015454113522714108\n",
      "--------------------------\n",
      "0 :  cross entropy loss =  871.621 ; reg =  434.092\n",
      "100 :  cross entropy loss =  628.985 ; reg =  437.801\n",
      "200 :  cross entropy loss =  746.738 ; reg =  416.99\n",
      "300 :  cross entropy loss =  963.996 ; reg =  431.632\n",
      "400 :  cross entropy loss =  941.967 ; reg =  414.574\n",
      "65 : train_accuracy:  0.513 ; validation_accuracy:  0.51\n",
      "66 :  learning rate =  0.015299572387486967\n",
      "--------------------------\n",
      "0 :  cross entropy loss =  1123.42 ; reg =  441.732\n",
      "100 :  cross entropy loss =  1036.09 ; reg =  458.202\n",
      "200 :  cross entropy loss =  711.312 ; reg =  406.126\n",
      "300 :  cross entropy loss =  524.528 ; reg =  433.503\n",
      "400 :  cross entropy loss =  875.373 ; reg =  416.478\n",
      "66 : train_accuracy:  0.5174 ; validation_accuracy:  0.5168\n",
      "67 :  learning rate =  0.015146576663612098\n",
      "--------------------------\n",
      "0 :  cross entropy loss =  1600.93 ; reg =  439.302\n",
      "100 :  cross entropy loss =  1031.87 ; reg =  431.046\n",
      "200 :  cross entropy loss =  580.981 ; reg =  411.984\n",
      "300 :  cross entropy loss =  839.202 ; reg =  442.176\n",
      "400 :  cross entropy loss =  785.189 ; reg =  402.028\n",
      "67 : train_accuracy:  0.5138 ; validation_accuracy:  0.4936\n",
      "68 :  learning rate =  0.014995110896975977\n",
      "--------------------------\n",
      "0 :  cross entropy loss =  1230.21 ; reg =  418.055\n",
      "100 :  cross entropy loss =  1066.06 ; reg =  433.332\n",
      "200 :  cross entropy loss =  805.712 ; reg =  407.951\n",
      "300 :  cross entropy loss =  960.409 ; reg =  415.691\n",
      "400 :  cross entropy loss =  941.082 ; reg =  409.278\n",
      "68 : train_accuracy:  0.547 ; validation_accuracy:  0.5648\n",
      "69 :  learning rate =  0.014845159788006218\n",
      "--------------------------\n",
      "0 :  cross entropy loss =  1297.72 ; reg =  408.422\n",
      "100 :  cross entropy loss =  795.891 ; reg =  429.267\n",
      "200 :  cross entropy loss =  854.732 ; reg =  423.716\n",
      "300 :  cross entropy loss =  742.596 ; reg =  407.704\n",
      "400 :  cross entropy loss =  710.156 ; reg =  433.574\n",
      "69 : train_accuracy:  0.493 ; validation_accuracy:  0.51\n",
      "70 :  learning rate =  0.014696708190126155\n",
      "--------------------------\n",
      "0 :  cross entropy loss =  1360.26 ; reg =  436.047\n",
      "100 :  cross entropy loss =  785.491 ; reg =  429.07\n",
      "200 :  cross entropy loss =  734.314 ; reg =  417.531\n",
      "300 :  cross entropy loss =  452.538 ; reg =  437.068\n",
      "400 :  cross entropy loss =  652.091 ; reg =  398.824\n",
      "70 : train_accuracy:  0.491 ; validation_accuracy:  0.498\n",
      "71 :  learning rate =  0.014549741108224894\n",
      "--------------------------\n",
      "0 :  cross entropy loss =  1657.96 ; reg =  402.405\n",
      "100 :  cross entropy loss =  1034.34 ; reg =  445.513\n",
      "200 :  cross entropy loss =  958.217 ; reg =  396.366\n",
      "300 :  cross entropy loss =  869.297 ; reg =  387.045\n",
      "400 :  cross entropy loss =  714.64 ; reg =  392.21\n",
      "71 : train_accuracy:  0.4588 ; validation_accuracy:  0.4472\n",
      "72 :  learning rate =  0.014404243697142645\n",
      "--------------------------\n",
      "0 :  cross entropy loss =  1661.73 ; reg =  403.815\n",
      "100 :  cross entropy loss =  927.852 ; reg =  450.276\n",
      "200 :  cross entropy loss =  973.835 ; reg =  404.69\n",
      "300 :  cross entropy loss =  647.399 ; reg =  394.278\n",
      "400 :  cross entropy loss =  617.364 ; reg =  399.987\n",
      "72 : train_accuracy:  0.4756 ; validation_accuracy:  0.4864\n",
      "73 :  learning rate =  0.014260201260171218\n",
      "--------------------------\n",
      "0 :  cross entropy loss =  1154.44 ; reg =  375.351\n",
      "100 :  cross entropy loss =  621.872 ; reg =  403.601\n",
      "200 :  cross entropy loss =  431.846 ; reg =  390.521\n",
      "300 :  cross entropy loss =  732.949 ; reg =  386.804\n",
      "400 :  cross entropy loss =  852.483 ; reg =  388.377\n",
      "73 : train_accuracy:  0.5672 ; validation_accuracy:  0.5816\n",
      "74 :  learning rate =  0.014117599247569506\n",
      "--------------------------\n",
      "0 :  cross entropy loss =  734.249 ; reg =  386.509\n",
      "100 :  cross entropy loss =  667.4 ; reg =  393.076\n",
      "200 :  cross entropy loss =  718.631 ; reg =  381.945\n",
      "300 :  cross entropy loss =  566.228 ; reg =  392.493\n",
      "400 :  cross entropy loss =  583.277 ; reg =  393.387\n",
      "74 : train_accuracy:  0.5358 ; validation_accuracy:  0.516\n",
      "75 :  learning rate =  0.01397642325509381\n",
      "--------------------------\n",
      "0 :  cross entropy loss =  884.969 ; reg =  381.807\n",
      "100 :  cross entropy loss =  552.108 ; reg =  392.883\n",
      "200 :  cross entropy loss =  980.885 ; reg =  376.405\n",
      "300 :  cross entropy loss =  657.064 ; reg =  371.456\n",
      "400 :  cross entropy loss =  858.763 ; reg =  378.794\n",
      "75 : train_accuracy:  0.522 ; validation_accuracy:  0.516\n",
      "76 :  learning rate =  0.013836659022542873\n",
      "--------------------------\n",
      "0 :  cross entropy loss =  1506.49 ; reg =  399.109\n",
      "100 :  cross entropy loss =  543.77 ; reg =  395.01\n",
      "200 :  cross entropy loss =  791.78 ; reg =  379.486\n",
      "300 :  cross entropy loss =  636.557 ; reg =  380.005\n",
      "400 :  cross entropy loss =  643.868 ; reg =  385.899\n",
      "76 : train_accuracy:  0.5072 ; validation_accuracy:  0.5284\n",
      "77 :  learning rate =  0.013698292432317445\n",
      "--------------------------\n",
      "0 :  cross entropy loss =  1301.13 ; reg =  392.269\n",
      "100 :  cross entropy loss =  773.883 ; reg =  377.39\n",
      "200 :  cross entropy loss =  483.131 ; reg =  411.668\n",
      "300 :  cross entropy loss =  718.448 ; reg =  382.63\n",
      "400 :  cross entropy loss =  818.331 ; reg =  396.852\n",
      "77 : train_accuracy:  0.482 ; validation_accuracy:  0.4876\n",
      "78 :  learning rate =  0.01356130950799427\n",
      "--------------------------\n",
      "0 :  cross entropy loss =  1272.54 ; reg =  391.38\n",
      "100 :  cross entropy loss =  660.123 ; reg =  384.574\n",
      "200 :  cross entropy loss =  1058.38 ; reg =  358.49\n",
      "300 :  cross entropy loss =  662.188 ; reg =  377.259\n",
      "400 :  cross entropy loss =  722.021 ; reg =  374.054\n",
      "78 : train_accuracy:  0.5812 ; validation_accuracy:  0.5784\n",
      "79 :  learning rate =  0.013425696412914327\n",
      "--------------------------\n",
      "0 :  cross entropy loss =  770.126 ; reg =  379.988\n",
      "100 :  cross entropy loss =  555.87 ; reg =  383.066\n",
      "200 :  cross entropy loss =  712.546 ; reg =  361.513\n",
      "300 :  cross entropy loss =  693.826 ; reg =  375.684\n",
      "400 :  cross entropy loss =  719.933 ; reg =  356.117\n",
      "79 : train_accuracy:  0.5022 ; validation_accuracy:  0.5052\n",
      "80 :  learning rate =  0.013291439448785183\n",
      "--------------------------\n",
      "0 :  cross entropy loss =  1332.91 ; reg =  350.48\n",
      "100 :  cross entropy loss =  996.531 ; reg =  396.125\n",
      "200 :  cross entropy loss =  609.991 ; reg =  361.721\n",
      "300 :  cross entropy loss =  521.579 ; reg =  362.982\n",
      "400 :  cross entropy loss =  1035.91 ; reg =  334.047\n",
      "80 : train_accuracy:  0.5032 ; validation_accuracy:  0.496\n",
      "81 :  learning rate =  0.013158525054297331\n",
      "--------------------------\n",
      "0 :  cross entropy loss =  2014.96 ; reg =  355.369\n",
      "100 :  cross entropy loss =  599.323 ; reg =  406.851\n",
      "200 :  cross entropy loss =  749.115 ; reg =  355.227\n",
      "300 :  cross entropy loss =  636.671 ; reg =  365.002\n",
      "400 :  cross entropy loss =  748.384 ; reg =  363.882\n",
      "81 : train_accuracy:  0.4008 ; validation_accuracy:  0.3944\n",
      "82 :  learning rate =  0.013026939803754358\n",
      "--------------------------\n",
      "0 :  cross entropy loss =  1730.01 ; reg =  367.575\n",
      "100 :  cross entropy loss =  682.831 ; reg =  387.731\n",
      "200 :  cross entropy loss =  706.581 ; reg =  350.455\n",
      "300 :  cross entropy loss =  778.339 ; reg =  354.065\n",
      "400 :  cross entropy loss =  815.802 ; reg =  350.274\n",
      "82 : train_accuracy:  0.4914 ; validation_accuracy:  0.4976\n",
      "83 :  learning rate =  0.012896670405716813\n",
      "--------------------------\n",
      "0 :  cross entropy loss =  1148.1 ; reg =  356.679\n",
      "100 :  cross entropy loss =  677.122 ; reg =  382.832\n",
      "200 :  cross entropy loss =  963.609 ; reg =  352.012\n",
      "300 :  cross entropy loss =  444.838 ; reg =  344.417\n",
      "400 :  cross entropy loss =  776.627 ; reg =  342.326\n",
      "83 : train_accuracy:  0.5626 ; validation_accuracy:  0.5544\n",
      "84 :  learning rate =  0.012767703701659645\n",
      "--------------------------\n",
      "0 :  cross entropy loss =  793.015 ; reg =  355.486\n",
      "100 :  cross entropy loss =  476.724 ; reg =  357.603\n",
      "200 :  cross entropy loss =  640.004 ; reg =  336.021\n",
      "300 :  cross entropy loss =  597.93 ; reg =  356.673\n",
      "400 :  cross entropy loss =  726.498 ; reg =  346.395\n",
      "84 : train_accuracy:  0.5906 ; validation_accuracy:  0.6064\n",
      "85 :  learning rate =  0.01264002666464305\n",
      "--------------------------\n",
      "0 :  cross entropy loss =  708.578 ; reg =  343.432\n",
      "100 :  cross entropy loss =  801.486 ; reg =  356.169\n",
      "200 :  cross entropy loss =  465.155 ; reg =  347.915\n",
      "300 :  cross entropy loss =  441.167 ; reg =  343.168\n",
      "400 :  cross entropy loss =  574.745 ; reg =  346.488\n",
      "85 : train_accuracy:  0.5246 ; validation_accuracy:  0.5204\n",
      "86 :  learning rate =  0.012513626397996618\n",
      "--------------------------\n",
      "0 :  cross entropy loss =  691.543 ; reg =  349.289\n",
      "100 :  cross entropy loss =  706.812 ; reg =  355.786\n",
      "200 :  cross entropy loss =  823.329 ; reg =  351.41\n",
      "300 :  cross entropy loss =  632.401 ; reg =  324.825\n",
      "400 :  cross entropy loss =  767.242 ; reg =  333.73\n",
      "86 : train_accuracy:  0.534 ; validation_accuracy:  0.5144\n",
      "87 :  learning rate =  0.012388490134016652\n",
      "--------------------------\n",
      "0 :  cross entropy loss =  956.572 ; reg =  340.768\n",
      "100 :  cross entropy loss =  645.752 ; reg =  356.906\n",
      "200 :  cross entropy loss =  671.269 ; reg =  345.015\n",
      "300 :  cross entropy loss =  450.519 ; reg =  352.71\n",
      "400 :  cross entropy loss =  591.537 ; reg =  343.283\n",
      "87 : train_accuracy:  0.5632 ; validation_accuracy:  0.5408\n",
      "88 :  learning rate =  0.012264605232676485\n",
      "--------------------------\n",
      "0 :  cross entropy loss =  756.134 ; reg =  332.575\n",
      "100 :  cross entropy loss =  411.473 ; reg =  342.092\n",
      "200 :  cross entropy loss =  660.195 ; reg =  335.228\n",
      "300 :  cross entropy loss =  758.97 ; reg =  329.361\n",
      "400 :  cross entropy loss =  745.004 ; reg =  335.874\n",
      "88 : train_accuracy:  0.5128 ; validation_accuracy:  0.4972\n",
      "89 :  learning rate =  0.01214195918034972\n",
      "--------------------------\n",
      "0 :  cross entropy loss =  1326.46 ; reg =  330.101\n",
      "100 :  cross entropy loss =  609.373 ; reg =  342.824\n",
      "200 :  cross entropy loss =  536.844 ; reg =  331.818\n",
      "300 :  cross entropy loss =  600.048 ; reg =  343.397\n",
      "400 :  cross entropy loss =  372.793 ; reg =  321.01\n",
      "89 : train_accuracy:  0.552 ; validation_accuracy:  0.568\n",
      "90 :  learning rate =  0.012020539588546222\n",
      "--------------------------\n",
      "0 :  cross entropy loss =  982.835 ; reg =  329.543\n",
      "100 :  cross entropy loss =  632.799 ; reg =  335.801\n",
      "200 :  cross entropy loss =  598.775 ; reg =  326.334\n",
      "300 :  cross entropy loss =  566.207 ; reg =  327.879\n",
      "400 :  cross entropy loss =  384.315 ; reg =  329.293\n",
      "90 : train_accuracy:  0.5226 ; validation_accuracy:  0.528\n",
      "91 :  learning rate =  0.011900334192660761\n",
      "--------------------------\n",
      "0 :  cross entropy loss =  1177.85 ; reg =  326.431\n",
      "100 :  cross entropy loss =  541.528 ; reg =  333.828\n",
      "200 :  cross entropy loss =  654.315 ; reg =  332.133\n",
      "300 :  cross entropy loss =  604.387 ; reg =  348.737\n",
      "400 :  cross entropy loss =  686.531 ; reg =  315.097\n",
      "91 : train_accuracy:  0.5002 ; validation_accuracy:  0.4916\n",
      "92 :  learning rate =  0.011781330850734153\n",
      "--------------------------\n",
      "0 :  cross entropy loss =  1282.95 ; reg =  311.826\n",
      "100 :  cross entropy loss =  408.436 ; reg =  328.019\n",
      "200 :  cross entropy loss =  703.305 ; reg =  336.232\n",
      "300 :  cross entropy loss =  599.18 ; reg =  320.387\n",
      "400 :  cross entropy loss =  659.938 ; reg =  317.796\n",
      "92 : train_accuracy:  0.5152 ; validation_accuracy:  0.5004\n",
      "93 :  learning rate =  0.011663517542226812\n",
      "--------------------------\n",
      "0 :  cross entropy loss =  950.04 ; reg =  322.083\n",
      "100 :  cross entropy loss =  571.185 ; reg =  320.415\n",
      "200 :  cross entropy loss =  560.702 ; reg =  315.598\n",
      "300 :  cross entropy loss =  687.2 ; reg =  309.666\n",
      "400 :  cross entropy loss =  540.029 ; reg =  305.918\n",
      "93 : train_accuracy:  0.5238 ; validation_accuracy:  0.5188\n",
      "94 :  learning rate =  0.011546882366804543\n",
      "--------------------------\n",
      "0 :  cross entropy loss =  943.935 ; reg =  315.207\n",
      "100 :  cross entropy loss =  989.624 ; reg =  332.694\n",
      "200 :  cross entropy loss =  459.46 ; reg =  313.246\n",
      "300 :  cross entropy loss =  532.748 ; reg =  319.817\n",
      "400 :  cross entropy loss =  521.322 ; reg =  302.258\n",
      "94 : train_accuracy:  0.5446 ; validation_accuracy:  0.5296\n",
      "95 :  learning rate =  0.011431413543136497\n",
      "--------------------------\n",
      "0 :  cross entropy loss =  852.412 ; reg =  319.088\n",
      "100 :  cross entropy loss =  627.585 ; reg =  345.405\n",
      "200 :  cross entropy loss =  466.183 ; reg =  327.827\n",
      "300 :  cross entropy loss =  492.503 ; reg =  318.531\n",
      "400 :  cross entropy loss =  595.441 ; reg =  320.647\n",
      "95 : train_accuracy:  0.4512 ; validation_accuracy:  0.4456\n",
      "96 :  learning rate =  0.011317099407705132\n",
      "--------------------------\n",
      "0 :  cross entropy loss =  1286.5 ; reg =  297.289\n",
      "100 :  cross entropy loss =  528.048 ; reg =  318.492\n",
      "200 :  cross entropy loss =  549.042 ; reg =  318.631\n",
      "300 :  cross entropy loss =  457.044 ; reg =  310.914\n",
      "400 :  cross entropy loss =  727.966 ; reg =  308.132\n",
      "96 : train_accuracy:  0.547 ; validation_accuracy:  0.558\n",
      "97 :  learning rate =  0.011203928413628082\n",
      "--------------------------\n",
      "0 :  cross entropy loss =  859.112 ; reg =  307.314\n",
      "100 :  cross entropy loss =  427.825 ; reg =  302.589\n",
      "200 :  cross entropy loss =  572.515 ; reg =  313.03\n",
      "300 :  cross entropy loss =  423.932 ; reg =  292.194\n",
      "400 :  cross entropy loss =  667.359 ; reg =  297.869\n",
      "97 : train_accuracy:  0.5684 ; validation_accuracy:  0.5736\n",
      "98 :  learning rate =  0.0110918891294918\n",
      "--------------------------\n",
      "0 :  cross entropy loss =  711.771 ; reg =  298.214\n",
      "100 :  cross entropy loss =  647.463 ; reg =  315.064\n",
      "200 :  cross entropy loss =  702.091 ; reg =  302.222\n",
      "300 :  cross entropy loss =  511.393 ; reg =  305.575\n",
      "400 :  cross entropy loss =  531.582 ; reg =  292.439\n",
      "98 : train_accuracy:  0.5228 ; validation_accuracy:  0.5256\n",
      "99 :  learning rate =  0.010980970238196882\n",
      "--------------------------\n",
      "0 :  cross entropy loss =  819.05 ; reg =  291.223\n",
      "100 :  cross entropy loss =  403.895 ; reg =  311.218\n",
      "200 :  cross entropy loss =  436.958 ; reg =  289.486\n",
      "300 :  cross entropy loss =  636.26 ; reg =  301.185\n",
      "400 :  cross entropy loss =  566.244 ; reg =  288.363\n",
      "99 : train_accuracy:  0.5364 ; validation_accuracy:  0.5424\n",
      "100 :  learning rate =  0.010871160535814913\n",
      "--------------------------\n",
      "0 :  cross entropy loss =  806.956 ; reg =  282.694\n",
      "100 :  cross entropy loss =  573.243 ; reg =  306.159\n",
      "200 :  cross entropy loss =  723.625 ; reg =  287.268\n",
      "300 :  cross entropy loss =  429.53 ; reg =  294.713\n",
      "400 :  cross entropy loss =  652.771 ; reg =  276.743\n",
      "100 : train_accuracy:  0.5458 ; validation_accuracy:  0.5564\n",
      "101 :  learning rate =  0.010762448930456763\n",
      "--------------------------\n",
      "0 :  cross entropy loss =  837.223 ; reg =  283.458\n",
      "100 :  cross entropy loss =  494.075 ; reg =  298.034\n",
      "200 :  cross entropy loss =  581.995 ; reg =  291.564\n",
      "300 :  cross entropy loss =  561.375 ; reg =  307.735\n",
      "400 :  cross entropy loss =  557.136 ; reg =  289.893\n",
      "101 : train_accuracy:  0.577 ; validation_accuracy:  0.5708\n",
      "102 :  learning rate =  0.010654824441152195\n",
      "--------------------------\n",
      "0 :  cross entropy loss =  682.808 ; reg =  286.573\n",
      "100 :  cross entropy loss =  755.113 ; reg =  286.667\n",
      "200 :  cross entropy loss =  432.998 ; reg =  293.377\n",
      "300 :  cross entropy loss =  387.503 ; reg =  277.541\n",
      "400 :  cross entropy loss =  626.59 ; reg =  288.735\n",
      "102 : train_accuracy:  0.4692 ; validation_accuracy:  0.48\n",
      "103 :  learning rate =  0.010548276196740673\n",
      "--------------------------\n",
      "0 :  cross entropy loss =  1411.77 ; reg =  283.632\n",
      "100 :  cross entropy loss =  560.08 ; reg =  317.311\n",
      "200 :  cross entropy loss =  435.56 ; reg =  289.83\n",
      "300 :  cross entropy loss =  855.576 ; reg =  290.964\n",
      "400 :  cross entropy loss =  509.306 ; reg =  280.0\n",
      "103 : train_accuracy:  0.4546 ; validation_accuracy:  0.4752\n",
      "104 :  learning rate =  0.010442793434773267\n",
      "--------------------------\n",
      "0 :  cross entropy loss =  1135.27 ; reg =  280.135\n",
      "100 :  cross entropy loss =  558.919 ; reg =  300.059\n",
      "200 :  cross entropy loss =  604.199 ; reg =  280.443\n",
      "300 :  cross entropy loss =  542.578 ; reg =  278.193\n",
      "400 :  cross entropy loss =  591.008 ; reg =  280.37\n",
      "104 : train_accuracy:  0.5778 ; validation_accuracy:  0.5788\n",
      "105 :  learning rate =  0.010338365500425533\n",
      "--------------------------\n",
      "0 :  cross entropy loss =  674.501 ; reg =  287.708\n",
      "100 :  cross entropy loss =  422.529 ; reg =  298.625\n",
      "200 :  cross entropy loss =  466.885 ; reg =  288.981\n",
      "300 :  cross entropy loss =  488.576 ; reg =  283.273\n",
      "400 :  cross entropy loss =  613.452 ; reg =  271.234\n",
      "105 : train_accuracy:  0.53 ; validation_accuracy:  0.534\n",
      "106 :  learning rate =  0.010234981845421277\n",
      "--------------------------\n",
      "0 :  cross entropy loss =  765.199 ; reg =  264.549\n",
      "100 :  cross entropy loss =  478.19 ; reg =  273.588\n",
      "200 :  cross entropy loss =  630.107 ; reg =  273.805\n",
      "300 :  cross entropy loss =  460.123 ; reg =  273.786\n",
      "400 :  cross entropy loss =  731.649 ; reg =  270.47\n",
      "106 : train_accuracy:  0.5314 ; validation_accuracy:  0.5544\n",
      "107 :  learning rate =  0.010132632026967065\n",
      "--------------------------\n",
      "0 :  cross entropy loss =  1318.3 ; reg =  277.921\n",
      "100 :  cross entropy loss =  543.872 ; reg =  293.705\n",
      "200 :  cross entropy loss =  373.157 ; reg =  267.275\n",
      "300 :  cross entropy loss =  825.81 ; reg =  280.925\n",
      "400 :  cross entropy loss =  512.735 ; reg =  270.068\n",
      "107 : train_accuracy:  0.5334 ; validation_accuracy:  0.5192\n",
      "108 :  learning rate =  0.010031305706697394\n",
      "--------------------------\n",
      "0 :  cross entropy loss =  860.734 ; reg =  270.796\n",
      "100 :  cross entropy loss =  474.854 ; reg =  280.425\n",
      "200 :  cross entropy loss =  342.319 ; reg =  264.511\n",
      "300 :  cross entropy loss =  442.378 ; reg =  274.583\n",
      "400 :  cross entropy loss =  436.856 ; reg =  268.749\n",
      "108 : train_accuracy:  0.5468 ; validation_accuracy:  0.5656\n",
      "109 :  learning rate =  0.00993099264963042\n",
      "--------------------------\n",
      "0 :  cross entropy loss =  866.95 ; reg =  267.74\n",
      "100 :  cross entropy loss =  537.478 ; reg =  282.444\n",
      "200 :  cross entropy loss =  646.738 ; reg =  263.839\n",
      "300 :  cross entropy loss =  437.82 ; reg =  263.401\n",
      "400 :  cross entropy loss =  583.269 ; reg =  260.2\n",
      "109 : train_accuracy:  0.5994 ; validation_accuracy:  0.608\n",
      "110 :  learning rate =  0.009831682723134116\n",
      "--------------------------\n",
      "0 :  cross entropy loss =  672.435 ; reg =  273.526\n",
      "100 :  cross entropy loss =  543.786 ; reg =  269.842\n",
      "200 :  cross entropy loss =  429.949 ; reg =  254.905\n",
      "300 :  cross entropy loss =  627.52 ; reg =  263.273\n",
      "400 :  cross entropy loss =  548.265 ; reg =  253.58\n",
      "110 : train_accuracy:  0.5458 ; validation_accuracy:  0.5532\n",
      "111 :  learning rate =  0.009733365895902775\n",
      "--------------------------\n",
      "0 :  cross entropy loss =  1119.91 ; reg =  261.012\n",
      "100 :  cross entropy loss =  687.0 ; reg =  277.037\n",
      "200 :  cross entropy loss =  410.665 ; reg =  262.096\n",
      "300 :  cross entropy loss =  384.673 ; reg =  264.215\n",
      "400 :  cross entropy loss =  368.783 ; reg =  254.738\n",
      "111 : train_accuracy:  0.5516 ; validation_accuracy:  0.5564\n",
      "112 :  learning rate =  0.009636032236943747\n",
      "--------------------------\n",
      "0 :  cross entropy loss =  518.617 ; reg =  255.769\n",
      "100 :  cross entropy loss =  508.036 ; reg =  275.706\n",
      "200 :  cross entropy loss =  342.734 ; reg =  263.513\n",
      "300 :  cross entropy loss =  391.91 ; reg =  266.062\n",
      "400 :  cross entropy loss =  552.556 ; reg =  245.236\n",
      "112 : train_accuracy:  0.525 ; validation_accuracy:  0.5348\n",
      "113 :  learning rate =  0.00953967191457431\n",
      "--------------------------\n",
      "0 :  cross entropy loss =  1116.61 ; reg =  260.226\n",
      "100 :  cross entropy loss =  422.721 ; reg =  283.444\n",
      "200 :  cross entropy loss =  524.911 ; reg =  255.306\n",
      "300 :  cross entropy loss =  720.616 ; reg =  249.96\n",
      "400 :  cross entropy loss =  548.825 ; reg =  263.761\n",
      "113 : train_accuracy:  0.5114 ; validation_accuracy:  0.5152\n",
      "114 :  learning rate =  0.009444275195428566\n",
      "--------------------------\n",
      "0 :  cross entropy loss =  1003.84 ; reg =  260.069\n",
      "100 :  cross entropy loss =  618.264 ; reg =  272.168\n",
      "200 :  cross entropy loss =  534.067 ; reg =  257.849\n",
      "300 :  cross entropy loss =  343.808 ; reg =  243.844\n",
      "400 :  cross entropy loss =  331.027 ; reg =  237.234\n",
      "114 : train_accuracy:  0.5406 ; validation_accuracy:  0.5364\n",
      "115 :  learning rate =  0.00934983244347428\n",
      "--------------------------\n",
      "0 :  cross entropy loss =  1269.24 ; reg =  243.427\n",
      "100 :  cross entropy loss =  347.871 ; reg =  266.208\n",
      "200 :  cross entropy loss =  403.296 ; reg =  241.463\n",
      "300 :  cross entropy loss =  514.582 ; reg =  257.576\n",
      "400 :  cross entropy loss =  450.802 ; reg =  247.822\n",
      "115 : train_accuracy:  0.5814 ; validation_accuracy:  0.5788\n",
      "116 :  learning rate =  0.009256334119039538\n",
      "--------------------------\n",
      "0 :  cross entropy loss =  794.463 ; reg =  254.452\n",
      "100 :  cross entropy loss =  614.27 ; reg =  262.906\n",
      "200 :  cross entropy loss =  409.323 ; reg =  254.041\n",
      "300 :  cross entropy loss =  586.201 ; reg =  242.861\n",
      "400 :  cross entropy loss =  467.786 ; reg =  247.287\n",
      "116 : train_accuracy:  0.556 ; validation_accuracy:  0.5652\n",
      "117 :  learning rate =  0.009163770777849143\n",
      "--------------------------\n",
      "0 :  cross entropy loss =  839.601 ; reg =  248.642\n",
      "100 :  cross entropy loss =  526.245 ; reg =  265.155\n",
      "200 :  cross entropy loss =  304.617 ; reg =  238.059\n",
      "300 :  cross entropy loss =  507.686 ; reg =  247.79\n",
      "400 :  cross entropy loss =  652.497 ; reg =  241.085\n",
      "117 : train_accuracy:  0.5762 ; validation_accuracy:  0.5696\n",
      "118 :  learning rate =  0.009072133070070652\n",
      "--------------------------\n",
      "0 :  cross entropy loss =  580.242 ; reg =  252.663\n",
      "100 :  cross entropy loss =  607.791 ; reg =  251.998\n",
      "200 :  cross entropy loss =  383.283 ; reg =  241.113\n",
      "300 :  cross entropy loss =  388.548 ; reg =  234.578\n",
      "400 :  cross entropy loss =  373.236 ; reg =  234.378\n",
      "118 : train_accuracy:  0.5896 ; validation_accuracy:  0.6056\n",
      "119 :  learning rate =  0.008981411739369945\n",
      "--------------------------\n",
      "0 :  cross entropy loss =  537.412 ; reg =  228.29\n",
      "100 :  cross entropy loss =  373.499 ; reg =  238.716\n",
      "200 :  cross entropy loss =  509.449 ; reg =  236.968\n",
      "300 :  cross entropy loss =  442.457 ; reg =  234.552\n",
      "400 :  cross entropy loss =  423.635 ; reg =  230.518\n",
      "119 : train_accuracy:  0.5412 ; validation_accuracy:  0.556\n",
      "120 :  learning rate =  0.008891597621976246\n",
      "--------------------------\n",
      "0 :  cross entropy loss =  639.282 ; reg =  239.264\n",
      "100 :  cross entropy loss =  395.73 ; reg =  256.317\n",
      "200 :  cross entropy loss =  395.94 ; reg =  241.185\n",
      "300 :  cross entropy loss =  441.462 ; reg =  237.956\n",
      "400 :  cross entropy loss =  525.395 ; reg =  238.802\n",
      "120 : train_accuracy:  0.5634 ; validation_accuracy:  0.552\n",
      "121 :  learning rate =  0.008802681645756483\n",
      "--------------------------\n",
      "0 :  cross entropy loss =  889.284 ; reg =  243.117\n",
      "100 :  cross entropy loss =  541.635 ; reg =  253.013\n",
      "200 :  cross entropy loss =  441.696 ; reg =  239.555\n",
      "300 :  cross entropy loss =  404.305 ; reg =  247.025\n",
      "400 :  cross entropy loss =  529.499 ; reg =  238.258\n",
      "121 : train_accuracy:  0.559 ; validation_accuracy:  0.5736\n",
      "122 :  learning rate =  0.008714654829298918\n",
      "--------------------------\n",
      "0 :  cross entropy loss =  671.356 ; reg =  237.088\n",
      "100 :  cross entropy loss =  479.559 ; reg =  248.74\n",
      "200 :  cross entropy loss =  459.704 ; reg =  233.724\n",
      "300 :  cross entropy loss =  463.037 ; reg =  233.08\n",
      "400 :  cross entropy loss =  444.444 ; reg =  236.379\n",
      "122 : train_accuracy:  0.5732 ; validation_accuracy:  0.5876\n",
      "123 :  learning rate =  0.00862750828100593\n",
      "--------------------------\n",
      "0 :  cross entropy loss =  803.367 ; reg =  240.466\n",
      "100 :  cross entropy loss =  576.619 ; reg =  259.874\n",
      "200 :  cross entropy loss =  331.943 ; reg =  235.555\n",
      "300 :  cross entropy loss =  365.543 ; reg =  231.584\n",
      "400 :  cross entropy loss =  423.743 ; reg =  233.129\n",
      "123 : train_accuracy:  0.4906 ; validation_accuracy:  0.4876\n",
      "124 :  learning rate =  0.00854123319819587\n",
      "--------------------------\n",
      "0 :  cross entropy loss =  1085.39 ; reg =  230.154\n",
      "100 :  cross entropy loss =  300.782 ; reg =  239.724\n",
      "200 :  cross entropy loss =  374.885 ; reg =  226.814\n",
      "300 :  cross entropy loss =  447.023 ; reg =  230.338\n",
      "400 :  cross entropy loss =  639.251 ; reg =  227.469\n",
      "124 : train_accuracy:  0.6002 ; validation_accuracy:  0.5932\n",
      "125 :  learning rate =  0.00845582086621391\n",
      "--------------------------\n",
      "0 :  cross entropy loss =  611.046 ; reg =  225.122\n",
      "100 :  cross entropy loss =  447.508 ; reg =  242.227\n",
      "200 :  cross entropy loss =  283.267 ; reg =  236.429\n",
      "300 :  cross entropy loss =  418.08 ; reg =  221.296\n",
      "400 :  cross entropy loss =  515.601 ; reg =  216.815\n",
      "125 : train_accuracy:  0.5726 ; validation_accuracy:  0.5668\n",
      "126 :  learning rate =  0.00837126265755177\n",
      "--------------------------\n",
      "0 :  cross entropy loss =  544.818 ; reg =  217.662\n",
      "100 :  cross entropy loss =  244.588 ; reg =  215.424\n",
      "200 :  cross entropy loss =  445.963 ; reg =  230.491\n",
      "300 :  cross entropy loss =  328.718 ; reg =  217.275\n",
      "400 :  cross entropy loss =  455.632 ; reg =  215.484\n",
      "126 : train_accuracy:  0.578 ; validation_accuracy:  0.5764\n",
      "127 :  learning rate =  0.008287550030976252\n",
      "--------------------------\n",
      "0 :  cross entropy loss =  451.132 ; reg =  213.878\n",
      "100 :  cross entropy loss =  405.274 ; reg =  219.044\n",
      "200 :  cross entropy loss =  477.566 ; reg =  218.31\n",
      "300 :  cross entropy loss =  403.412 ; reg =  220.237\n",
      "400 :  cross entropy loss =  375.675 ; reg =  210.733\n",
      "127 : train_accuracy:  0.5226 ; validation_accuracy:  0.5312\n",
      "128 :  learning rate =  0.00820467453066649\n",
      "--------------------------\n",
      "0 :  cross entropy loss =  1068.41 ; reg =  218.095\n",
      "100 :  cross entropy loss =  250.931 ; reg =  231.636\n",
      "200 :  cross entropy loss =  474.954 ; reg =  225.278\n",
      "300 :  cross entropy loss =  420.631 ; reg =  219.308\n",
      "400 :  cross entropy loss =  638.513 ; reg =  231.264\n",
      "128 : train_accuracy:  0.57 ; validation_accuracy:  0.5788\n",
      "129 :  learning rate =  0.008122627785359826\n",
      "--------------------------\n",
      "0 :  cross entropy loss =  795.413 ; reg =  223.789\n",
      "100 :  cross entropy loss =  364.995 ; reg =  234.836\n",
      "200 :  cross entropy loss =  281.485 ; reg =  221.902\n",
      "300 :  cross entropy loss =  363.486 ; reg =  217.664\n",
      "400 :  cross entropy loss =  389.43 ; reg =  217.936\n",
      "129 : train_accuracy:  0.5382 ; validation_accuracy:  0.5432\n",
      "130 :  learning rate =  0.008041401507506228\n",
      "--------------------------\n",
      "0 :  cross entropy loss =  599.706 ; reg =  216.929\n",
      "100 :  cross entropy loss =  396.398 ; reg =  220.701\n",
      "200 :  cross entropy loss =  361.067 ; reg =  219.93\n",
      "300 :  cross entropy loss =  482.506 ; reg =  210.114\n",
      "400 :  cross entropy loss =  336.254 ; reg =  207.449\n",
      "130 : train_accuracy:  0.5798 ; validation_accuracy:  0.5812\n",
      "131 :  learning rate =  0.007960987492431166\n",
      "--------------------------\n",
      "0 :  cross entropy loss =  714.735 ; reg =  211.437\n",
      "100 :  cross entropy loss =  436.206 ; reg =  224.594\n",
      "200 :  cross entropy loss =  494.787 ; reg =  205.877\n",
      "300 :  cross entropy loss =  439.023 ; reg =  208.135\n",
      "400 :  cross entropy loss =  375.435 ; reg =  204.153\n",
      "131 : train_accuracy:  0.567 ; validation_accuracy:  0.5712\n",
      "132 :  learning rate =  0.007881377617506855\n",
      "--------------------------\n",
      "0 :  cross entropy loss =  554.7 ; reg =  204.565\n",
      "100 :  cross entropy loss =  334.109 ; reg =  223.972\n",
      "200 :  cross entropy loss =  454.221 ; reg =  209.182\n",
      "300 :  cross entropy loss =  378.232 ; reg =  204.932\n",
      "400 :  cross entropy loss =  428.405 ; reg =  207.199\n",
      "132 : train_accuracy:  0.5712 ; validation_accuracy:  0.5804\n",
      "133 :  learning rate =  0.0078025638413317866\n",
      "--------------------------\n",
      "0 :  cross entropy loss =  492.519 ; reg =  211.388\n",
      "100 :  cross entropy loss =  542.029 ; reg =  220.64\n",
      "200 :  cross entropy loss =  331.574 ; reg =  214.665\n",
      "300 :  cross entropy loss =  331.093 ; reg =  208.853\n",
      "400 :  cross entropy loss =  337.054 ; reg =  198.625\n",
      "133 : train_accuracy:  0.5636 ; validation_accuracy:  0.5824\n",
      "134 :  learning rate =  0.007724538202918469\n",
      "--------------------------\n",
      "0 :  cross entropy loss =  599.906 ; reg =  200.432\n",
      "100 :  cross entropy loss =  320.833 ; reg =  204.349\n",
      "200 :  cross entropy loss =  322.842 ; reg =  209.95\n",
      "300 :  cross entropy loss =  182.491 ; reg =  194.835\n",
      "400 :  cross entropy loss =  281.557 ; reg =  197.931\n",
      "134 : train_accuracy:  0.62 ; validation_accuracy:  0.6168\n",
      "135 :  learning rate =  0.0076472928208892845\n",
      "--------------------------\n",
      "0 :  cross entropy loss =  348.237 ; reg =  202.73\n",
      "100 :  cross entropy loss =  411.448 ; reg =  224.052\n",
      "200 :  cross entropy loss =  414.476 ; reg =  207.672\n",
      "300 :  cross entropy loss =  383.078 ; reg =  200.861\n",
      "400 :  cross entropy loss =  490.786 ; reg =  198.801\n",
      "135 : train_accuracy:  0.6102 ; validation_accuracy:  0.6104\n",
      "136 :  learning rate =  0.007570819892680392\n",
      "--------------------------\n",
      "0 :  cross entropy loss =  473.361 ; reg =  203.955\n",
      "100 :  cross entropy loss =  414.816 ; reg =  200.942\n",
      "200 :  cross entropy loss =  422.997 ; reg =  202.555\n",
      "300 :  cross entropy loss =  376.643 ; reg =  196.997\n",
      "400 :  cross entropy loss =  285.828 ; reg =  201.785\n",
      "136 : train_accuracy:  0.612 ; validation_accuracy:  0.6016\n",
      "137 :  learning rate =  0.007495111693753588\n",
      "--------------------------\n",
      "0 :  cross entropy loss =  382.654 ; reg =  198.756\n",
      "100 :  cross entropy loss =  491.441 ; reg =  201.394\n",
      "200 :  cross entropy loss =  270.916 ; reg =  204.667\n",
      "300 :  cross entropy loss =  349.986 ; reg =  195.437\n",
      "400 :  cross entropy loss =  422.646 ; reg =  199.437\n",
      "137 : train_accuracy:  0.613 ; validation_accuracy:  0.6148\n",
      "138 :  learning rate =  0.007420160576816052\n",
      "--------------------------\n",
      "0 :  cross entropy loss =  359.468 ; reg =  197.011\n",
      "100 :  cross entropy loss =  324.706 ; reg =  200.276\n",
      "200 :  cross entropy loss =  461.101 ; reg =  202.247\n",
      "300 :  cross entropy loss =  351.845 ; reg =  197.501\n",
      "400 :  cross entropy loss =  486.055 ; reg =  201.128\n",
      "138 : train_accuracy:  0.566 ; validation_accuracy:  0.5784\n",
      "139 :  learning rate =  0.007345958971047891\n",
      "--------------------------\n",
      "0 :  cross entropy loss =  472.791 ; reg =  197.836\n",
      "100 :  cross entropy loss =  212.103 ; reg =  207.481\n",
      "200 :  cross entropy loss =  325.235 ; reg =  190.083\n",
      "300 :  cross entropy loss =  424.425 ; reg =  190.474\n",
      "400 :  cross entropy loss =  328.903 ; reg =  195.0\n",
      "139 : train_accuracy:  0.634 ; validation_accuracy:  0.6172\n",
      "140 :  learning rate =  0.007272499381337412\n",
      "--------------------------\n",
      "0 :  cross entropy loss =  420.356 ; reg =  191.428\n",
      "100 :  cross entropy loss =  386.086 ; reg =  204.043\n",
      "200 :  cross entropy loss =  440.156 ; reg =  194.678\n",
      "300 :  cross entropy loss =  364.81 ; reg =  197.926\n",
      "400 :  cross entropy loss =  360.344 ; reg =  191.892\n",
      "140 : train_accuracy:  0.5796 ; validation_accuracy:  0.5668\n",
      "141 :  learning rate =  0.007199774387524038\n",
      "--------------------------\n",
      "0 :  cross entropy loss =  431.95 ; reg =  187.351\n",
      "100 :  cross entropy loss =  485.987 ; reg =  191.013\n",
      "200 :  cross entropy loss =  438.94 ; reg =  189.377\n",
      "300 :  cross entropy loss =  320.843 ; reg =  189.44\n",
      "400 :  cross entropy loss =  267.751 ; reg =  182.904\n",
      "141 : train_accuracy:  0.6002 ; validation_accuracy:  0.6044\n",
      "142 :  learning rate =  0.007127776643648797\n",
      "--------------------------\n",
      "0 :  cross entropy loss =  551.05 ; reg =  186.914\n",
      "100 :  cross entropy loss =  404.611 ; reg =  212.177\n",
      "200 :  cross entropy loss =  377.013 ; reg =  194.725\n",
      "300 :  cross entropy loss =  301.948 ; reg =  188.378\n",
      "400 :  cross entropy loss =  261.837 ; reg =  184.238\n",
      "142 : train_accuracy:  0.5708 ; validation_accuracy:  0.5808\n",
      "143 :  learning rate =  0.007056498877212309\n",
      "--------------------------\n",
      "0 :  cross entropy loss =  658.294 ; reg =  183.29\n",
      "100 :  cross entropy loss =  343.787 ; reg =  195.51\n",
      "200 :  cross entropy loss =  311.761 ; reg =  186.341\n",
      "300 :  cross entropy loss =  347.708 ; reg =  181.036\n",
      "400 :  cross entropy loss =  343.392 ; reg =  182.579\n",
      "143 : train_accuracy:  0.5592 ; validation_accuracy:  0.5424\n",
      "144 :  learning rate =  0.006985933888440187\n",
      "--------------------------\n",
      "0 :  cross entropy loss =  580.712 ; reg =  180.773\n",
      "100 :  cross entropy loss =  232.907 ; reg =  195.888\n",
      "200 :  cross entropy loss =  222.56 ; reg =  183.504\n",
      "300 :  cross entropy loss =  429.951 ; reg =  193.874\n",
      "400 :  cross entropy loss =  173.283 ; reg =  191.342\n",
      "144 : train_accuracy:  0.5874 ; validation_accuracy:  0.5776\n",
      "145 :  learning rate =  0.006916074549555785\n",
      "--------------------------\n",
      "0 :  cross entropy loss =  524.48 ; reg =  192.176\n",
      "100 :  cross entropy loss =  416.147 ; reg =  186.396\n",
      "200 :  cross entropy loss =  429.119 ; reg =  180.454\n",
      "300 :  cross entropy loss =  234.084 ; reg =  176.041\n",
      "400 :  cross entropy loss =  313.906 ; reg =  175.852\n",
      "145 : train_accuracy:  0.5462 ; validation_accuracy:  0.552\n",
      "146 :  learning rate =  0.006846913804060227\n",
      "--------------------------\n",
      "0 :  cross entropy loss =  556.315 ; reg =  176.738\n",
      "100 :  cross entropy loss =  326.431 ; reg =  194.089\n",
      "200 :  cross entropy loss =  212.422 ; reg =  177.895\n",
      "300 :  cross entropy loss =  224.202 ; reg =  174.699\n",
      "400 :  cross entropy loss =  383.699 ; reg =  182.054\n",
      "146 : train_accuracy:  0.6298 ; validation_accuracy:  0.6308\n",
      "147 :  learning rate =  0.006778444666019625\n",
      "--------------------------\n",
      "0 :  cross entropy loss =  495.61 ; reg =  183.521\n",
      "100 :  cross entropy loss =  307.264 ; reg =  196.25\n",
      "200 :  cross entropy loss =  327.363 ; reg =  181.45\n",
      "300 :  cross entropy loss =  318.255 ; reg =  184.825\n",
      "400 :  cross entropy loss =  263.973 ; reg =  175.291\n",
      "147 : train_accuracy:  0.5982 ; validation_accuracy:  0.6104\n",
      "148 :  learning rate =  0.006710660219359428\n",
      "--------------------------\n",
      "0 :  cross entropy loss =  508.141 ; reg =  176.307\n",
      "100 :  cross entropy loss =  149.815 ; reg =  183.009\n",
      "200 :  cross entropy loss =  310.47 ; reg =  177.789\n",
      "300 :  cross entropy loss =  308.993 ; reg =  178.723\n",
      "400 :  cross entropy loss =  284.249 ; reg =  175.017\n",
      "148 : train_accuracy:  0.5716 ; validation_accuracy:  0.5676\n",
      "149 :  learning rate =  0.006643553617165834\n",
      "--------------------------\n",
      "0 :  cross entropy loss =  395.52 ; reg =  170.813\n",
      "100 :  cross entropy loss =  341.221 ; reg =  175.621\n",
      "200 :  cross entropy loss =  358.275 ; reg =  176.28\n",
      "300 :  cross entropy loss =  296.52 ; reg =  165.572\n",
      "400 :  cross entropy loss =  487.503 ; reg =  174.031\n",
      "149 : train_accuracy:  0.5894 ; validation_accuracy:  0.5964\n",
      "150 :  learning rate =  0.006577118080994176\n",
      "--------------------------\n",
      "0 :  cross entropy loss =  469.355 ; reg =  176.314\n",
      "100 :  cross entropy loss =  280.264 ; reg =  196.052\n",
      "200 :  cross entropy loss =  370.881 ; reg =  183.387\n",
      "300 :  cross entropy loss =  177.377 ; reg =  172.907\n",
      "400 :  cross entropy loss =  362.17 ; reg =  169.958\n",
      "150 : train_accuracy:  0.579 ; validation_accuracy:  0.6068\n",
      "151 :  learning rate =  0.006511346900184234\n",
      "--------------------------\n",
      "0 :  cross entropy loss =  372.774 ; reg =  169.489\n",
      "100 :  cross entropy loss =  312.562 ; reg =  177.352\n",
      "200 :  cross entropy loss =  310.19 ; reg =  172.818\n",
      "300 :  cross entropy loss =  267.507 ; reg =  170.754\n",
      "400 :  cross entropy loss =  296.728 ; reg =  167.151\n",
      "151 : train_accuracy:  0.6028 ; validation_accuracy:  0.6088\n",
      "152 :  learning rate =  0.006446233431182392\n",
      "--------------------------\n",
      "0 :  cross entropy loss =  530.697 ; reg =  169.206\n",
      "100 :  cross entropy loss =  321.431 ; reg =  171.064\n",
      "200 :  cross entropy loss =  169.736 ; reg =  164.635\n",
      "300 :  cross entropy loss =  278.541 ; reg =  167.214\n",
      "400 :  cross entropy loss =  392.124 ; reg =  169.333\n",
      "152 : train_accuracy:  0.5676 ; validation_accuracy:  0.5712\n",
      "153 :  learning rate =  0.006381771096870568\n",
      "--------------------------\n",
      "0 :  cross entropy loss =  798.197 ; reg =  169.763\n",
      "100 :  cross entropy loss =  510.112 ; reg =  172.873\n",
      "200 :  cross entropy loss =  329.098 ; reg =  172.613\n",
      "300 :  cross entropy loss =  421.233 ; reg =  168.71\n",
      "400 :  cross entropy loss =  223.312 ; reg =  166.144\n",
      "153 : train_accuracy:  0.625 ; validation_accuracy:  0.6184\n",
      "154 :  learning rate =  0.006317953385901862\n",
      "--------------------------\n",
      "0 :  cross entropy loss =  512.522 ; reg =  169.096\n",
      "100 :  cross entropy loss =  251.779 ; reg =  172.894\n",
      "200 :  cross entropy loss =  164.303 ; reg =  162.714\n",
      "300 :  cross entropy loss =  269.731 ; reg =  157.664\n",
      "400 :  cross entropy loss =  234.767 ; reg =  159.452\n",
      "154 : train_accuracy:  0.6 ; validation_accuracy:  0.6132\n",
      "155 :  learning rate =  0.006254773852042843\n",
      "--------------------------\n",
      "0 :  cross entropy loss =  536.585 ; reg =  160.404\n",
      "100 :  cross entropy loss =  240.753 ; reg =  180.7\n",
      "200 :  cross entropy loss =  285.979 ; reg =  167.481\n",
      "300 :  cross entropy loss =  306.224 ; reg =  166.244\n",
      "400 :  cross entropy loss =  341.99 ; reg =  166.162\n",
      "155 : train_accuracy:  0.593 ; validation_accuracy:  0.5952\n",
      "156 :  learning rate =  0.006192226113522415\n",
      "--------------------------\n",
      "0 :  cross entropy loss =  461.449 ; reg =  165.828\n",
      "100 :  cross entropy loss =  391.511 ; reg =  169.598\n",
      "200 :  cross entropy loss =  339.311 ; reg =  168.441\n",
      "300 :  cross entropy loss =  334.143 ; reg =  166.824\n",
      "400 :  cross entropy loss =  337.721 ; reg =  168.215\n",
      "156 : train_accuracy:  0.544 ; validation_accuracy:  0.5396\n",
      "157 :  learning rate =  0.0061303038523871905\n",
      "--------------------------\n",
      "0 :  cross entropy loss =  574.87 ; reg =  169.238\n",
      "100 :  cross entropy loss =  186.06 ; reg =  171.746\n",
      "200 :  cross entropy loss =  171.614 ; reg =  158.808\n",
      "300 :  cross entropy loss =  182.408 ; reg =  153.653\n",
      "400 :  cross entropy loss =  317.573 ; reg =  152.593\n",
      "157 : train_accuracy:  0.5256 ; validation_accuracy:  0.5192\n",
      "158 :  learning rate =  0.0060690008138633185\n",
      "--------------------------\n",
      "0 :  cross entropy loss =  821.483 ; reg =  160.28\n",
      "100 :  cross entropy loss =  191.838 ; reg =  172.535\n",
      "200 :  cross entropy loss =  412.008 ; reg =  161.16\n",
      "300 :  cross entropy loss =  343.45 ; reg =  155.078\n",
      "400 :  cross entropy loss =  197.714 ; reg =  160.296\n",
      "158 : train_accuracy:  0.5892 ; validation_accuracy:  0.5956\n",
      "159 :  learning rate =  0.006008310805724685\n",
      "--------------------------\n",
      "0 :  cross entropy loss =  359.905 ; reg =  158.195\n",
      "100 :  cross entropy loss =  219.401 ; reg =  169.979\n",
      "200 :  cross entropy loss =  345.339 ; reg =  156.499\n",
      "300 :  cross entropy loss =  295.048 ; reg =  157.194\n",
      "400 :  cross entropy loss =  265.485 ; reg =  151.38\n",
      "159 : train_accuracy:  0.6048 ; validation_accuracy:  0.6244\n",
      "160 :  learning rate =  0.005948227697667438\n",
      "--------------------------\n",
      "0 :  cross entropy loss =  299.755 ; reg =  152.501\n",
      "100 :  cross entropy loss =  248.165 ; reg =  163.339\n",
      "200 :  cross entropy loss =  264.311 ; reg =  152.336\n",
      "300 :  cross entropy loss =  266.239 ; reg =  151.486\n",
      "400 :  cross entropy loss =  332.426 ; reg =  149.626\n",
      "160 : train_accuracy:  0.5572 ; validation_accuracy:  0.5624\n",
      "161 :  learning rate =  0.005888745420690763\n",
      "--------------------------\n",
      "0 :  cross entropy loss =  490.601 ; reg =  150.541\n",
      "100 :  cross entropy loss =  398.019 ; reg =  158.355\n",
      "200 :  cross entropy loss =  337.348 ; reg =  157.291\n",
      "300 :  cross entropy loss =  323.509 ; reg =  159.485\n",
      "400 :  cross entropy loss =  267.461 ; reg =  154.784\n",
      "161 : train_accuracy:  0.571 ; validation_accuracy:  0.5584\n",
      "162 :  learning rate =  0.0058298579664838555\n",
      "--------------------------\n",
      "0 :  cross entropy loss =  503.429 ; reg =  153.095\n",
      "100 :  cross entropy loss =  314.149 ; reg =  177.62\n",
      "200 :  cross entropy loss =  236.117 ; reg =  160.285\n",
      "300 :  cross entropy loss =  246.393 ; reg =  151.546\n",
      "400 :  cross entropy loss =  279.997 ; reg =  152.258\n",
      "162 : train_accuracy:  0.6046 ; validation_accuracy:  0.6008\n",
      "163 :  learning rate =  0.005771559386819017\n",
      "--------------------------\n",
      "0 :  cross entropy loss =  421.858 ; reg =  153.578\n",
      "100 :  cross entropy loss =  270.696 ; reg =  158.776\n",
      "200 :  cross entropy loss =  257.615 ; reg =  160.472\n",
      "300 :  cross entropy loss =  283.446 ; reg =  154.174\n",
      "400 :  cross entropy loss =  373.794 ; reg =  153.108\n",
      "163 : train_accuracy:  0.6056 ; validation_accuracy:  0.6132\n",
      "164 :  learning rate =  0.005713843792950826\n",
      "--------------------------\n",
      "0 :  cross entropy loss =  272.62 ; reg =  150.833\n",
      "100 :  cross entropy loss =  221.912 ; reg =  152.416\n",
      "200 :  cross entropy loss =  290.085 ; reg =  147.832\n",
      "300 :  cross entropy loss =  229.755 ; reg =  145.697\n",
      "400 :  cross entropy loss =  282.574 ; reg =  142.596\n",
      "164 : train_accuracy:  0.579 ; validation_accuracy:  0.5608\n",
      "165 :  learning rate =  0.005656705355021318\n",
      "--------------------------\n",
      "0 :  cross entropy loss =  444.52 ; reg =  140.688\n",
      "100 :  cross entropy loss =  208.446 ; reg =  156.32\n",
      "200 :  cross entropy loss =  229.031 ; reg =  148.363\n",
      "300 :  cross entropy loss =  313.075 ; reg =  149.688\n",
      "400 :  cross entropy loss =  333.69 ; reg =  148.941\n",
      "165 : train_accuracy:  0.5272 ; validation_accuracy:  0.5212\n",
      "166 :  learning rate =  0.005600138301471104\n",
      "--------------------------\n",
      "0 :  cross entropy loss =  625.878 ; reg =  145.993\n",
      "100 :  cross entropy loss =  217.519 ; reg =  162.228\n",
      "200 :  cross entropy loss =  211.653 ; reg =  149.617\n",
      "300 :  cross entropy loss =  247.716 ; reg =  147.326\n",
      "400 :  cross entropy loss =  328.335 ; reg =  142.475\n",
      "166 : train_accuracy:  0.6218 ; validation_accuracy:  0.6084\n",
      "167 :  learning rate =  0.005544136918456393\n",
      "--------------------------\n",
      "0 :  cross entropy loss =  472.782 ; reg =  148.627\n",
      "100 :  cross entropy loss =  233.959 ; reg =  148.949\n",
      "200 :  cross entropy loss =  170.644 ; reg =  139.735\n",
      "300 :  cross entropy loss =  182.443 ; reg =  142.336\n",
      "400 :  cross entropy loss =  243.689 ; reg =  139.06\n",
      "167 : train_accuracy:  0.528 ; validation_accuracy:  0.5144\n",
      "168 :  learning rate =  0.0054886955492718294\n",
      "--------------------------\n",
      "0 :  cross entropy loss =  645.805 ; reg =  141.389\n",
      "100 :  cross entropy loss =  213.555 ; reg =  150.55\n",
      "200 :  cross entropy loss =  344.641 ; reg =  147.703\n",
      "300 :  cross entropy loss =  220.124 ; reg =  142.999\n",
      "400 :  cross entropy loss =  196.912 ; reg =  139.866\n",
      "168 : train_accuracy:  0.5766 ; validation_accuracy:  0.574\n",
      "169 :  learning rate =  0.005433808593779111\n",
      "--------------------------\n",
      "0 :  cross entropy loss =  445.863 ; reg =  142.739\n",
      "100 :  cross entropy loss =  305.845 ; reg =  153.821\n",
      "200 :  cross entropy loss =  263.725 ; reg =  142.105\n",
      "300 :  cross entropy loss =  225.024 ; reg =  138.875\n",
      "400 :  cross entropy loss =  267.421 ; reg =  136.736\n",
      "169 : train_accuracy:  0.582 ; validation_accuracy:  0.5872\n",
      "170 :  learning rate =  0.00537947050784132\n",
      "--------------------------\n",
      "0 :  cross entropy loss =  478.394 ; reg =  139.265\n",
      "100 :  cross entropy loss =  359.966 ; reg =  153.357\n",
      "200 :  cross entropy loss =  208.98 ; reg =  143.068\n",
      "300 :  cross entropy loss =  270.374 ; reg =  139.281\n",
      "400 :  cross entropy loss =  218.246 ; reg =  144.684\n",
      "170 : train_accuracy:  0.6264 ; validation_accuracy:  0.6152\n",
      "171 :  learning rate =  0.005325675802762907\n",
      "--------------------------\n",
      "0 :  cross entropy loss =  248.108 ; reg =  147.429\n",
      "100 :  cross entropy loss =  228.06 ; reg =  149.848\n",
      "200 :  cross entropy loss =  159.484 ; reg =  141.32\n",
      "300 :  cross entropy loss =  243.01 ; reg =  140.547\n",
      "400 :  cross entropy loss =  169.642 ; reg =  135.829\n",
      "171 : train_accuracy:  0.5754 ; validation_accuracy:  0.5664\n",
      "172 :  learning rate =  0.0052724190447352775\n",
      "--------------------------\n",
      "0 :  cross entropy loss =  549.755 ; reg =  134.497\n",
      "100 :  cross entropy loss =  322.582 ; reg =  141.511\n",
      "200 :  cross entropy loss =  261.253 ; reg =  135.638\n",
      "300 :  cross entropy loss =  202.279 ; reg =  133.347\n",
      "400 :  cross entropy loss =  279.209 ; reg =  132.224\n",
      "172 : train_accuracy:  0.5898 ; validation_accuracy:  0.5792\n",
      "173 :  learning rate =  0.005219694854287925\n",
      "--------------------------\n",
      "0 :  cross entropy loss =  391.31 ; reg =  137.168\n",
      "100 :  cross entropy loss =  264.23 ; reg =  142.761\n",
      "200 :  cross entropy loss =  247.288 ; reg =  139.55\n",
      "300 :  cross entropy loss =  269.58 ; reg =  132.453\n",
      "400 :  cross entropy loss =  212.547 ; reg =  134.723\n",
      "173 : train_accuracy:  0.616 ; validation_accuracy:  0.6128\n",
      "174 :  learning rate =  0.005167497905745045\n",
      "--------------------------\n",
      "0 :  cross entropy loss =  409.568 ; reg =  133.772\n",
      "100 :  cross entropy loss =  228.402 ; reg =  134.647\n",
      "200 :  cross entropy loss =  189.788 ; reg =  133.606\n",
      "300 :  cross entropy loss =  247.113 ; reg =  133.829\n",
      "400 :  cross entropy loss =  241.125 ; reg =  133.968\n",
      "174 : train_accuracy:  0.5774 ; validation_accuracy:  0.5864\n",
      "175 :  learning rate =  0.005115822926687595\n",
      "--------------------------\n",
      "0 :  cross entropy loss =  397.203 ; reg =  129.761\n",
      "100 :  cross entropy loss =  196.375 ; reg =  140.807\n",
      "200 :  cross entropy loss =  277.67 ; reg =  138.011\n",
      "300 :  cross entropy loss =  134.286 ; reg =  134.099\n",
      "400 :  cross entropy loss =  200.151 ; reg =  133.083\n",
      "175 : train_accuracy:  0.5834 ; validation_accuracy:  0.5624\n",
      "176 :  learning rate =  0.005064664697420718\n",
      "--------------------------\n",
      "0 :  cross entropy loss =  447.046 ; reg =  132.023\n",
      "100 :  cross entropy loss =  209.076 ; reg =  138.144\n",
      "200 :  cross entropy loss =  257.863 ; reg =  135.998\n",
      "300 :  cross entropy loss =  181.386 ; reg =  132.709\n",
      "400 :  cross entropy loss =  272.347 ; reg =  128.651\n",
      "176 : train_accuracy:  0.6266 ; validation_accuracy:  0.6528\n",
      "177 :  learning rate =  0.005014018050446511\n",
      "--------------------------\n",
      "0 :  cross entropy loss =  258.432 ; reg =  130.277\n",
      "100 :  cross entropy loss =  247.696 ; reg =  134.209\n",
      "200 :  cross entropy loss =  200.854 ; reg =  138.427\n",
      "300 :  cross entropy loss =  169.495 ; reg =  131.327\n",
      "400 :  cross entropy loss =  367.195 ; reg =  131.76\n",
      "177 : train_accuracy:  0.653 ; validation_accuracy:  0.6628\n",
      "178 :  learning rate =  0.004963877869942046\n",
      "--------------------------\n",
      "0 :  cross entropy loss =  269.051 ; reg =  129.21\n",
      "100 :  cross entropy loss =  213.179 ; reg =  134.338\n",
      "200 :  cross entropy loss =  192.791 ; reg =  129.49\n",
      "300 :  cross entropy loss =  164.653 ; reg =  124.167\n",
      "400 :  cross entropy loss =  310.757 ; reg =  125.782\n",
      "178 : train_accuracy:  0.5942 ; validation_accuracy:  0.5804\n",
      "179 :  learning rate =  0.004914239091242625\n",
      "--------------------------\n",
      "0 :  cross entropy loss =  325.113 ; reg =  129.011\n",
      "100 :  cross entropy loss =  281.792 ; reg =  131.857\n",
      "200 :  cross entropy loss =  279.872 ; reg =  126.805\n",
      "300 :  cross entropy loss =  230.188 ; reg =  129.914\n",
      "400 :  cross entropy loss =  247.957 ; reg =  126.694\n",
      "179 : train_accuracy:  0.6282 ; validation_accuracy:  0.6216\n",
      "180 :  learning rate =  0.004865096700330199\n",
      "--------------------------\n",
      "0 :  cross entropy loss =  240.583 ; reg =  129.963\n",
      "100 :  cross entropy loss =  271.183 ; reg =  132.711\n",
      "200 :  cross entropy loss =  249.225 ; reg =  128.094\n",
      "300 :  cross entropy loss =  308.631 ; reg =  123.249\n",
      "400 :  cross entropy loss =  309.084 ; reg =  125.887\n",
      "180 : train_accuracy:  0.5544 ; validation_accuracy:  0.552\n",
      "181 :  learning rate =  0.004816445733326897\n",
      "--------------------------\n",
      "0 :  cross entropy loss =  618.46 ; reg =  123.123\n",
      "100 :  cross entropy loss =  194.043 ; reg =  132.726\n",
      "200 :  cross entropy loss =  252.084 ; reg =  128.68\n",
      "300 :  cross entropy loss =  179.257 ; reg =  123.333\n",
      "400 :  cross entropy loss =  245.999 ; reg =  121.382\n",
      "181 : train_accuracy:  0.639 ; validation_accuracy:  0.6304\n",
      "182 :  learning rate =  0.004768281275993628\n",
      "--------------------------\n",
      "0 :  cross entropy loss =  293.615 ; reg =  120.324\n",
      "100 :  cross entropy loss =  192.36 ; reg =  125.851\n",
      "200 :  cross entropy loss =  330.683 ; reg =  124.292\n",
      "300 :  cross entropy loss =  176.063 ; reg =  120.165\n",
      "400 :  cross entropy loss =  189.804 ; reg =  119.573\n",
      "182 : train_accuracy:  0.5892 ; validation_accuracy:  0.5916\n",
      "183 :  learning rate =  0.004720598463233692\n",
      "--------------------------\n",
      "0 :  cross entropy loss =  379.121 ; reg =  117.557\n",
      "100 :  cross entropy loss =  187.58 ; reg =  123.445\n",
      "200 :  cross entropy loss =  145.918 ; reg =  119.002\n",
      "300 :  cross entropy loss =  217.375 ; reg =  118.413\n",
      "400 :  cross entropy loss =  145.911 ; reg =  120.528\n",
      "183 : train_accuracy:  0.5878 ; validation_accuracy:  0.5976\n",
      "184 :  learning rate =  0.004673392478601355\n",
      "--------------------------\n",
      "0 :  cross entropy loss =  546.103 ; reg =  121.383\n",
      "100 :  cross entropy loss =  253.429 ; reg =  128.882\n",
      "200 :  cross entropy loss =  173.606 ; reg =  125.791\n",
      "300 :  cross entropy loss =  172.058 ; reg =  119.692\n",
      "400 :  cross entropy loss =  180.839 ; reg =  123.475\n",
      "184 : train_accuracy:  0.5952 ; validation_accuracy:  0.584\n",
      "185 :  learning rate =  0.004626658553815341\n",
      "--------------------------\n",
      "0 :  cross entropy loss =  340.076 ; reg =  121.292\n",
      "100 :  cross entropy loss =  166.136 ; reg =  122.036\n",
      "200 :  cross entropy loss =  181.629 ; reg =  123.767\n",
      "300 :  cross entropy loss =  190.979 ; reg =  124.732\n",
      "400 :  cross entropy loss =  187.844 ; reg =  119.983\n",
      "185 : train_accuracy:  0.6114 ; validation_accuracy:  0.5948\n",
      "186 :  learning rate =  0.004580391968277188\n",
      "--------------------------\n",
      "0 :  cross entropy loss =  413.044 ; reg =  118.138\n",
      "100 :  cross entropy loss =  258.975 ; reg =  123.918\n",
      "200 :  cross entropy loss =  148.469 ; reg =  120.631\n",
      "300 :  cross entropy loss =  302.71 ; reg =  118.933\n",
      "400 :  cross entropy loss =  213.263 ; reg =  114.601\n",
      "186 : train_accuracy:  0.6662 ; validation_accuracy:  0.6656\n",
      "187 :  learning rate =  0.004534588048594416\n",
      "--------------------------\n",
      "0 :  cross entropy loss =  199.848 ; reg =  116.926\n",
      "100 :  cross entropy loss =  264.754 ; reg =  117.671\n",
      "200 :  cross entropy loss =  237.903 ; reg =  115.895\n",
      "300 :  cross entropy loss =  284.25 ; reg =  114.272\n",
      "400 :  cross entropy loss =  89.1356 ; reg =  114.533\n",
      "187 : train_accuracy:  0.6092 ; validation_accuracy:  0.6084\n",
      "188 :  learning rate =  0.004489242168108472\n",
      "--------------------------\n",
      "0 :  cross entropy loss =  468.36 ; reg =  115.274\n",
      "100 :  cross entropy loss =  154.585 ; reg =  119.363\n",
      "200 :  cross entropy loss =  230.373 ; reg =  114.653\n",
      "300 :  cross entropy loss =  202.187 ; reg =  115.038\n",
      "400 :  cross entropy loss =  216.827 ; reg =  115.232\n",
      "188 : train_accuracy:  0.6234 ; validation_accuracy:  0.6132\n",
      "189 :  learning rate =  0.004444349746427387\n",
      "--------------------------\n",
      "0 :  cross entropy loss =  344.025 ; reg =  115.018\n",
      "100 :  cross entropy loss =  170.962 ; reg =  121.874\n",
      "200 :  cross entropy loss =  226.641 ; reg =  118.733\n",
      "300 :  cross entropy loss =  201.831 ; reg =  117.449\n",
      "400 :  cross entropy loss =  177.385 ; reg =  114.69\n",
      "189 : train_accuracy:  0.5486 ; validation_accuracy:  0.5364\n",
      "190 :  learning rate =  0.0043999062489631134\n",
      "--------------------------\n",
      "0 :  cross entropy loss =  549.75 ; reg =  114.355\n",
      "100 :  cross entropy loss =  179.38 ; reg =  120.123\n",
      "200 :  cross entropy loss =  226.503 ; reg =  114.737\n",
      "300 :  cross entropy loss =  212.172 ; reg =  109.911\n",
      "400 :  cross entropy loss =  253.946 ; reg =  109.109\n",
      "190 : train_accuracy:  0.6362 ; validation_accuracy:  0.6352\n",
      "191 :  learning rate =  0.0043559071864734825\n",
      "--------------------------\n",
      "0 :  cross entropy loss =  248.113 ; reg =  111.695\n",
      "100 :  cross entropy loss =  228.69 ; reg =  114.456\n",
      "200 :  cross entropy loss =  179.405 ; reg =  112.74\n",
      "300 :  cross entropy loss =  177.465 ; reg =  112.509\n",
      "400 :  cross entropy loss =  205.961 ; reg =  113.833\n",
      "191 : train_accuracy:  0.6046 ; validation_accuracy:  0.6012\n",
      "192 :  learning rate =  0.0043123481146087475\n",
      "--------------------------\n",
      "0 :  cross entropy loss =  326.813 ; reg =  115.46\n",
      "100 :  cross entropy loss =  171.388 ; reg =  117.414\n",
      "200 :  cross entropy loss =  182.091 ; reg =  116.78\n",
      "300 :  cross entropy loss =  247.255 ; reg =  112.52\n",
      "400 :  cross entropy loss =  216.734 ; reg =  111.171\n",
      "192 : train_accuracy:  0.6206 ; validation_accuracy:  0.6092\n",
      "193 :  learning rate =  0.00426922463346266\n",
      "--------------------------\n",
      "0 :  cross entropy loss =  207.131 ; reg =  110.8\n",
      "100 :  cross entropy loss =  332.315 ; reg =  109.625\n",
      "200 :  cross entropy loss =  270.444 ; reg =  109.79\n",
      "300 :  cross entropy loss =  179.595 ; reg =  109.628\n",
      "400 :  cross entropy loss =  165.904 ; reg =  112.282\n",
      "193 : train_accuracy:  0.6154 ; validation_accuracy:  0.6044\n",
      "194 :  learning rate =  0.0042265323871280335\n",
      "--------------------------\n",
      "0 :  cross entropy loss =  465.669 ; reg =  111.188\n",
      "100 :  cross entropy loss =  221.414 ; reg =  121.839\n",
      "200 :  cross entropy loss =  141.045 ; reg =  111.932\n",
      "300 :  cross entropy loss =  195.352 ; reg =  109.698\n",
      "400 :  cross entropy loss =  221.447 ; reg =  107.538\n",
      "194 : train_accuracy:  0.616 ; validation_accuracy:  0.6088\n",
      "195 :  learning rate =  0.004184267063256753\n",
      "--------------------------\n",
      "0 :  cross entropy loss =  287.566 ; reg =  107.774\n",
      "100 :  cross entropy loss =  277.944 ; reg =  114.042\n",
      "200 :  cross entropy loss =  147.845 ; reg =  111.445\n",
      "300 :  cross entropy loss =  215.182 ; reg =  109.057\n",
      "400 :  cross entropy loss =  128.253 ; reg =  107.684\n",
      "195 : train_accuracy:  0.6304 ; validation_accuracy:  0.6236\n",
      "196 :  learning rate =  0.004142424392624185\n",
      "--------------------------\n",
      "0 :  cross entropy loss =  335.396 ; reg =  104.629\n",
      "100 :  cross entropy loss =  193.072 ; reg =  109.581\n",
      "200 :  cross entropy loss =  147.32 ; reg =  109.011\n",
      "300 :  cross entropy loss =  163.351 ; reg =  109.453\n",
      "400 :  cross entropy loss =  145.623 ; reg =  107.601\n",
      "196 : train_accuracy:  0.6262 ; validation_accuracy:  0.616\n",
      "197 :  learning rate =  0.004101000148697943\n",
      "--------------------------\n",
      "0 :  cross entropy loss =  337.706 ; reg =  108.296\n",
      "100 :  cross entropy loss =  191.61 ; reg =  111.366\n",
      "200 :  cross entropy loss =  151.62 ; reg =  108.499\n",
      "300 :  cross entropy loss =  181.01 ; reg =  106.546\n",
      "400 :  cross entropy loss =  269.162 ; reg =  105.862\n",
      "197 : train_accuracy:  0.5992 ; validation_accuracy:  0.5756\n",
      "198 :  learning rate =  0.004059990147210964\n",
      "--------------------------\n",
      "0 :  cross entropy loss =  713.813 ; reg =  107.773\n",
      "100 :  cross entropy loss =  193.867 ; reg =  115.067\n",
      "200 :  cross entropy loss =  152.939 ; reg =  110.117\n",
      "300 :  cross entropy loss =  134.71 ; reg =  105.703\n",
      "400 :  cross entropy loss =  234.045 ; reg =  103.271\n",
      "198 : train_accuracy:  0.5924 ; validation_accuracy:  0.564\n",
      "199 :  learning rate =  0.004019390245738855\n",
      "--------------------------\n",
      "0 :  cross entropy loss =  292.595 ; reg =  103.568\n",
      "100 :  cross entropy loss =  151.908 ; reg =  108.732\n",
      "200 :  cross entropy loss =  105.848 ; reg =  104.132\n",
      "300 :  cross entropy loss =  137.471 ; reg =  102.866\n",
      "400 :  cross entropy loss =  183.345 ; reg =  104.037\n",
      "199 : train_accuracy:  0.6412 ; validation_accuracy:  0.6216\n",
      "200 :  learning rate =  0.003979196343281467\n",
      "--------------------------\n",
      "0 :  cross entropy loss =  356.134 ; reg =  102.992\n",
      "100 :  cross entropy loss =  154.116 ; reg =  108.881\n",
      "200 :  cross entropy loss =  147.567 ; reg =  103.434\n",
      "300 :  cross entropy loss =  136.361 ; reg =  100.851\n",
      "400 :  cross entropy loss =  224.888 ; reg =  96.9256\n",
      "200 : train_accuracy:  0.655 ; validation_accuracy:  0.6484\n",
      "201 :  learning rate =  0.003939404379848652\n",
      "--------------------------\n",
      "0 :  cross entropy loss =  421.072 ; reg =  98.6662\n",
      "100 :  cross entropy loss =  209.588 ; reg =  108.187\n",
      "200 :  cross entropy loss =  253.459 ; reg =  105.101\n",
      "300 :  cross entropy loss =  132.696 ; reg =  102.495\n",
      "400 :  cross entropy loss =  161.123 ; reg =  100.494\n",
      "201 : train_accuracy:  0.633 ; validation_accuracy:  0.6204\n",
      "202 :  learning rate =  0.0039000103360501658\n",
      "--------------------------\n",
      "0 :  cross entropy loss =  265.989 ; reg =  101.867\n",
      "100 :  cross entropy loss =  145.204 ; reg =  102.297\n",
      "200 :  cross entropy loss =  128.779 ; reg =  98.6932\n",
      "300 :  cross entropy loss =  223.701 ; reg =  101.07\n",
      "400 :  cross entropy loss =  118.805 ; reg =  100.023\n",
      "202 : train_accuracy:  0.6396 ; validation_accuracy:  0.6456\n",
      "203 :  learning rate =  0.003861010232689664\n",
      "--------------------------\n",
      "0 :  cross entropy loss =  169.726 ; reg =  98.6036\n",
      "100 :  cross entropy loss =  176.853 ; reg =  98.864\n",
      "200 :  cross entropy loss =  149.603 ; reg =  93.696\n",
      "300 :  cross entropy loss =  189.728 ; reg =  97.4982\n",
      "400 :  cross entropy loss =  134.972 ; reg =  99.2056\n",
      "203 : train_accuracy:  0.6376 ; validation_accuracy:  0.6424\n",
      "204 :  learning rate =  0.0038224001303627676\n",
      "--------------------------\n",
      "0 :  cross entropy loss =  219.123 ; reg =  96.8086\n",
      "100 :  cross entropy loss =  131.983 ; reg =  99.1465\n",
      "200 :  cross entropy loss =  212.907 ; reg =  95.8055\n",
      "300 :  cross entropy loss =  166.175 ; reg =  97.099\n",
      "400 :  cross entropy loss =  207.554 ; reg =  96.2179\n",
      "204 : train_accuracy:  0.6188 ; validation_accuracy:  0.6268\n",
      "205 :  learning rate =  0.00378417612905914\n",
      "--------------------------\n",
      "0 :  cross entropy loss =  253.944 ; reg =  96.2557\n",
      "100 :  cross entropy loss =  128.309 ; reg =  104.098\n",
      "200 :  cross entropy loss =  262.887 ; reg =  100.83\n",
      "300 :  cross entropy loss =  194.463 ; reg =  98.1945\n",
      "400 :  cross entropy loss =  158.635 ; reg =  94.4947\n",
      "205 : train_accuracy:  0.6118 ; validation_accuracy:  0.6156\n",
      "206 :  learning rate =  0.0037463343677685483\n",
      "--------------------------\n",
      "0 :  cross entropy loss =  199.376 ; reg =  94.999\n",
      "100 :  cross entropy loss =  172.945 ; reg =  95.911\n",
      "200 :  cross entropy loss =  142.945 ; reg =  96.666\n",
      "300 :  cross entropy loss =  179.111 ; reg =  94.7517\n",
      "400 :  cross entropy loss =  121.841 ; reg =  94.7005\n",
      "206 : train_accuracy:  0.5856 ; validation_accuracy:  0.5804\n",
      "207 :  learning rate =  0.003708871024090863\n",
      "--------------------------\n",
      "0 :  cross entropy loss =  190.904 ; reg =  96.5888\n",
      "100 :  cross entropy loss =  118.976 ; reg =  99.3157\n",
      "200 :  cross entropy loss =  177.678 ; reg =  95.3021\n",
      "300 :  cross entropy loss =  181.853 ; reg =  94.1256\n",
      "400 :  cross entropy loss =  174.818 ; reg =  92.3409\n",
      "207 : train_accuracy:  0.656 ; validation_accuracy:  0.6304\n",
      "208 :  learning rate =  0.003671782313849954\n",
      "--------------------------\n",
      "0 :  cross entropy loss =  262.489 ; reg =  93.9041\n",
      "100 :  cross entropy loss =  109.853 ; reg =  99.3395\n",
      "200 :  cross entropy loss =  218.401 ; reg =  97.3179\n",
      "300 :  cross entropy loss =  149.176 ; reg =  96.7565\n",
      "400 :  cross entropy loss =  196.844 ; reg =  95.576\n",
      "208 : train_accuracy:  0.6378 ; validation_accuracy:  0.6208\n",
      "209 :  learning rate =  0.0036350644907114545\n",
      "--------------------------\n",
      "0 :  cross entropy loss =  311.65 ; reg =  97.0224\n",
      "100 :  cross entropy loss =  257.087 ; reg =  100.769\n",
      "200 :  cross entropy loss =  152.955 ; reg =  95.3808\n",
      "300 :  cross entropy loss =  167.05 ; reg =  95.4685\n",
      "400 :  cross entropy loss =  198.584 ; reg =  96.2476\n",
      "209 : train_accuracy:  0.602 ; validation_accuracy:  0.5952\n",
      "210 :  learning rate =  0.0035987138458043397\n",
      "--------------------------\n",
      "0 :  cross entropy loss =  479.242 ; reg =  95.3594\n",
      "100 :  cross entropy loss =  126.712 ; reg =  100.301\n",
      "200 :  cross entropy loss =  175.194 ; reg =  95.8462\n",
      "300 :  cross entropy loss =  226.841 ; reg =  95.4252\n",
      "400 :  cross entropy loss =  142.292 ; reg =  94.5738\n",
      "210 : train_accuracy:  0.6648 ; validation_accuracy:  0.6472\n",
      "211 :  learning rate =  0.003562726707346296\n",
      "--------------------------\n",
      "0 :  cross entropy loss =  157.758 ; reg =  93.9319\n",
      "100 :  cross entropy loss =  190.127 ; reg =  97.2373\n",
      "200 :  cross entropy loss =  120.333 ; reg =  93.3348\n",
      "300 :  cross entropy loss =  226.019 ; reg =  91.5473\n",
      "400 :  cross entropy loss =  202.278 ; reg =  89.5291\n",
      "211 : train_accuracy:  0.6366 ; validation_accuracy:  0.6448\n",
      "212 :  learning rate =  0.003527099440272833\n",
      "--------------------------\n",
      "0 :  cross entropy loss =  203.064 ; reg =  89.239\n",
      "100 :  cross entropy loss =  185.79 ; reg =  92.4119\n",
      "200 :  cross entropy loss =  136.664 ; reg =  92.4911\n",
      "300 :  cross entropy loss =  118.21 ; reg =  91.5694\n",
      "400 :  cross entropy loss =  202.204 ; reg =  91.3018\n",
      "212 : train_accuracy:  0.646 ; validation_accuracy:  0.654\n",
      "213 :  learning rate =  0.0034918284458701045\n",
      "--------------------------\n",
      "0 :  cross entropy loss =  247.321 ; reg =  92.7807\n",
      "100 :  cross entropy loss =  111.691 ; reg =  95.5805\n",
      "200 :  cross entropy loss =  156.587 ; reg =  91.6535\n",
      "300 :  cross entropy loss =  161.247 ; reg =  89.5096\n",
      "400 :  cross entropy loss =  207.808 ; reg =  90.2379\n",
      "213 : train_accuracy:  0.5772 ; validation_accuracy:  0.5652\n",
      "214 :  learning rate =  0.0034569101614114034\n",
      "--------------------------\n",
      "0 :  cross entropy loss =  342.795 ; reg =  89.0158\n",
      "100 :  cross entropy loss =  135.492 ; reg =  92.609\n",
      "200 :  cross entropy loss =  109.873 ; reg =  91.0823\n",
      "300 :  cross entropy loss =  203.158 ; reg =  88.8595\n",
      "400 :  cross entropy loss =  130.733 ; reg =  87.1936\n",
      "214 : train_accuracy:  0.5574 ; validation_accuracy:  0.5584\n",
      "215 :  learning rate =  0.0034223410597972893\n",
      "--------------------------\n",
      "0 :  cross entropy loss =  261.759 ; reg =  86.6678\n",
      "100 :  cross entropy loss =  108.486 ; reg =  92.02\n",
      "200 :  cross entropy loss =  128.035 ; reg =  88.999\n",
      "300 :  cross entropy loss =  136.735 ; reg =  87.8917\n",
      "400 :  cross entropy loss =  160.305 ; reg =  86.3074\n",
      "215 : train_accuracy:  0.6118 ; validation_accuracy:  0.6036\n",
      "216 :  learning rate =  0.0033881176491993162\n",
      "--------------------------\n",
      "0 :  cross entropy loss =  344.352 ; reg =  84.2704\n",
      "100 :  cross entropy loss =  98.8093 ; reg =  90.156\n",
      "200 :  cross entropy loss =  84.91 ; reg =  88.6923\n",
      "300 :  cross entropy loss =  146.431 ; reg =  88.4425\n",
      "400 :  cross entropy loss =  177.559 ; reg =  85.8501\n",
      "216 : train_accuracy:  0.5968 ; validation_accuracy:  0.5788\n",
      "217 :  learning rate =  0.003354236472707323\n",
      "--------------------------\n",
      "0 :  cross entropy loss =  273.639 ; reg =  83.1988\n",
      "100 :  cross entropy loss =  141.177 ; reg =  92.0019\n",
      "200 :  cross entropy loss =  187.135 ; reg =  86.1636\n",
      "300 :  cross entropy loss =  126.678 ; reg =  85.6384\n",
      "400 :  cross entropy loss =  147.845 ; reg =  84.3415\n",
      "217 : train_accuracy:  0.6356 ; validation_accuracy:  0.6388\n",
      "218 :  learning rate =  0.0033206941079802496\n",
      "--------------------------\n",
      "0 :  cross entropy loss =  170.707 ; reg =  85.2121\n",
      "100 :  cross entropy loss =  92.8713 ; reg =  84.405\n",
      "200 :  cross entropy loss =  142.722 ; reg =  83.7781\n",
      "300 :  cross entropy loss =  106.606 ; reg =  83.3382\n",
      "400 :  cross entropy loss =  104.71 ; reg =  84.4098\n",
      "218 : train_accuracy:  0.6124 ; validation_accuracy:  0.6176\n",
      "219 :  learning rate =  0.003287487166900447\n",
      "--------------------------\n",
      "0 :  cross entropy loss =  261.618 ; reg =  87.5303\n",
      "100 :  cross entropy loss =  111.428 ; reg =  94.4376\n",
      "200 :  cross entropy loss =  139.699 ; reg =  89.247\n",
      "300 :  cross entropy loss =  164.318 ; reg =  86.8134\n",
      "400 :  cross entropy loss =  135.875 ; reg =  84.8748\n",
      "219 : train_accuracy:  0.6658 ; validation_accuracy:  0.6632\n",
      "220 :  learning rate =  0.0032546122952314426\n",
      "--------------------------\n",
      "0 :  cross entropy loss =  142.857 ; reg =  83.4529\n",
      "100 :  cross entropy loss =  127.471 ; reg =  82.5377\n",
      "200 :  cross entropy loss =  193.329 ; reg =  82.1738\n",
      "300 :  cross entropy loss =  149.668 ; reg =  80.8424\n",
      "400 :  cross entropy loss =  157.036 ; reg =  79.8584\n",
      "220 : train_accuracy:  0.6538 ; validation_accuracy:  0.642\n",
      "221 :  learning rate =  0.003222066172279128\n",
      "--------------------------\n",
      "0 :  cross entropy loss =  117.713 ; reg =  79.6419\n",
      "100 :  cross entropy loss =  148.434 ; reg =  83.0288\n",
      "200 :  cross entropy loss =  131.12 ; reg =  83.7772\n",
      "300 :  cross entropy loss =  117.702 ; reg =  82.4193\n",
      "400 :  cross entropy loss =  98.9706 ; reg =  80.8745\n",
      "221 : train_accuracy:  0.6284 ; validation_accuracy:  0.6132\n",
      "222 :  learning rate =  0.0031898455105563365\n",
      "--------------------------\n",
      "0 :  cross entropy loss =  335.634 ; reg =  82.8826\n",
      "100 :  cross entropy loss =  118.499 ; reg =  85.9466\n",
      "200 :  cross entropy loss =  155.614 ; reg =  82.9028\n",
      "300 :  cross entropy loss =  185.456 ; reg =  80.9889\n",
      "400 :  cross entropy loss =  132.345 ; reg =  79.7872\n",
      "222 : train_accuracy:  0.6142 ; validation_accuracy:  0.6032\n",
      "223 :  learning rate =  0.0031579470554507732\n",
      "--------------------------\n",
      "0 :  cross entropy loss =  321.919 ; reg =  80.4404\n",
      "100 :  cross entropy loss =  110.648 ; reg =  84.7651\n",
      "200 :  cross entropy loss =  139.405 ; reg =  83.6653\n",
      "300 :  cross entropy loss =  128.474 ; reg =  81.9212\n",
      "400 :  cross entropy loss =  194.557 ; reg =  83.2807\n",
      "223 : train_accuracy:  0.6624 ; validation_accuracy:  0.6664\n",
      "224 :  learning rate =  0.0031263675848962657\n",
      "--------------------------\n",
      "0 :  cross entropy loss =  218.16 ; reg =  82.9332\n",
      "100 :  cross entropy loss =  137.524 ; reg =  85.5194\n",
      "200 :  cross entropy loss =  163.095 ; reg =  82.2111\n",
      "300 :  cross entropy loss =  120.133 ; reg =  80.9744\n",
      "400 :  cross entropy loss =  174.859 ; reg =  80.4433\n",
      "224 : train_accuracy:  0.628 ; validation_accuracy:  0.6124\n",
      "225 :  learning rate =  0.003095103909047303\n",
      "--------------------------\n",
      "0 :  cross entropy loss =  194.607 ; reg =  81.2796\n",
      "100 :  cross entropy loss =  124.088 ; reg =  79.4161\n",
      "200 :  cross entropy loss =  117.463 ; reg =  78.2278\n",
      "300 :  cross entropy loss =  167.287 ; reg =  78.7854\n",
      "400 :  cross entropy loss =  134.971 ; reg =  78.297\n",
      "225 : train_accuracy:  0.6486 ; validation_accuracy:  0.6356\n",
      "226 :  learning rate =  0.0030641528699568298\n",
      "--------------------------\n",
      "0 :  cross entropy loss =  286.618 ; reg =  78.5265\n",
      "100 :  cross entropy loss =  62.8934 ; reg =  79.6619\n",
      "200 :  cross entropy loss =  107.611 ; reg =  78.6844\n",
      "300 :  cross entropy loss =  156.116 ; reg =  78.7988\n",
      "400 :  cross entropy loss =  126.433 ; reg =  77.9834\n",
      "226 : train_accuracy:  0.6694 ; validation_accuracy:  0.6604\n",
      "227 :  learning rate =  0.0030335113412572616\n",
      "--------------------------\n",
      "0 :  cross entropy loss =  192.923 ; reg =  77.6795\n",
      "100 :  cross entropy loss =  193.613 ; reg =  78.9499\n",
      "200 :  cross entropy loss =  228.813 ; reg =  78.315\n",
      "300 :  cross entropy loss =  136.443 ; reg =  76.3615\n",
      "400 :  cross entropy loss =  168.059 ; reg =  78.7307\n",
      "227 : train_accuracy:  0.6186 ; validation_accuracy:  0.6056\n",
      "228 :  learning rate =  0.003003176227844689\n",
      "--------------------------\n",
      "0 :  cross entropy loss =  357.357 ; reg =  78.9132\n",
      "100 :  cross entropy loss =  104.485 ; reg =  81.9376\n",
      "200 :  cross entropy loss =  185.707 ; reg =  78.7803\n",
      "300 :  cross entropy loss =  113.925 ; reg =  78.125\n",
      "400 :  cross entropy loss =  99.4898 ; reg =  77.6861\n",
      "228 : train_accuracy:  0.6788 ; validation_accuracy:  0.664\n",
      "229 :  learning rate =  0.0029731444655662423\n",
      "--------------------------\n",
      "0 :  cross entropy loss =  163.106 ; reg =  75.679\n",
      "100 :  cross entropy loss =  101.301 ; reg =  79.8382\n",
      "200 :  cross entropy loss =  156.863 ; reg =  75.8756\n",
      "300 :  cross entropy loss =  162.109 ; reg =  75.2065\n",
      "400 :  cross entropy loss =  141.07 ; reg =  76.0493\n",
      "229 : train_accuracy:  0.646 ; validation_accuracy:  0.6112\n",
      "230 :  learning rate =  0.00294341302091058\n",
      "--------------------------\n",
      "0 :  cross entropy loss =  286.701 ; reg =  76.1502\n",
      "100 :  cross entropy loss =  108.508 ; reg =  76.5699\n",
      "200 :  cross entropy loss =  58.9177 ; reg =  76.7294\n",
      "300 :  cross entropy loss =  108.616 ; reg =  75.0363\n",
      "400 :  cross entropy loss =  75.1963 ; reg =  75.9551\n",
      "230 : train_accuracy:  0.6608 ; validation_accuracy:  0.656\n",
      "231 :  learning rate =  0.002913978890701474\n",
      "--------------------------\n",
      "0 :  cross entropy loss =  244.832 ; reg =  76.4037\n",
      "100 :  cross entropy loss =  128.673 ; reg =  78.7745\n",
      "200 :  cross entropy loss =  132.729 ; reg =  75.1591\n",
      "300 :  cross entropy loss =  103.898 ; reg =  75.1462\n",
      "400 :  cross entropy loss =  111.624 ; reg =  75.2782\n",
      "231 : train_accuracy:  0.6066 ; validation_accuracy:  0.6036\n",
      "232 :  learning rate =  0.0028848391017944593\n",
      "--------------------------\n",
      "0 :  cross entropy loss =  350.39 ; reg =  73.7657\n",
      "100 :  cross entropy loss =  152.346 ; reg =  78.7242\n",
      "200 :  cross entropy loss =  187.445 ; reg =  75.8532\n",
      "300 :  cross entropy loss =  175.188 ; reg =  72.1666\n",
      "400 :  cross entropy loss =  166.062 ; reg =  72.2954\n",
      "232 : train_accuracy:  0.6174 ; validation_accuracy:  0.6136\n",
      "233 :  learning rate =  0.0028559907107765146\n",
      "--------------------------\n",
      "0 :  cross entropy loss =  246.269 ; reg =  71.3996\n",
      "100 :  cross entropy loss =  134.067 ; reg =  73.8902\n",
      "200 :  cross entropy loss =  70.7728 ; reg =  71.9286\n",
      "300 :  cross entropy loss =  148.534 ; reg =  72.4922\n",
      "400 :  cross entropy loss =  84.2052 ; reg =  71.596\n",
      "233 : train_accuracy:  0.6502 ; validation_accuracy:  0.6468\n",
      "234 :  learning rate =  0.0028274308036687493\n",
      "--------------------------\n",
      "0 :  cross entropy loss =  162.237 ; reg =  72.7597\n",
      "100 :  cross entropy loss =  109.033 ; reg =  72.5938\n",
      "200 :  cross entropy loss =  139.13 ; reg =  71.0797\n",
      "300 :  cross entropy loss =  131.177 ; reg =  72.1144\n",
      "400 :  cross entropy loss =  92.1753 ; reg =  70.5897\n",
      "234 : train_accuracy:  0.6546 ; validation_accuracy:  0.6344\n",
      "235 :  learning rate =  0.002799156495632062\n",
      "--------------------------\n",
      "0 :  cross entropy loss =  281.421 ; reg =  71.6971\n",
      "100 :  cross entropy loss =  152.284 ; reg =  75.7291\n",
      "200 :  cross entropy loss =  89.3795 ; reg =  73.2186\n",
      "300 :  cross entropy loss =  133.674 ; reg =  72.0669\n",
      "400 :  cross entropy loss =  130.275 ; reg =  72.2952\n",
      "235 : train_accuracy:  0.6478 ; validation_accuracy:  0.6296\n",
      "236 :  learning rate =  0.0027711649306757413\n",
      "--------------------------\n",
      "0 :  cross entropy loss =  353.502 ; reg =  71.5128\n",
      "100 :  cross entropy loss =  83.3062 ; reg =  77.8097\n",
      "200 :  cross entropy loss =  85.2637 ; reg =  73.6902\n",
      "300 :  cross entropy loss =  123.273 ; reg =  72.6614\n",
      "400 :  cross entropy loss =  97.9702 ; reg =  70.0527\n",
      "236 : train_accuracy:  0.7052 ; validation_accuracy:  0.6868\n",
      "237 :  learning rate =  0.002743453281368984\n",
      "--------------------------\n",
      "0 :  cross entropy loss =  158.495 ; reg =  69.0991\n",
      "100 :  cross entropy loss =  173.288 ; reg =  71.8371\n",
      "200 :  cross entropy loss =  131.554 ; reg =  69.7645\n",
      "300 :  cross entropy loss =  125.808 ; reg =  70.6786\n",
      "400 :  cross entropy loss =  91.3779 ; reg =  70.6077\n",
      "237 : train_accuracy:  0.6658 ; validation_accuracy:  0.6548\n",
      "238 :  learning rate =  0.0027160187485552943\n",
      "--------------------------\n",
      "0 :  cross entropy loss =  115.498 ; reg =  70.4681\n",
      "100 :  cross entropy loss =  117.256 ; reg =  70.2417\n",
      "200 :  cross entropy loss =  132.934 ; reg =  70.3188\n",
      "300 :  cross entropy loss =  170.801 ; reg =  70.3602\n",
      "400 :  cross entropy loss =  85.2403 ; reg =  69.9102\n",
      "238 : train_accuracy:  0.6538 ; validation_accuracy:  0.6328\n",
      "239 :  learning rate =  0.002688858561069741\n",
      "--------------------------\n",
      "0 :  cross entropy loss =  184.726 ; reg =  70.1052\n",
      "100 :  cross entropy loss =  106.928 ; reg =  73.9875\n",
      "200 :  cross entropy loss =  126.15 ; reg =  71.3805\n",
      "300 :  cross entropy loss =  144.017 ; reg =  70.8498\n",
      "400 :  cross entropy loss =  169.215 ; reg =  69.2464\n",
      "239 : train_accuracy:  0.6214 ; validation_accuracy:  0.6036\n",
      "240 :  learning rate =  0.0026619699754590435\n",
      "--------------------------\n",
      "0 :  cross entropy loss =  294.576 ; reg =  68.5304\n",
      "100 :  cross entropy loss =  79.3373 ; reg =  74.3076\n",
      "200 :  cross entropy loss =  137.472 ; reg =  71.2175\n",
      "300 :  cross entropy loss =  134.591 ; reg =  69.5585\n",
      "400 :  cross entropy loss =  103.341 ; reg =  66.517\n",
      "240 : train_accuracy:  0.6946 ; validation_accuracy:  0.6812\n",
      "241 :  learning rate =  0.002635350275704453\n",
      "--------------------------\n",
      "0 :  cross entropy loss =  116.775 ; reg =  65.8652\n",
      "100 :  cross entropy loss =  72.3105 ; reg =  68.0707\n",
      "200 :  cross entropy loss =  167.953 ; reg =  67.6975\n",
      "300 :  cross entropy loss =  111.021 ; reg =  65.6616\n",
      "400 :  cross entropy loss =  147.114 ; reg =  65.2393\n",
      "241 : train_accuracy:  0.6576 ; validation_accuracy:  0.6428\n",
      "242 :  learning rate =  0.0026089967729474085\n",
      "--------------------------\n",
      "0 :  cross entropy loss =  135.139 ; reg =  64.5118\n",
      "100 :  cross entropy loss =  95.5611 ; reg =  64.632\n",
      "200 :  cross entropy loss =  75.1691 ; reg =  65.3804\n",
      "300 :  cross entropy loss =  110.219 ; reg =  64.2927\n",
      "400 :  cross entropy loss =  121.012 ; reg =  65.5887\n",
      "242 : train_accuracy:  0.6748 ; validation_accuracy:  0.6544\n",
      "243 :  learning rate =  0.0025829068052179343\n",
      "--------------------------\n",
      "0 :  cross entropy loss =  96.3084 ; reg =  67.0957\n",
      "100 :  cross entropy loss =  104.601 ; reg =  66.0644\n",
      "200 :  cross entropy loss =  137.87 ; reg =  66.3236\n",
      "300 :  cross entropy loss =  77.6537 ; reg =  65.4117\n",
      "400 :  cross entropy loss =  119.332 ; reg =  65.7839\n",
      "243 : train_accuracy:  0.6646 ; validation_accuracy:  0.658\n",
      "244 :  learning rate =  0.0025570777371657547\n",
      "--------------------------\n",
      "0 :  cross entropy loss =  135.455 ; reg =  65.938\n",
      "100 :  cross entropy loss =  60.0408 ; reg =  65.9256\n",
      "200 :  cross entropy loss =  101.722 ; reg =  65.1714\n",
      "300 :  cross entropy loss =  91.983 ; reg =  63.8051\n",
      "400 :  cross entropy loss =  55.88 ; reg =  63.6206\n",
      "244 : train_accuracy:  0.6748 ; validation_accuracy:  0.6492\n",
      "245 :  learning rate =  0.0025315069597940973\n",
      "--------------------------\n",
      "0 :  cross entropy loss =  161.868 ; reg =  62.1367\n",
      "100 :  cross entropy loss =  79.2378 ; reg =  63.4056\n",
      "200 :  cross entropy loss =  77.9895 ; reg =  64.0008\n",
      "300 :  cross entropy loss =  90.5136 ; reg =  64.0829\n",
      "400 :  cross entropy loss =  79.2914 ; reg =  63.1567\n",
      "245 : train_accuracy:  0.6736 ; validation_accuracy:  0.6668\n",
      "246 :  learning rate =  0.0025061918901961564\n",
      "--------------------------\n",
      "0 :  cross entropy loss =  147.464 ; reg =  63.8913\n",
      "100 :  cross entropy loss =  127.77 ; reg =  64.4436\n",
      "200 :  cross entropy loss =  96.2677 ; reg =  64.4115\n",
      "300 :  cross entropy loss =  126.382 ; reg =  64.3233\n",
      "400 :  cross entropy loss =  119.186 ; reg =  63.8699\n",
      "246 : train_accuracy:  0.6428 ; validation_accuracy:  0.6236\n",
      "247 :  learning rate =  0.0024811299712941947\n",
      "--------------------------\n",
      "0 :  cross entropy loss =  244.916 ; reg =  63.8196\n",
      "100 :  cross entropy loss =  114.35 ; reg =  68.0738\n",
      "200 :  cross entropy loss =  77.126 ; reg =  63.4743\n",
      "300 :  cross entropy loss =  127.089 ; reg =  63.7523\n",
      "400 :  cross entropy loss =  129.247 ; reg =  62.9467\n",
      "247 : train_accuracy:  0.6316 ; validation_accuracy:  0.608\n",
      "248 :  learning rate =  0.0024563186715812527\n",
      "--------------------------\n",
      "0 :  cross entropy loss =  213.937 ; reg =  61.5872\n",
      "100 :  cross entropy loss =  92.5679 ; reg =  63.4652\n",
      "200 :  cross entropy loss =  71.0176 ; reg =  61.7066\n",
      "300 :  cross entropy loss =  117.333 ; reg =  62.7794\n",
      "400 :  cross entropy loss =  104.798 ; reg =  61.5977\n",
      "248 : train_accuracy:  0.6924 ; validation_accuracy:  0.6532\n",
      "249 :  learning rate =  0.00243175548486544\n",
      "--------------------------\n",
      "0 :  cross entropy loss =  101.031 ; reg =  61.0081\n",
      "100 :  cross entropy loss =  67.9869 ; reg =  60.1126\n",
      "200 :  cross entropy loss =  95.5298 ; reg =  60.6642\n",
      "300 :  cross entropy loss =  83.7106 ; reg =  60.2028\n",
      "400 :  cross entropy loss =  94.0698 ; reg =  60.3737\n",
      "249 : train_accuracy:  0.6732 ; validation_accuracy:  0.6388\n",
      "250 :  learning rate =  0.002407437930016786\n",
      "--------------------------\n",
      "0 :  cross entropy loss =  178.853 ; reg =  60.3901\n",
      "100 :  cross entropy loss =  157.885 ; reg =  63.6923\n",
      "200 :  cross entropy loss =  85.8383 ; reg =  63.6382\n",
      "300 :  cross entropy loss =  108.587 ; reg =  62.1651\n",
      "400 :  cross entropy loss =  90.6338 ; reg =  61.3079\n",
      "250 : train_accuracy:  0.6444 ; validation_accuracy:  0.6268\n",
      "251 :  learning rate =  0.002383363550716618\n",
      "--------------------------\n",
      "0 :  cross entropy loss =  214.517 ; reg =  60.8686\n",
      "100 :  cross entropy loss =  72.3659 ; reg =  62.9142\n",
      "200 :  cross entropy loss =  67.5287 ; reg =  61.5737\n",
      "300 :  cross entropy loss =  115.409 ; reg =  61.2552\n",
      "400 :  cross entropy loss =  98.3358 ; reg =  59.949\n",
      "251 : train_accuracy:  0.6176 ; validation_accuracy:  0.6108\n",
      "252 :  learning rate =  0.002359529915209452\n",
      "--------------------------\n",
      "0 :  cross entropy loss =  255.957 ; reg =  59.9211\n",
      "100 :  cross entropy loss =  84.8439 ; reg =  62.2108\n",
      "200 :  cross entropy loss =  100.728 ; reg =  60.4407\n",
      "300 :  cross entropy loss =  84.1011 ; reg =  60.7249\n",
      "400 :  cross entropy loss =  142.789 ; reg =  60.3709\n",
      "252 : train_accuracy:  0.6738 ; validation_accuracy:  0.6432\n",
      "253 :  learning rate =  0.0023359346160573575\n",
      "--------------------------\n",
      "0 :  cross entropy loss =  185.947 ; reg =  61.6109\n",
      "100 :  cross entropy loss =  68.4502 ; reg =  61.1635\n",
      "200 :  cross entropy loss =  125.691 ; reg =  59.8749\n",
      "300 :  cross entropy loss =  70.3817 ; reg =  58.9008\n",
      "400 :  cross entropy loss =  90.9268 ; reg =  58.7309\n",
      "253 : train_accuracy:  0.6646 ; validation_accuracy:  0.658\n",
      "254 :  learning rate =  0.002312575269896784\n",
      "--------------------------\n",
      "0 :  cross entropy loss =  178.955 ; reg =  58.8381\n",
      "100 :  cross entropy loss =  99.5688 ; reg =  60.6376\n",
      "200 :  cross entropy loss =  108.615 ; reg =  59.3267\n",
      "300 :  cross entropy loss =  63.3244 ; reg =  58.3555\n",
      "400 :  cross entropy loss =  73.8974 ; reg =  59.306\n",
      "254 : train_accuracy:  0.6536 ; validation_accuracy:  0.618\n",
      "255 :  learning rate =  0.002289449517197816\n",
      "--------------------------\n",
      "0 :  cross entropy loss =  179.276 ; reg =  58.1528\n",
      "100 :  cross entropy loss =  108.406 ; reg =  61.6422\n",
      "200 :  cross entropy loss =  60.0336 ; reg =  59.6433\n",
      "300 :  cross entropy loss =  109.435 ; reg =  58.8335\n",
      "400 :  cross entropy loss =  54.467 ; reg =  55.9291\n",
      "255 : train_accuracy:  0.7236 ; validation_accuracy:  0.7196\n",
      "256 :  learning rate =  0.0022665550220258377\n",
      "--------------------------\n",
      "0 :  cross entropy loss =  83.0581 ; reg =  55.7283\n",
      "100 :  cross entropy loss =  65.8345 ; reg =  56.0016\n",
      "200 :  cross entropy loss =  80.0898 ; reg =  55.5371\n",
      "300 :  cross entropy loss =  104.022 ; reg =  54.9834\n",
      "400 :  cross entropy loss =  90.2262 ; reg =  55.2464\n",
      "256 : train_accuracy:  0.6624 ; validation_accuracy:  0.6588\n",
      "257 :  learning rate =  0.002243889471805579\n",
      "--------------------------\n",
      "0 :  cross entropy loss =  120.76 ; reg =  56.1889\n",
      "100 :  cross entropy loss =  79.9924 ; reg =  57.3074\n",
      "200 :  cross entropy loss =  116.562 ; reg =  57.9537\n",
      "300 :  cross entropy loss =  76.0137 ; reg =  56.927\n",
      "400 :  cross entropy loss =  92.5499 ; reg =  56.0935\n",
      "257 : train_accuracy:  0.6826 ; validation_accuracy:  0.6692\n",
      "258 :  learning rate =  0.0022214505770875234\n",
      "--------------------------\n",
      "0 :  cross entropy loss =  129.413 ; reg =  55.9326\n",
      "100 :  cross entropy loss =  45.4808 ; reg =  58.298\n",
      "200 :  cross entropy loss =  166.063 ; reg =  57.0664\n",
      "300 :  cross entropy loss =  117.712 ; reg =  57.4621\n",
      "400 :  cross entropy loss =  94.6983 ; reg =  56.6781\n",
      "258 : train_accuracy:  0.6834 ; validation_accuracy:  0.6796\n",
      "259 :  learning rate =  0.002199236071316648\n",
      "--------------------------\n",
      "0 :  cross entropy loss =  179.964 ; reg =  55.7727\n",
      "100 :  cross entropy loss =  111.202 ; reg =  56.5593\n",
      "200 :  cross entropy loss =  127.668 ; reg =  56.7037\n",
      "300 :  cross entropy loss =  92.7455 ; reg =  56.4169\n",
      "400 :  cross entropy loss =  93.723 ; reg =  55.3437\n",
      "259 : train_accuracy:  0.6872 ; validation_accuracy:  0.6724\n",
      "260 :  learning rate =  0.0021772437106034816\n",
      "--------------------------\n",
      "0 :  cross entropy loss =  132.863 ; reg =  54.5993\n",
      "100 :  cross entropy loss =  87.5917 ; reg =  56.7064\n",
      "200 :  cross entropy loss =  85.7126 ; reg =  55.8613\n",
      "300 :  cross entropy loss =  92.31 ; reg =  55.2767\n",
      "400 :  cross entropy loss =  87.0493 ; reg =  54.695\n",
      "260 : train_accuracy:  0.706 ; validation_accuracy:  0.68\n",
      "261 :  learning rate =  0.002155471273497447\n",
      "--------------------------\n",
      "0 :  cross entropy loss =  82.1316 ; reg =  54.6911\n",
      "100 :  cross entropy loss =  94.3073 ; reg =  54.2909\n",
      "200 :  cross entropy loss =  57.513 ; reg =  54.1287\n",
      "300 :  cross entropy loss =  114.51 ; reg =  54.12\n",
      "400 :  cross entropy loss =  59.1177 ; reg =  54.3596\n",
      "261 : train_accuracy:  0.6896 ; validation_accuracy:  0.6736\n",
      "262 :  learning rate =  0.0021339165607624725\n",
      "--------------------------\n",
      "0 :  cross entropy loss =  199.886 ; reg =  55.1097\n",
      "100 :  cross entropy loss =  117.723 ; reg =  56.6713\n",
      "200 :  cross entropy loss =  63.8029 ; reg =  54.561\n",
      "300 :  cross entropy loss =  78.7142 ; reg =  53.3002\n",
      "400 :  cross entropy loss =  82.0915 ; reg =  53.1031\n",
      "262 : train_accuracy:  0.671 ; validation_accuracy:  0.6608\n",
      "263 :  learning rate =  0.0021125773951548477\n",
      "--------------------------\n",
      "0 :  cross entropy loss =  119.477 ; reg =  53.031\n",
      "100 :  cross entropy loss =  98.7818 ; reg =  54.0542\n",
      "200 :  cross entropy loss =  50.059 ; reg =  53.2858\n",
      "300 :  cross entropy loss =  74.2758 ; reg =  53.1775\n",
      "400 :  cross entropy loss =  106.316 ; reg =  54.3856\n",
      "263 : train_accuracy:  0.7028 ; validation_accuracy:  0.688\n",
      "264 :  learning rate =  0.002091451621203299\n",
      "--------------------------\n",
      "0 :  cross entropy loss =  86.6704 ; reg =  54.1445\n",
      "100 :  cross entropy loss =  72.6964 ; reg =  53.6674\n",
      "200 :  cross entropy loss =  72.0984 ; reg =  52.9096\n",
      "300 :  cross entropy loss =  88.5843 ; reg =  52.423\n",
      "400 :  cross entropy loss =  90.9875 ; reg =  51.9553\n",
      "264 : train_accuracy:  0.6336 ; validation_accuracy:  0.6244\n",
      "265 :  learning rate =  0.002070537104991266\n",
      "--------------------------\n",
      "0 :  cross entropy loss =  203.626 ; reg =  52.5542\n",
      "100 :  cross entropy loss =  81.9871 ; reg =  56.0758\n",
      "200 :  cross entropy loss =  122.342 ; reg =  54.0786\n",
      "300 :  cross entropy loss =  85.9039 ; reg =  52.938\n",
      "400 :  cross entropy loss =  73.1095 ; reg =  52.6896\n",
      "265 : train_accuracy:  0.7136 ; validation_accuracy:  0.696\n",
      "266 :  learning rate =  0.0020498317339413532\n",
      "--------------------------\n",
      "0 :  cross entropy loss =  125.898 ; reg =  53.2399\n",
      "100 :  cross entropy loss =  70.0602 ; reg =  52.9704\n",
      "200 :  cross entropy loss =  108.223 ; reg =  52.4591\n",
      "300 :  cross entropy loss =  87.2788 ; reg =  52.0775\n",
      "400 :  cross entropy loss =  82.6368 ; reg =  51.76\n",
      "266 : train_accuracy:  0.7248 ; validation_accuracy:  0.6916\n",
      "267 :  learning rate =  0.0020293334166019395\n",
      "--------------------------\n",
      "0 :  cross entropy loss =  70.7266 ; reg =  51.2332\n",
      "100 :  cross entropy loss =  72.987 ; reg =  50.9097\n",
      "200 :  cross entropy loss =  103.636 ; reg =  50.9488\n",
      "300 :  cross entropy loss =  76.0329 ; reg =  50.437\n",
      "400 :  cross entropy loss =  88.4458 ; reg =  51.14\n",
      "267 : train_accuracy:  0.6858 ; validation_accuracy:  0.6644\n",
      "268 :  learning rate =  0.00200904008243592\n",
      "--------------------------\n",
      "0 :  cross entropy loss =  120.273 ; reg =  51.3964\n",
      "100 :  cross entropy loss =  71.8815 ; reg =  53.2351\n",
      "200 :  cross entropy loss =  73.1286 ; reg =  51.6128\n",
      "300 :  cross entropy loss =  83.3253 ; reg =  51.6223\n",
      "400 :  cross entropy loss =  79.3164 ; reg =  51.4095\n",
      "268 : train_accuracy:  0.6906 ; validation_accuracy:  0.682\n",
      "269 :  learning rate =  0.001988949681611561\n",
      "--------------------------\n",
      "0 :  cross entropy loss =  124.438 ; reg =  51.3015\n",
      "100 :  cross entropy loss =  89.7793 ; reg =  50.3635\n",
      "200 :  cross entropy loss =  52.0771 ; reg =  49.9628\n",
      "300 :  cross entropy loss =  60.78 ; reg =  49.3425\n",
      "400 :  cross entropy loss =  68.0763 ; reg =  49.6049\n",
      "269 : train_accuracy:  0.6714 ; validation_accuracy:  0.6388\n",
      "270 :  learning rate =  0.0019690601847954453\n",
      "--------------------------\n",
      "0 :  cross entropy loss =  167.768 ; reg =  48.9071\n",
      "100 :  cross entropy loss =  82.0464 ; reg =  51.1344\n",
      "200 :  cross entropy loss =  119.575 ; reg =  50.3167\n",
      "300 :  cross entropy loss =  66.3502 ; reg =  50.3787\n",
      "400 :  cross entropy loss =  64.1274 ; reg =  50.2441\n",
      "270 : train_accuracy:  0.6804 ; validation_accuracy:  0.654\n",
      "271 :  learning rate =  0.001949369582947491\n",
      "--------------------------\n",
      "0 :  cross entropy loss =  144.044 ; reg =  49.452\n",
      "100 :  cross entropy loss =  75.7383 ; reg =  51.0832\n",
      "200 :  cross entropy loss =  54.5537 ; reg =  49.9316\n",
      "300 :  cross entropy loss =  72.8058 ; reg =  49.7694\n",
      "400 :  cross entropy loss =  106.198 ; reg =  49.1208\n",
      "271 : train_accuracy:  0.6992 ; validation_accuracy:  0.6932\n",
      "272 :  learning rate =  0.001929875887118016\n",
      "--------------------------\n",
      "0 :  cross entropy loss =  105.032 ; reg =  48.539\n",
      "100 :  cross entropy loss =  139.793 ; reg =  49.8005\n",
      "200 :  cross entropy loss =  63.2594 ; reg =  49.2589\n",
      "300 :  cross entropy loss =  87.8396 ; reg =  49.1663\n",
      "400 :  cross entropy loss =  98.2009 ; reg =  48.8256\n",
      "272 : train_accuracy:  0.7068 ; validation_accuracy:  0.682\n",
      "273 :  learning rate =  0.0019105771282468358\n",
      "--------------------------\n",
      "0 :  cross entropy loss =  93.1739 ; reg =  48.6314\n",
      "100 :  cross entropy loss =  48.098 ; reg =  49.5721\n",
      "200 :  cross entropy loss =  36.9543 ; reg =  48.0445\n",
      "300 :  cross entropy loss =  63.1521 ; reg =  48.5396\n",
      "400 :  cross entropy loss =  84.1209 ; reg =  47.3266\n",
      "273 : train_accuracy:  0.653 ; validation_accuracy:  0.6284\n",
      "274 :  learning rate =  0.0018914713569643674\n",
      "--------------------------\n",
      "0 :  cross entropy loss =  208.067 ; reg =  48.355\n",
      "100 :  cross entropy loss =  89.1966 ; reg =  51.6324\n",
      "200 :  cross entropy loss =  71.0617 ; reg =  50.3165\n",
      "300 :  cross entropy loss =  88.5092 ; reg =  48.9323\n",
      "400 :  cross entropy loss =  149.762 ; reg =  48.5438\n",
      "274 : train_accuracy:  0.717 ; validation_accuracy:  0.6872\n",
      "275 :  learning rate =  0.0018725566433947236\n",
      "--------------------------\n",
      "0 :  cross entropy loss =  108.917 ; reg =  48.7139\n",
      "100 :  cross entropy loss =  79.0429 ; reg =  49.3188\n",
      "200 :  cross entropy loss =  62.8321 ; reg =  47.8482\n",
      "300 :  cross entropy loss =  90.4498 ; reg =  47.6407\n",
      "400 :  cross entropy loss =  86.3506 ; reg =  46.4391\n",
      "275 : train_accuracy:  0.686 ; validation_accuracy:  0.66\n",
      "276 :  learning rate =  0.0018538310769607763\n",
      "--------------------------\n",
      "0 :  cross entropy loss =  142.059 ; reg =  46.4185\n",
      "100 :  cross entropy loss =  50.9387 ; reg =  49.2345\n",
      "200 :  cross entropy loss =  54.7847 ; reg =  47.4487\n",
      "300 :  cross entropy loss =  80.9681 ; reg =  46.6776\n",
      "400 :  cross entropy loss =  67.688 ; reg =  46.8466\n",
      "276 : train_accuracy:  0.6998 ; validation_accuracy:  0.6952\n",
      "277 :  learning rate =  0.0018352927661911685\n",
      "--------------------------\n",
      "0 :  cross entropy loss =  152.01 ; reg =  46.378\n",
      "100 :  cross entropy loss =  84.0826 ; reg =  48.0647\n",
      "200 :  cross entropy loss =  59.3493 ; reg =  46.6076\n",
      "300 :  cross entropy loss =  54.4599 ; reg =  45.6494\n",
      "400 :  cross entropy loss =  98.0224 ; reg =  45.0929\n",
      "277 : train_accuracy:  0.6588 ; validation_accuracy:  0.6404\n",
      "278 :  learning rate =  0.001816939838529257\n",
      "--------------------------\n",
      "0 :  cross entropy loss =  125.87 ; reg =  44.5957\n",
      "100 :  cross entropy loss =  46.9408 ; reg =  45.4101\n",
      "200 :  cross entropy loss =  83.8008 ; reg =  45.3264\n",
      "300 :  cross entropy loss =  69.972 ; reg =  45.0485\n",
      "400 :  cross entropy loss =  71.4127 ; reg =  44.7718\n",
      "278 : train_accuracy:  0.7156 ; validation_accuracy:  0.6912\n",
      "279 :  learning rate =  0.0017987704401439643\n",
      "--------------------------\n",
      "0 :  cross entropy loss =  96.9225 ; reg =  44.5739\n",
      "100 :  cross entropy loss =  40.1918 ; reg =  45.4308\n",
      "200 :  cross entropy loss =  101.454 ; reg =  45.1137\n",
      "300 :  cross entropy loss =  79.9956 ; reg =  44.4734\n",
      "400 :  cross entropy loss =  99.5508 ; reg =  44.9858\n",
      "279 : train_accuracy:  0.7022 ; validation_accuracy:  0.6676\n",
      "280 :  learning rate =  0.0017807827357425247\n",
      "--------------------------\n",
      "0 :  cross entropy loss =  86.8947 ; reg =  45.3815\n",
      "100 :  cross entropy loss =  86.0739 ; reg =  45.3174\n",
      "200 :  cross entropy loss =  70.2055 ; reg =  45.2081\n",
      "300 :  cross entropy loss =  72.9969 ; reg =  44.6916\n",
      "400 :  cross entropy loss =  54.0798 ; reg =  44.4243\n",
      "280 : train_accuracy:  0.7032 ; validation_accuracy:  0.694\n",
      "281 :  learning rate =  0.0017629749083850994\n",
      "--------------------------\n",
      "0 :  cross entropy loss =  101.003 ; reg =  44.7715\n",
      "100 :  cross entropy loss =  67.5842 ; reg =  44.5224\n",
      "200 :  cross entropy loss =  43.6873 ; reg =  44.1017\n",
      "300 :  cross entropy loss =  82.4029 ; reg =  44.3019\n",
      "400 :  cross entropy loss =  80.5731 ; reg =  44.4553\n",
      "281 : train_accuracy:  0.6956 ; validation_accuracy:  0.674\n",
      "282 :  learning rate =  0.0017453451593012483\n",
      "--------------------------\n",
      "0 :  cross entropy loss =  82.1207 ; reg =  44.3468\n",
      "100 :  cross entropy loss =  62.2663 ; reg =  45.3665\n",
      "200 :  cross entropy loss =  43.9933 ; reg =  44.9823\n",
      "300 :  cross entropy loss =  42.1183 ; reg =  44.8497\n",
      "400 :  cross entropy loss =  76.9814 ; reg =  44.7367\n",
      "282 : train_accuracy:  0.6756 ; validation_accuracy:  0.6492\n",
      "283 :  learning rate =  0.0017278917077082358\n",
      "--------------------------\n",
      "0 :  cross entropy loss =  198.55 ; reg =  45.1941\n",
      "100 :  cross entropy loss =  39.4855 ; reg =  48.6618\n",
      "200 :  cross entropy loss =  33.6248 ; reg =  46.4745\n",
      "300 :  cross entropy loss =  72.9359 ; reg =  45.0584\n",
      "400 :  cross entropy loss =  61.4121 ; reg =  44.2612\n",
      "283 : train_accuracy:  0.7582 ; validation_accuracy:  0.7284\n",
      "284 :  learning rate =  0.0017106127906311535\n",
      "--------------------------\n",
      "0 :  cross entropy loss =  56.0621 ; reg =  43.2766\n",
      "100 :  cross entropy loss =  75.3598 ; reg =  42.9118\n",
      "200 :  cross entropy loss =  62.504 ; reg =  43.0755\n",
      "300 :  cross entropy loss =  39.5523 ; reg =  41.9212\n",
      "400 :  cross entropy loss =  68.4994 ; reg =  42.3648\n",
      "284 : train_accuracy:  0.7184 ; validation_accuracy:  0.7016\n",
      "285 :  learning rate =  0.0016935066627248418\n",
      "--------------------------\n",
      "0 :  cross entropy loss =  97.39 ; reg =  42.4556\n",
      "100 :  cross entropy loss =  63.9111 ; reg =  43.3497\n",
      "200 :  cross entropy loss =  98.2648 ; reg =  42.5087\n",
      "300 :  cross entropy loss =  83.9296 ; reg =  42.5592\n",
      "400 :  cross entropy loss =  39.4776 ; reg =  42.0599\n",
      "285 : train_accuracy:  0.6988 ; validation_accuracy:  0.6896\n",
      "286 :  learning rate =  0.0016765715960975933\n",
      "--------------------------\n",
      "0 :  cross entropy loss =  95.8674 ; reg =  41.7755\n",
      "100 :  cross entropy loss =  55.5783 ; reg =  43.6572\n",
      "200 :  cross entropy loss =  77.7068 ; reg =  42.6924\n",
      "300 :  cross entropy loss =  48.5622 ; reg =  41.6481\n",
      "400 :  cross entropy loss =  82.4867 ; reg =  41.0876\n",
      "286 : train_accuracy:  0.7386 ; validation_accuracy:  0.7044\n",
      "287 :  learning rate =  0.0016598058801366173\n",
      "--------------------------\n",
      "0 :  cross entropy loss =  93.4823 ; reg =  40.9772\n",
      "100 :  cross entropy loss =  66.7111 ; reg =  41.7643\n",
      "200 :  cross entropy loss =  54.3812 ; reg =  41.3756\n",
      "300 :  cross entropy loss =  105.329 ; reg =  41.5844\n",
      "400 :  cross entropy loss =  85.7029 ; reg =  41.4666\n",
      "287 : train_accuracy:  0.6828 ; validation_accuracy:  0.6656\n",
      "288 :  learning rate =  0.0016432078213352512\n",
      "--------------------------\n",
      "0 :  cross entropy loss =  140.943 ; reg =  40.895\n",
      "100 :  cross entropy loss =  69.4473 ; reg =  41.9619\n",
      "200 :  cross entropy loss =  134.274 ; reg =  41.7917\n",
      "300 :  cross entropy loss =  35.689 ; reg =  41.964\n",
      "400 :  cross entropy loss =  59.2378 ; reg =  41.9269\n",
      "288 : train_accuracy:  0.7124 ; validation_accuracy:  0.6748\n",
      "289 :  learning rate =  0.0016267757431218987\n",
      "--------------------------\n",
      "0 :  cross entropy loss =  114.481 ; reg =  41.394\n",
      "100 :  cross entropy loss =  47.8806 ; reg =  41.7532\n",
      "200 :  cross entropy loss =  43.5965 ; reg =  41.3791\n",
      "300 :  cross entropy loss =  73.2685 ; reg =  40.7932\n",
      "400 :  cross entropy loss =  66.6938 ; reg =  40.7172\n",
      "289 : train_accuracy:  0.7174 ; validation_accuracy:  0.672\n",
      "290 :  learning rate =  0.0016105079856906796\n",
      "--------------------------\n",
      "0 :  cross entropy loss =  79.8853 ; reg =  40.596\n",
      "100 :  cross entropy loss =  38.5708 ; reg =  40.2309\n",
      "200 :  cross entropy loss =  61.8747 ; reg =  39.9412\n",
      "300 :  cross entropy loss =  72.0331 ; reg =  39.4687\n",
      "400 :  cross entropy loss =  48.6541 ; reg =  39.6127\n",
      "290 : train_accuracy:  0.699 ; validation_accuracy:  0.6732\n",
      "291 :  learning rate =  0.0015944029058337728\n",
      "--------------------------\n",
      "0 :  cross entropy loss =  74.5703 ; reg =  39.7239\n",
      "100 :  cross entropy loss =  61.8102 ; reg =  40.7383\n",
      "200 :  cross entropy loss =  59.7185 ; reg =  40.2213\n",
      "300 :  cross entropy loss =  63.231 ; reg =  39.8092\n",
      "400 :  cross entropy loss =  46.167 ; reg =  39.4266\n",
      "291 : train_accuracy:  0.718 ; validation_accuracy:  0.6784\n",
      "292 :  learning rate =  0.001578458876775435\n",
      "--------------------------\n",
      "0 :  cross entropy loss =  103.463 ; reg =  39.2963\n",
      "100 :  cross entropy loss =  51.8063 ; reg =  41.0556\n",
      "200 :  cross entropy loss =  45.2011 ; reg =  40.5281\n",
      "300 :  cross entropy loss =  53.7016 ; reg =  40.0399\n",
      "400 :  cross entropy loss =  54.0922 ; reg =  39.8437\n",
      "292 : train_accuracy:  0.7044 ; validation_accuracy:  0.6776\n",
      "293 :  learning rate =  0.0015626742880076806\n",
      "--------------------------\n",
      "0 :  cross entropy loss =  60.4682 ; reg =  39.3562\n",
      "100 :  cross entropy loss =  71.7427 ; reg =  38.8643\n",
      "200 :  cross entropy loss =  56.4141 ; reg =  38.6395\n",
      "300 :  cross entropy loss =  44.6067 ; reg =  38.6152\n",
      "400 :  cross entropy loss =  74.1121 ; reg =  38.3102\n",
      "293 : train_accuracy:  0.686 ; validation_accuracy:  0.6664\n",
      "294 :  learning rate =  0.0015470475451276038\n",
      "--------------------------\n",
      "0 :  cross entropy loss =  126.896 ; reg =  38.5091\n",
      "100 :  cross entropy loss =  66.487 ; reg =  40.4135\n",
      "200 :  cross entropy loss =  48.3059 ; reg =  40.0786\n",
      "300 :  cross entropy loss =  77.71 ; reg =  39.6961\n",
      "400 :  cross entropy loss =  63.1251 ; reg =  39.5971\n",
      "294 : train_accuracy:  0.7462 ; validation_accuracy:  0.7104\n",
      "295 :  learning rate =  0.0015315770696763278\n",
      "--------------------------\n",
      "0 :  cross entropy loss =  63.0922 ; reg =  39.1818\n",
      "100 :  cross entropy loss =  43.6634 ; reg =  39.8809\n",
      "200 :  cross entropy loss =  69.372 ; reg =  39.2951\n",
      "300 :  cross entropy loss =  70.2747 ; reg =  39.1634\n",
      "400 :  cross entropy loss =  75.7678 ; reg =  38.3281\n",
      "295 : train_accuracy:  0.7318 ; validation_accuracy:  0.7124\n",
      "296 :  learning rate =  0.0015162612989795645\n",
      "--------------------------\n",
      "0 :  cross entropy loss =  77.0801 ; reg =  37.9872\n",
      "100 :  cross entropy loss =  32.8887 ; reg =  38.9934\n",
      "200 :  cross entropy loss =  84.9079 ; reg =  38.7898\n",
      "300 :  cross entropy loss =  66.1676 ; reg =  38.8442\n",
      "400 :  cross entropy loss =  65.4336 ; reg =  38.6196\n",
      "296 : train_accuracy:  0.6984 ; validation_accuracy:  0.6788\n",
      "297 :  learning rate =  0.0015010986859897689\n",
      "--------------------------\n",
      "0 :  cross entropy loss =  151.354 ; reg =  38.6714\n",
      "100 :  cross entropy loss =  41.631 ; reg =  39.1952\n",
      "200 :  cross entropy loss =  52.9458 ; reg =  38.5318\n",
      "300 :  cross entropy loss =  63.9499 ; reg =  37.8209\n",
      "400 :  cross entropy loss =  46.5164 ; reg =  37.2562\n",
      "297 : train_accuracy:  0.7322 ; validation_accuracy:  0.7076\n",
      "298 :  learning rate =  0.0014860876991298713\n",
      "--------------------------\n",
      "0 :  cross entropy loss =  47.1248 ; reg =  36.7739\n",
      "100 :  cross entropy loss =  41.6924 ; reg =  37.3799\n",
      "200 :  cross entropy loss =  69.9294 ; reg =  36.9781\n",
      "300 :  cross entropy loss =  62.8092 ; reg =  37.1667\n",
      "400 :  cross entropy loss =  63.3188 ; reg =  36.7603\n",
      "298 : train_accuracy:  0.699 ; validation_accuracy:  0.698\n",
      "299 :  learning rate =  0.0014712268221385725\n",
      "--------------------------\n",
      "0 :  cross entropy loss =  81.6491 ; reg =  36.3936\n",
      "100 :  cross entropy loss =  71.3493 ; reg =  38.7115\n",
      "200 :  cross entropy loss =  52.1879 ; reg =  37.7915\n",
      "300 :  cross entropy loss =  60.8051 ; reg =  37.5167\n",
      "400 :  cross entropy loss =  65.4121 ; reg =  36.6843\n",
      "299 : train_accuracy:  0.725 ; validation_accuracy:  0.6984\n",
      "300 :  learning rate =  0.0014565145539171868\n",
      "--------------------------\n",
      "0 :  cross entropy loss =  58.7849 ; reg =  36.3919\n",
      "100 :  cross entropy loss =  61.4811 ; reg =  37.5525\n",
      "200 :  cross entropy loss =  46.8571 ; reg =  36.8483\n",
      "300 :  cross entropy loss =  73.4827 ; reg =  36.5011\n",
      "400 :  cross entropy loss =  54.5812 ; reg =  36.1969\n",
      "300 : train_accuracy:  0.7094 ; validation_accuracy:  0.6952\n",
      "301 :  learning rate =  0.0014419494083780149\n",
      "--------------------------\n",
      "0 :  cross entropy loss =  99.6813 ; reg =  36.2414\n",
      "100 :  cross entropy loss =  59.1132 ; reg =  36.7171\n",
      "200 :  cross entropy loss =  80.6896 ; reg =  37.1595\n",
      "300 :  cross entropy loss =  60.3741 ; reg =  37.178\n",
      "400 :  cross entropy loss =  72.729 ; reg =  36.2463\n",
      "301 : train_accuracy:  0.6894 ; validation_accuracy:  0.67\n",
      "302 :  learning rate =  0.0014275299142942348\n",
      "--------------------------\n",
      "0 :  cross entropy loss =  148.918 ; reg =  36.1716\n",
      "100 :  cross entropy loss =  55.9051 ; reg =  38.7912\n",
      "200 :  cross entropy loss =  54.2227 ; reg =  37.7063\n",
      "300 :  cross entropy loss =  45.5601 ; reg =  36.9597\n",
      "400 :  cross entropy loss =  66.1347 ; reg =  36.3489\n",
      "302 : train_accuracy:  0.7052 ; validation_accuracy:  0.6832\n",
      "303 :  learning rate =  0.0014132546151512924\n",
      "--------------------------\n",
      "0 :  cross entropy loss =  77.7303 ; reg =  35.9512\n",
      "100 :  cross entropy loss =  51.8715 ; reg =  36.4818\n",
      "200 :  cross entropy loss =  57.0164 ; reg =  35.7117\n",
      "300 :  cross entropy loss =  61.5828 ; reg =  35.1935\n",
      "400 :  cross entropy loss =  77.5784 ; reg =  35.6237\n",
      "303 : train_accuracy:  0.7324 ; validation_accuracy:  0.6988\n",
      "304 :  learning rate =  0.0013991220689997795\n",
      "--------------------------\n",
      "0 :  cross entropy loss =  26.2443 ; reg =  35.6563\n",
      "100 :  cross entropy loss =  62.0073 ; reg =  35.7359\n",
      "200 :  cross entropy loss =  50.518 ; reg =  35.791\n",
      "300 :  cross entropy loss =  60.6234 ; reg =  35.514\n",
      "400 :  cross entropy loss =  40.1704 ; reg =  34.8506\n",
      "304 : train_accuracy:  0.6524 ; validation_accuracy:  0.624\n",
      "305 :  learning rate =  0.0013851308483097816\n",
      "--------------------------\n",
      "0 :  cross entropy loss =  158.734 ; reg =  34.9775\n",
      "100 :  cross entropy loss =  75.7479 ; reg =  35.4935\n",
      "200 :  cross entropy loss =  42.3944 ; reg =  34.8736\n",
      "300 :  cross entropy loss =  49.862 ; reg =  34.2344\n",
      "400 :  cross entropy loss =  63.047 ; reg =  34.338\n",
      "305 : train_accuracy:  0.6874 ; validation_accuracy:  0.6504\n",
      "306 :  learning rate =  0.0013712795398266838\n",
      "--------------------------\n",
      "0 :  cross entropy loss =  72.1367 ; reg =  34.4078\n",
      "100 :  cross entropy loss =  45.2236 ; reg =  34.5454\n",
      "200 :  cross entropy loss =  35.6314 ; reg =  33.7984\n",
      "300 :  cross entropy loss =  82.0219 ; reg =  33.9136\n",
      "400 :  cross entropy loss =  56.89 ; reg =  33.852\n",
      "306 : train_accuracy:  0.6852 ; validation_accuracy:  0.648\n",
      "307 :  learning rate =  0.0013575667444284169\n",
      "--------------------------\n",
      "0 :  cross entropy loss =  147.08 ; reg =  34.2989\n",
      "100 :  cross entropy loss =  49.2834 ; reg =  34.9045\n",
      "200 :  cross entropy loss =  56.8583 ; reg =  34.647\n",
      "300 :  cross entropy loss =  63.8445 ; reg =  34.1582\n",
      "400 :  cross entropy loss =  48.7118 ; reg =  33.8655\n",
      "307 : train_accuracy:  0.707 ; validation_accuracy:  0.7048\n",
      "308 :  learning rate =  0.0013439910769841327\n",
      "--------------------------\n",
      "0 :  cross entropy loss =  81.7905 ; reg =  33.66\n",
      "100 :  cross entropy loss =  76.5393 ; reg =  34.6424\n",
      "200 :  cross entropy loss =  57.4871 ; reg =  34.3347\n",
      "300 :  cross entropy loss =  29.4975 ; reg =  34.0714\n",
      "400 :  cross entropy loss =  48.5989 ; reg =  33.9168\n",
      "308 : train_accuracy:  0.7052 ; validation_accuracy:  0.694\n",
      "309 :  learning rate =  0.0013305511662142914\n",
      "--------------------------\n",
      "0 :  cross entropy loss =  81.015 ; reg =  33.6203\n",
      "100 :  cross entropy loss =  46.7321 ; reg =  34.9039\n",
      "200 :  cross entropy loss =  50.5018 ; reg =  33.9799\n",
      "300 :  cross entropy loss =  57.7311 ; reg =  33.8081\n",
      "400 :  cross entropy loss =  64.6961 ; reg =  33.1714\n",
      "309 : train_accuracy:  0.7538 ; validation_accuracy:  0.7184\n",
      "310 :  learning rate =  0.0013172456545521485\n",
      "--------------------------\n",
      "0 :  cross entropy loss =  45.3222 ; reg =  32.9599\n",
      "100 :  cross entropy loss =  61.1818 ; reg =  32.5736\n",
      "200 :  cross entropy loss =  35.0006 ; reg =  32.1073\n",
      "300 :  cross entropy loss =  62.1792 ; reg =  32.6255\n",
      "400 :  cross entropy loss =  38.4817 ; reg =  32.724\n",
      "310 : train_accuracy:  0.667 ; validation_accuracy:  0.6484\n",
      "311 :  learning rate =  0.001304073198006627\n",
      "--------------------------\n",
      "0 :  cross entropy loss =  80.0557 ; reg =  32.8152\n",
      "100 :  cross entropy loss =  49.7665 ; reg =  33.2558\n",
      "200 :  cross entropy loss =  44.8569 ; reg =  32.8876\n",
      "300 :  cross entropy loss =  71.976 ; reg =  32.8425\n",
      "400 :  cross entropy loss =  83.3828 ; reg =  32.5122\n",
      "311 : train_accuracy:  0.6966 ; validation_accuracy:  0.6664\n",
      "312 :  learning rate =  0.0012910324660265606\n",
      "--------------------------\n",
      "0 :  cross entropy loss =  112.464 ; reg =  32.9092\n",
      "100 :  cross entropy loss =  41.5386 ; reg =  34.2012\n",
      "200 :  cross entropy loss =  57.467 ; reg =  33.3364\n",
      "300 :  cross entropy loss =  69.6772 ; reg =  32.9091\n",
      "400 :  cross entropy loss =  47.5207 ; reg =  32.6303\n",
      "312 : train_accuracy:  0.7402 ; validation_accuracy:  0.7096\n",
      "313 :  learning rate =  0.001278122141366295\n",
      "--------------------------\n",
      "0 :  cross entropy loss =  32.8175 ; reg =  32.115\n",
      "100 :  cross entropy loss =  39.1499 ; reg =  31.9609\n",
      "200 :  cross entropy loss =  47.8211 ; reg =  31.8965\n",
      "300 :  cross entropy loss =  59.0589 ; reg =  31.8407\n",
      "400 :  cross entropy loss =  29.7016 ; reg =  31.7708\n",
      "313 : train_accuracy:  0.7032 ; validation_accuracy:  0.6712\n",
      "314 :  learning rate =  0.001265340919952632\n",
      "--------------------------\n",
      "0 :  cross entropy loss =  96.6151 ; reg =  31.5683\n",
      "100 :  cross entropy loss =  38.1807 ; reg =  32.6958\n",
      "200 :  cross entropy loss =  58.4153 ; reg =  32.1153\n",
      "300 :  cross entropy loss =  40.4379 ; reg =  31.9764\n",
      "400 :  cross entropy loss =  47.1867 ; reg =  31.8952\n",
      "314 : train_accuracy:  0.692 ; validation_accuracy:  0.6736\n",
      "315 :  learning rate =  0.0012526875107531058\n",
      "--------------------------\n",
      "0 :  cross entropy loss =  54.6527 ; reg =  31.6831\n",
      "100 :  cross entropy loss =  42.1152 ; reg =  31.6621\n",
      "200 :  cross entropy loss =  53.986 ; reg =  31.4725\n",
      "300 :  cross entropy loss =  56.1818 ; reg =  31.4566\n",
      "400 :  cross entropy loss =  39.28 ; reg =  30.914\n",
      "315 : train_accuracy:  0.7212 ; validation_accuracy:  0.6976\n",
      "316 :  learning rate =  0.0012401606356455747\n",
      "--------------------------\n",
      "0 :  cross entropy loss =  39.3095 ; reg =  30.8468\n",
      "100 :  cross entropy loss =  32.065 ; reg =  31.0049\n",
      "200 :  cross entropy loss =  46.902 ; reg =  31.2436\n",
      "300 :  cross entropy loss =  67.2121 ; reg =  30.692\n",
      "400 :  cross entropy loss =  45.1833 ; reg =  31.2635\n",
      "316 : train_accuracy:  0.7198 ; validation_accuracy:  0.6836\n",
      "317 :  learning rate =  0.001227759029289119\n",
      "--------------------------\n",
      "0 :  cross entropy loss =  78.5654 ; reg =  31.3909\n",
      "100 :  cross entropy loss =  46.8807 ; reg =  31.4327\n",
      "200 :  cross entropy loss =  38.8429 ; reg =  30.7524\n",
      "300 :  cross entropy loss =  55.5571 ; reg =  30.4382\n",
      "400 :  cross entropy loss =  37.9888 ; reg =  30.606\n",
      "317 : train_accuracy:  0.719 ; validation_accuracy:  0.6976\n",
      "318 :  learning rate =  0.0012154814389962279\n",
      "--------------------------\n",
      "0 :  cross entropy loss =  40.0733 ; reg =  30.3988\n",
      "100 :  cross entropy loss =  33.2126 ; reg =  30.731\n",
      "200 :  cross entropy loss =  48.8458 ; reg =  30.3257\n",
      "300 :  cross entropy loss =  49.8373 ; reg =  30.6009\n",
      "400 :  cross entropy loss =  45.4566 ; reg =  30.6432\n",
      "318 : train_accuracy:  0.7242 ; validation_accuracy:  0.6936\n",
      "319 :  learning rate =  0.0012033266246062656\n",
      "--------------------------\n",
      "0 :  cross entropy loss =  105.098 ; reg =  30.4605\n",
      "100 :  cross entropy loss =  30.3321 ; reg =  31.4202\n",
      "200 :  cross entropy loss =  38.8327 ; reg =  31.0309\n",
      "300 :  cross entropy loss =  36.3102 ; reg =  30.3355\n",
      "400 :  cross entropy loss =  43.8692 ; reg =  30.0252\n",
      "319 : train_accuracy:  0.7216 ; validation_accuracy:  0.692\n",
      "320 :  learning rate =  0.001191293358360203\n",
      "--------------------------\n",
      "0 :  cross entropy loss =  90.1729 ; reg =  29.8771\n",
      "100 :  cross entropy loss =  46.7242 ; reg =  31.1244\n",
      "200 :  cross entropy loss =  63.1848 ; reg =  30.5723\n",
      "300 :  cross entropy loss =  62.0283 ; reg =  30.4702\n",
      "400 :  cross entropy loss =  58.247 ; reg =  30.1742\n",
      "320 : train_accuracy:  0.6934 ; validation_accuracy:  0.6612\n",
      "321 :  learning rate =  0.0011793804247766009\n",
      "--------------------------\n",
      "0 :  cross entropy loss =  63.6333 ; reg =  29.9868\n",
      "100 :  cross entropy loss =  39.4695 ; reg =  30.7763\n",
      "200 :  cross entropy loss =  40.8464 ; reg =  29.8769\n",
      "300 :  cross entropy loss =  35.4854 ; reg =  29.7133\n",
      "400 :  cross entropy loss =  43.6384 ; reg =  29.5216\n",
      "321 : train_accuracy:  0.7298 ; validation_accuracy:  0.7088\n",
      "322 :  learning rate =  0.0011675866205288349\n",
      "--------------------------\n",
      "0 :  cross entropy loss =  64.9965 ; reg =  29.3242\n",
      "100 :  cross entropy loss =  46.8616 ; reg =  29.0241\n",
      "200 :  cross entropy loss =  14.4419 ; reg =  28.7758\n",
      "300 :  cross entropy loss =  31.9866 ; reg =  28.3142\n",
      "400 :  cross entropy loss =  50.136 ; reg =  28.3808\n",
      "322 : train_accuracy:  0.702 ; validation_accuracy:  0.6888\n",
      "323 :  learning rate =  0.0011559107543235466\n",
      "--------------------------\n",
      "0 :  cross entropy loss =  40.6219 ; reg =  28.2527\n",
      "100 :  cross entropy loss =  27.799 ; reg =  27.9988\n",
      "200 :  cross entropy loss =  26.101 ; reg =  28.264\n",
      "300 :  cross entropy loss =  59.7263 ; reg =  28.4744\n",
      "400 :  cross entropy loss =  33.1349 ; reg =  28.3721\n",
      "323 : train_accuracy:  0.731 ; validation_accuracy:  0.708\n",
      "324 :  learning rate =  0.001144351646780311\n",
      "--------------------------\n",
      "0 :  cross entropy loss =  42.059 ; reg =  28.4245\n",
      "100 :  cross entropy loss =  55.5957 ; reg =  28.5002\n",
      "200 :  cross entropy loss =  32.026 ; reg =  28.4387\n",
      "300 :  cross entropy loss =  33.2486 ; reg =  28.216\n",
      "400 :  cross entropy loss =  28.1822 ; reg =  28.6489\n",
      "324 : train_accuracy:  0.7304 ; validation_accuracy:  0.7068\n",
      "325 :  learning rate =  0.001132908130312508\n",
      "--------------------------\n",
      "0 :  cross entropy loss =  58.1293 ; reg =  28.4684\n",
      "100 :  cross entropy loss =  45.6989 ; reg =  28.6373\n",
      "200 :  cross entropy loss =  36.7491 ; reg =  28.3976\n",
      "300 :  cross entropy loss =  48.5535 ; reg =  28.078\n",
      "400 :  cross entropy loss =  24.9665 ; reg =  27.9573\n",
      "325 : train_accuracy:  0.7272 ; validation_accuracy:  0.7144\n",
      "326 :  learning rate =  0.0011215790490093829\n",
      "--------------------------\n",
      "0 :  cross entropy loss =  68.2924 ; reg =  27.8549\n",
      "100 :  cross entropy loss =  39.1808 ; reg =  27.7969\n",
      "200 :  cross entropy loss =  40.3325 ; reg =  27.4778\n",
      "300 :  cross entropy loss =  42.2554 ; reg =  27.566\n",
      "400 :  cross entropy loss =  38.6638 ; reg =  27.3462\n",
      "326 : train_accuracy:  0.7054 ; validation_accuracy:  0.6704\n",
      "327 :  learning rate =  0.001110363258519289\n",
      "--------------------------\n",
      "0 :  cross entropy loss =  74.0363 ; reg =  27.1925\n",
      "100 :  cross entropy loss =  42.7558 ; reg =  28.4111\n",
      "200 :  cross entropy loss =  53.35 ; reg =  28.1787\n",
      "300 :  cross entropy loss =  46.5914 ; reg =  28.2454\n",
      "400 :  cross entropy loss =  58.6659 ; reg =  28.1291\n",
      "327 : train_accuracy:  0.7778 ; validation_accuracy:  0.7332\n",
      "328 :  learning rate =  0.001099259625934096\n",
      "--------------------------\n",
      "0 :  cross entropy loss =  42.4126 ; reg =  28.0113\n",
      "100 :  cross entropy loss =  28.3996 ; reg =  28.3339\n",
      "200 :  cross entropy loss =  40.815 ; reg =  28.1902\n",
      "300 :  cross entropy loss =  48.0237 ; reg =  28.0191\n",
      "400 :  cross entropy loss =  56.8855 ; reg =  27.8101\n",
      "328 : train_accuracy:  0.7448 ; validation_accuracy:  0.7156\n",
      "329 :  learning rate =  0.001088267029674755\n",
      "--------------------------\n",
      "0 :  cross entropy loss =  44.1541 ; reg =  27.554\n",
      "100 :  cross entropy loss =  29.4361 ; reg =  27.9456\n",
      "200 :  cross entropy loss =  35.8774 ; reg =  27.6212\n",
      "300 :  cross entropy loss =  41.3536 ; reg =  27.5394\n",
      "400 :  cross entropy loss =  63.5322 ; reg =  27.6649\n",
      "329 : train_accuracy:  0.7334 ; validation_accuracy:  0.7104\n",
      "330 :  learning rate =  0.0010773843593780074\n",
      "--------------------------\n",
      "0 :  cross entropy loss =  43.5539 ; reg =  27.4942\n",
      "100 :  cross entropy loss =  37.7081 ; reg =  27.3958\n",
      "200 :  cross entropy loss =  32.6597 ; reg =  27.1393\n",
      "300 :  cross entropy loss =  34.1008 ; reg =  26.9814\n",
      "400 :  cross entropy loss =  30.0081 ; reg =  26.7013\n",
      "330 : train_accuracy:  0.6864 ; validation_accuracy:  0.6672\n",
      "331 :  learning rate =  0.0010666105157842273\n",
      "--------------------------\n",
      "0 :  cross entropy loss =  146.946 ; reg =  26.5643\n",
      "100 :  cross entropy loss =  38.8799 ; reg =  27.3364\n",
      "200 :  cross entropy loss =  48.6048 ; reg =  27.3609\n",
      "300 :  cross entropy loss =  35.5365 ; reg =  27.2121\n",
      "400 :  cross entropy loss =  17.4362 ; reg =  27.0196\n",
      "331 : train_accuracy:  0.7726 ; validation_accuracy:  0.7484\n",
      "332 :  learning rate =  0.001055944410626385\n",
      "--------------------------\n",
      "0 :  cross entropy loss =  29.3806 ; reg =  26.7599\n",
      "100 :  cross entropy loss =  26.7235 ; reg =  26.6999\n",
      "200 :  cross entropy loss =  43.6912 ; reg =  26.2001\n",
      "300 :  cross entropy loss =  30.4112 ; reg =  26.3789\n",
      "400 :  cross entropy loss =  50.9935 ; reg =  26.0681\n",
      "332 : train_accuracy:  0.7484 ; validation_accuracy:  0.6956\n",
      "333 :  learning rate =  0.001045384966520121\n",
      "--------------------------\n",
      "0 :  cross entropy loss =  52.5921 ; reg =  26.101\n",
      "100 :  cross entropy loss =  29.2977 ; reg =  26.3131\n",
      "200 :  cross entropy loss =  36.2172 ; reg =  26.1188\n",
      "300 :  cross entropy loss =  25.9307 ; reg =  26.3086\n",
      "400 :  cross entropy loss =  47.8252 ; reg =  25.8503\n",
      "333 : train_accuracy:  0.7534 ; validation_accuracy:  0.7252\n",
      "334 :  learning rate =  0.0010349311168549198\n",
      "--------------------------\n",
      "0 :  cross entropy loss =  57.6082 ; reg =  25.9151\n",
      "100 :  cross entropy loss =  44.8869 ; reg =  26.7103\n",
      "200 :  cross entropy loss =  41.3001 ; reg =  26.4376\n",
      "300 :  cross entropy loss =  34.626 ; reg =  26.4248\n",
      "400 :  cross entropy loss =  47.8927 ; reg =  26.1849\n",
      "334 : train_accuracy:  0.7372 ; validation_accuracy:  0.7124\n",
      "335 :  learning rate =  0.0010245818056863706\n",
      "--------------------------\n",
      "0 :  cross entropy loss =  68.8244 ; reg =  26.1208\n",
      "100 :  cross entropy loss =  27.5787 ; reg =  26.4595\n",
      "200 :  cross entropy loss =  27.9145 ; reg =  25.7805\n",
      "300 :  cross entropy loss =  25.9099 ; reg =  25.591\n",
      "400 :  cross entropy loss =  35.1289 ; reg =  25.6578\n",
      "335 : train_accuracy:  0.7446 ; validation_accuracy:  0.7224\n",
      "336 :  learning rate =  0.001014335987629507\n",
      "--------------------------\n",
      "0 :  cross entropy loss =  53.2848 ; reg =  25.5797\n",
      "100 :  cross entropy loss =  26.9617 ; reg =  25.6026\n",
      "200 :  cross entropy loss =  27.3944 ; reg =  25.2627\n",
      "300 :  cross entropy loss =  36.2879 ; reg =  25.2954\n",
      "400 :  cross entropy loss =  24.1229 ; reg =  24.9773\n",
      "336 : train_accuracy:  0.7442 ; validation_accuracy:  0.7148\n",
      "337 :  learning rate =  0.001004192627753212\n",
      "--------------------------\n",
      "0 :  cross entropy loss =  40.9459 ; reg =  24.9629\n",
      "100 :  cross entropy loss =  28.2671 ; reg =  25.3651\n",
      "200 :  cross entropy loss =  31.3244 ; reg =  25.158\n",
      "300 :  cross entropy loss =  35.0541 ; reg =  24.9437\n",
      "400 :  cross entropy loss =  40.9713 ; reg =  24.8558\n",
      "337 : train_accuracy:  0.775 ; validation_accuracy:  0.7404\n",
      "338 :  learning rate =  0.0009941507014756797\n",
      "--------------------------\n",
      "0 :  cross entropy loss =  30.7989 ; reg =  24.8284\n",
      "100 :  cross entropy loss =  36.0648 ; reg =  24.688\n",
      "200 :  cross entropy loss =  26.5555 ; reg =  24.1623\n",
      "300 :  cross entropy loss =  33.4737 ; reg =  24.4132\n",
      "400 :  cross entropy loss =  47.6217 ; reg =  24.1522\n",
      "338 : train_accuracy:  0.7228 ; validation_accuracy:  0.6912\n",
      "339 :  learning rate =  0.0009941507014756797\n",
      "--------------------------\n",
      "0 :  cross entropy loss =  59.9408 ; reg =  24.1932\n",
      "100 :  cross entropy loss =  24.4386 ; reg =  25.1845\n",
      "200 :  cross entropy loss =  35.5883 ; reg =  24.8921\n",
      "300 :  cross entropy loss =  24.983 ; reg =  24.7913\n",
      "400 :  cross entropy loss =  39.3525 ; reg =  24.6634\n",
      "339 : train_accuracy:  0.73 ; validation_accuracy:  0.6752\n",
      "340 :  learning rate =  0.0009941507014756797\n",
      "--------------------------\n",
      "0 :  cross entropy loss =  31.434 ; reg =  24.7055\n",
      "100 :  cross entropy loss =  22.4505 ; reg =  25.4932\n",
      "200 :  cross entropy loss =  25.791 ; reg =  25.1886\n",
      "300 :  cross entropy loss =  45.8349 ; reg =  25.0167\n",
      "400 :  cross entropy loss =  27.1656 ; reg =  24.684\n",
      "340 : train_accuracy:  0.7452 ; validation_accuracy:  0.7216\n",
      "341 :  learning rate =  0.0009941507014756797\n",
      "--------------------------\n",
      "0 :  cross entropy loss =  41.2766 ; reg =  24.4851\n",
      "100 :  cross entropy loss =  41.6232 ; reg =  24.5615\n",
      "200 :  cross entropy loss =  30.0414 ; reg =  24.4746\n",
      "300 :  cross entropy loss =  40.4757 ; reg =  24.6864\n",
      "400 :  cross entropy loss =  40.5411 ; reg =  24.907\n",
      "341 : train_accuracy:  0.7638 ; validation_accuracy:  0.7576\n",
      "342 :  learning rate =  0.0009941507014756797\n",
      "--------------------------\n",
      "0 :  cross entropy loss =  28.7339 ; reg =  24.8469\n",
      "100 :  cross entropy loss =  31.4453 ; reg =  24.9773\n",
      "200 :  cross entropy loss =  35.5619 ; reg =  24.9056\n",
      "300 :  cross entropy loss =  39.5784 ; reg =  24.9204\n",
      "400 :  cross entropy loss =  14.3789 ; reg =  24.7865\n",
      "342 : train_accuracy:  0.7538 ; validation_accuracy:  0.7148\n",
      "343 :  learning rate =  0.0009941507014756797\n",
      "--------------------------\n",
      "0 :  cross entropy loss =  42.887 ; reg =  24.6762\n",
      "100 :  cross entropy loss =  37.5766 ; reg =  24.7138\n",
      "200 :  cross entropy loss =  32.8419 ; reg =  24.6343\n",
      "300 :  cross entropy loss =  34.926 ; reg =  24.7838\n",
      "400 :  cross entropy loss =  47.2363 ; reg =  24.8849\n",
      "343 : train_accuracy:  0.746 ; validation_accuracy:  0.7108\n",
      "344 :  learning rate =  0.0009941507014756797\n",
      "--------------------------\n",
      "0 :  cross entropy loss =  68.7907 ; reg =  24.8769\n",
      "100 :  cross entropy loss =  20.7593 ; reg =  25.0262\n",
      "200 :  cross entropy loss =  33.5776 ; reg =  24.8778\n",
      "300 :  cross entropy loss =  35.5678 ; reg =  24.9814\n",
      "400 :  cross entropy loss =  33.4864 ; reg =  24.823\n",
      "344 : train_accuracy:  0.7708 ; validation_accuracy:  0.7328\n",
      "345 :  learning rate =  0.0009941507014756797\n",
      "--------------------------\n",
      "0 :  cross entropy loss =  40.0831 ; reg =  24.7359\n",
      "100 :  cross entropy loss =  59.5004 ; reg =  25.4007\n",
      "200 :  cross entropy loss =  47.3082 ; reg =  25.3393\n",
      "300 :  cross entropy loss =  31.6309 ; reg =  25.1462\n",
      "400 :  cross entropy loss =  28.6583 ; reg =  24.8065\n",
      "345 : train_accuracy:  0.7536 ; validation_accuracy:  0.718\n",
      "346 :  learning rate =  0.0009941507014756797\n",
      "--------------------------\n",
      "0 :  cross entropy loss =  59.872 ; reg =  24.6281\n",
      "100 :  cross entropy loss =  28.2928 ; reg =  25.0082\n",
      "200 :  cross entropy loss =  37.3819 ; reg =  24.9378\n",
      "300 :  cross entropy loss =  14.5506 ; reg =  24.7055\n",
      "400 :  cross entropy loss =  29.4599 ; reg =  24.4528\n",
      "346 : train_accuracy:  0.734 ; validation_accuracy:  0.6952\n",
      "347 :  learning rate =  0.0009941507014756797\n",
      "--------------------------\n",
      "0 :  cross entropy loss =  35.2109 ; reg =  24.3081\n",
      "100 :  cross entropy loss =  43.8366 ; reg =  24.5084\n",
      "200 :  cross entropy loss =  23.197 ; reg =  24.8473\n",
      "300 :  cross entropy loss =  24.0628 ; reg =  24.6208\n",
      "400 :  cross entropy loss =  30.0197 ; reg =  24.3736\n",
      "347 : train_accuracy:  0.7418 ; validation_accuracy:  0.7152\n",
      "348 :  learning rate =  0.0009941507014756797\n",
      "--------------------------\n",
      "0 :  cross entropy loss =  44.7246 ; reg =  24.2075\n",
      "100 :  cross entropy loss =  40.6042 ; reg =  24.6009\n",
      "200 :  cross entropy loss =  60.0016 ; reg =  24.5409\n",
      "300 :  cross entropy loss =  34.2229 ; reg =  24.599\n",
      "400 :  cross entropy loss =  25.3611 ; reg =  24.913\n",
      "348 : train_accuracy:  0.7228 ; validation_accuracy:  0.6972\n",
      "349 :  learning rate =  0.0009941507014756797\n",
      "--------------------------\n",
      "0 :  cross entropy loss =  65.1245 ; reg =  24.8109\n",
      "100 :  cross entropy loss =  36.0206 ; reg =  25.4156\n",
      "200 :  cross entropy loss =  61.7399 ; reg =  25.2209\n",
      "300 :  cross entropy loss =  27.6771 ; reg =  24.803\n",
      "400 :  cross entropy loss =  32.883 ; reg =  24.5448\n",
      "349 : train_accuracy:  0.7138 ; validation_accuracy:  0.6732\n",
      "350 :  learning rate =  0.0009941507014756797\n",
      "--------------------------\n",
      "0 :  cross entropy loss =  66.6813 ; reg =  24.433\n",
      "100 :  cross entropy loss =  34.0597 ; reg =  24.9898\n",
      "200 :  cross entropy loss =  21.9333 ; reg =  24.6701\n",
      "300 :  cross entropy loss =  54.1965 ; reg =  24.2176\n",
      "400 :  cross entropy loss =  39.5316 ; reg =  24.1579\n",
      "350 : train_accuracy:  0.7802 ; validation_accuracy:  0.7316\n",
      "351 :  learning rate =  0.0009941507014756797\n",
      "--------------------------\n",
      "0 :  cross entropy loss =  27.6119 ; reg =  24.143\n",
      "100 :  cross entropy loss =  33.2671 ; reg =  23.9289\n",
      "200 :  cross entropy loss =  34.1206 ; reg =  24.0312\n",
      "300 :  cross entropy loss =  55.3739 ; reg =  24.2708\n",
      "400 :  cross entropy loss =  22.5866 ; reg =  24.0816\n",
      "351 : train_accuracy:  0.6654 ; validation_accuracy:  0.6604\n",
      "352 :  learning rate =  0.0009941507014756797\n",
      "--------------------------\n",
      "0 :  cross entropy loss =  71.6847 ; reg =  24.3931\n",
      "100 :  cross entropy loss =  33.5159 ; reg =  25.3128\n",
      "200 :  cross entropy loss =  45.8699 ; reg =  25.0285\n",
      "300 :  cross entropy loss =  36.5058 ; reg =  24.7501\n",
      "400 :  cross entropy loss =  42.348 ; reg =  24.5725\n",
      "352 : train_accuracy:  0.7684 ; validation_accuracy:  0.722\n",
      "353 :  learning rate =  0.0009941507014756797\n",
      "--------------------------\n",
      "0 :  cross entropy loss =  38.4337 ; reg =  24.5948\n",
      "100 :  cross entropy loss =  35.5817 ; reg =  25.4277\n",
      "200 :  cross entropy loss =  35.5732 ; reg =  25.242\n",
      "300 :  cross entropy loss =  19.0672 ; reg =  25.1497\n",
      "400 :  cross entropy loss =  24.2624 ; reg =  25.2715\n",
      "353 : train_accuracy:  0.7524 ; validation_accuracy:  0.7232\n",
      "354 :  learning rate =  0.0009941507014756797\n",
      "--------------------------\n",
      "0 :  cross entropy loss =  29.8607 ; reg =  25.4407\n",
      "100 :  cross entropy loss =  39.8318 ; reg =  25.1853\n",
      "200 :  cross entropy loss =  31.9107 ; reg =  24.848\n",
      "300 :  cross entropy loss =  38.6486 ; reg =  24.6415\n",
      "400 :  cross entropy loss =  44.3978 ; reg =  24.5354\n",
      "354 : train_accuracy:  0.7472 ; validation_accuracy:  0.7104\n",
      "355 :  learning rate =  0.0009941507014756797\n",
      "--------------------------\n",
      "0 :  cross entropy loss =  61.1877 ; reg =  24.6229\n",
      "100 :  cross entropy loss =  30.5201 ; reg =  25.1251\n",
      "200 :  cross entropy loss =  36.5722 ; reg =  24.8348\n",
      "300 :  cross entropy loss =  52.7669 ; reg =  24.6113\n",
      "400 :  cross entropy loss =  22.6193 ; reg =  24.2979\n",
      "355 : train_accuracy:  0.705 ; validation_accuracy:  0.6844\n",
      "356 :  learning rate =  0.0009941507014756797\n",
      "--------------------------\n",
      "0 :  cross entropy loss =  127.009 ; reg =  24.1892\n",
      "100 :  cross entropy loss =  18.85 ; reg =  25.4423\n",
      "200 :  cross entropy loss =  27.2771 ; reg =  25.0996\n",
      "300 :  cross entropy loss =  42.732 ; reg =  24.7086\n",
      "400 :  cross entropy loss =  24.7449 ; reg =  24.7259\n",
      "356 : train_accuracy:  0.7664 ; validation_accuracy:  0.7276\n",
      "357 :  learning rate =  0.0009941507014756797\n",
      "--------------------------\n",
      "0 :  cross entropy loss =  33.7675 ; reg =  24.7262\n",
      "100 :  cross entropy loss =  22.2656 ; reg =  24.5442\n",
      "200 :  cross entropy loss =  19.4515 ; reg =  24.3951\n",
      "300 :  cross entropy loss =  33.2424 ; reg =  24.2935\n",
      "400 :  cross entropy loss =  21.4645 ; reg =  24.3113\n",
      "357 : train_accuracy:  0.7364 ; validation_accuracy:  0.704\n",
      "358 :  learning rate =  0.0009941507014756797\n",
      "--------------------------\n",
      "0 :  cross entropy loss =  69.79 ; reg =  24.2905\n",
      "100 :  cross entropy loss =  37.9075 ; reg =  24.6519\n",
      "200 :  cross entropy loss =  35.3164 ; reg =  24.4861\n",
      "300 :  cross entropy loss =  46.5556 ; reg =  24.2128\n",
      "400 :  cross entropy loss =  41.1491 ; reg =  24.2352\n",
      "358 : train_accuracy:  0.7038 ; validation_accuracy:  0.682\n",
      "359 :  learning rate =  0.0009941507014756797\n",
      "--------------------------\n",
      "0 :  cross entropy loss =  50.808 ; reg =  24.1035\n",
      "100 :  cross entropy loss =  15.3573 ; reg =  24.4378\n",
      "200 :  cross entropy loss =  41.5823 ; reg =  24.4005\n",
      "300 :  cross entropy loss =  44.0205 ; reg =  24.313\n",
      "400 :  cross entropy loss =  43.3696 ; reg =  24.3891\n",
      "359 : train_accuracy:  0.7396 ; validation_accuracy:  0.6836\n",
      "360 :  learning rate =  0.0009941507014756797\n",
      "--------------------------\n",
      "0 :  cross entropy loss =  59.6136 ; reg =  24.3449\n",
      "100 :  cross entropy loss =  36.3537 ; reg =  24.9346\n",
      "200 :  cross entropy loss =  22.0944 ; reg =  24.8739\n",
      "300 :  cross entropy loss =  33.2403 ; reg =  24.7318\n",
      "400 :  cross entropy loss =  44.0547 ; reg =  24.4238\n",
      "360 : train_accuracy:  0.7602 ; validation_accuracy:  0.734\n",
      "361 :  learning rate =  0.0009941507014756797\n",
      "--------------------------\n",
      "0 :  cross entropy loss =  49.1222 ; reg =  24.439\n",
      "100 :  cross entropy loss =  27.0697 ; reg =  24.6135\n",
      "200 :  cross entropy loss =  28.7085 ; reg =  24.6225\n",
      "300 :  cross entropy loss =  23.6823 ; reg =  24.5722\n",
      "400 :  cross entropy loss =  40.8833 ; reg =  24.3975\n",
      "361 : train_accuracy:  0.7042 ; validation_accuracy:  0.6768\n",
      "362 :  learning rate =  0.0009941507014756797\n",
      "--------------------------\n",
      "0 :  cross entropy loss =  85.9629 ; reg =  24.4264\n",
      "100 :  cross entropy loss =  39.7605 ; reg =  24.7879\n",
      "200 :  cross entropy loss =  43.8555 ; reg =  24.7341\n",
      "300 :  cross entropy loss =  30.677 ; reg =  24.8597\n",
      "400 :  cross entropy loss =  17.734 ; reg =  24.7119\n",
      "362 : train_accuracy:  0.7476 ; validation_accuracy:  0.7128\n",
      "363 :  learning rate =  0.0009941507014756797\n",
      "--------------------------\n",
      "0 :  cross entropy loss =  43.8971 ; reg =  24.5859\n",
      "100 :  cross entropy loss =  23.6075 ; reg =  25.0834\n",
      "200 :  cross entropy loss =  35.1053 ; reg =  24.812\n",
      "300 :  cross entropy loss =  33.3743 ; reg =  24.8116\n",
      "400 :  cross entropy loss =  63.0018 ; reg =  24.9678\n",
      "363 : train_accuracy:  0.7294 ; validation_accuracy:  0.682\n",
      "364 :  learning rate =  0.0009941507014756797\n",
      "--------------------------\n",
      "0 :  cross entropy loss =  61.5837 ; reg =  25.1289\n",
      "100 :  cross entropy loss =  32.5437 ; reg =  25.7262\n",
      "200 :  cross entropy loss =  33.5512 ; reg =  25.2364\n",
      "300 :  cross entropy loss =  52.1613 ; reg =  25.2995\n",
      "400 :  cross entropy loss =  44.0794 ; reg =  25.123\n",
      "364 : train_accuracy:  0.7504 ; validation_accuracy:  0.72\n",
      "365 :  learning rate =  0.0009941507014756797\n",
      "--------------------------\n",
      "0 :  cross entropy loss =  42.7551 ; reg =  25.009\n",
      "100 :  cross entropy loss =  38.4907 ; reg =  25.517\n",
      "200 :  cross entropy loss =  43.7676 ; reg =  24.9888\n",
      "300 :  cross entropy loss =  38.1189 ; reg =  24.727\n",
      "400 :  cross entropy loss =  33.5056 ; reg =  24.8008\n",
      "365 : train_accuracy:  0.7612 ; validation_accuracy:  0.7276\n",
      "366 :  learning rate =  0.0009941507014756797\n",
      "--------------------------\n",
      "0 :  cross entropy loss =  43.8416 ; reg =  24.6923\n",
      "100 :  cross entropy loss =  27.5856 ; reg =  25.2285\n",
      "200 :  cross entropy loss =  26.8964 ; reg =  24.9857\n",
      "300 :  cross entropy loss =  53.7993 ; reg =  24.8095\n",
      "400 :  cross entropy loss =  44.8264 ; reg =  24.513\n",
      "366 : train_accuracy:  0.7362 ; validation_accuracy:  0.6916\n",
      "367 :  learning rate =  0.0009941507014756797\n",
      "--------------------------\n",
      "0 :  cross entropy loss =  53.8071 ; reg =  24.5603\n",
      "100 :  cross entropy loss =  28.9249 ; reg =  25.2309\n",
      "200 :  cross entropy loss =  34.1605 ; reg =  24.9581\n",
      "300 :  cross entropy loss =  21.7478 ; reg =  24.9412\n",
      "400 :  cross entropy loss =  43.0422 ; reg =  24.639\n",
      "367 : train_accuracy:  0.7374 ; validation_accuracy:  0.7096\n",
      "368 :  learning rate =  0.0009941507014756797\n",
      "--------------------------\n",
      "0 :  cross entropy loss =  59.1103 ; reg =  24.4026\n",
      "100 :  cross entropy loss =  44.5781 ; reg =  24.5942\n",
      "200 :  cross entropy loss =  43.8621 ; reg =  24.3732\n",
      "300 :  cross entropy loss =  42.2677 ; reg =  24.4279\n",
      "400 :  cross entropy loss =  25.1181 ; reg =  24.2852\n",
      "368 : train_accuracy:  0.747 ; validation_accuracy:  0.7076\n",
      "369 :  learning rate =  0.0009941507014756797\n",
      "--------------------------\n",
      "0 :  cross entropy loss =  40.7883 ; reg =  24.4144\n",
      "100 :  cross entropy loss =  43.5013 ; reg =  24.4903\n",
      "200 :  cross entropy loss =  21.2237 ; reg =  24.2832\n",
      "300 :  cross entropy loss =  28.2103 ; reg =  24.0561\n",
      "400 :  cross entropy loss =  39.4694 ; reg =  24.2519\n",
      "369 : train_accuracy:  0.7562 ; validation_accuracy:  0.7188\n",
      "370 :  learning rate =  0.0009941507014756797\n",
      "--------------------------\n",
      "0 :  cross entropy loss =  39.8783 ; reg =  24.3716\n",
      "100 :  cross entropy loss =  30.8224 ; reg =  24.4881\n",
      "200 :  cross entropy loss =  41.5184 ; reg =  24.2411\n",
      "300 :  cross entropy loss =  33.9166 ; reg =  24.5422\n",
      "400 :  cross entropy loss =  16.4806 ; reg =  24.3972\n",
      "370 : train_accuracy:  0.6808 ; validation_accuracy:  0.6524\n",
      "371 :  learning rate =  0.0009941507014756797\n",
      "--------------------------\n",
      "0 :  cross entropy loss =  42.6022 ; reg =  24.4565\n",
      "100 :  cross entropy loss =  33.017 ; reg =  25.2835\n",
      "200 :  cross entropy loss =  48.8358 ; reg =  25.0784\n",
      "300 :  cross entropy loss =  38.3756 ; reg =  24.7825\n",
      "400 :  cross entropy loss =  21.7093 ; reg =  24.5077\n",
      "371 : train_accuracy:  0.6928 ; validation_accuracy:  0.6568\n",
      "372 :  learning rate =  0.0009941507014756797\n",
      "--------------------------\n",
      "0 :  cross entropy loss =  97.2227 ; reg =  24.715\n",
      "100 :  cross entropy loss =  24.6239 ; reg =  25.9564\n",
      "200 :  cross entropy loss =  33.7799 ; reg =  25.6215\n",
      "300 :  cross entropy loss =  31.3445 ; reg =  25.0398\n",
      "400 :  cross entropy loss =  49.8611 ; reg =  24.5352\n",
      "372 : train_accuracy:  0.7368 ; validation_accuracy:  0.7068\n",
      "373 :  learning rate =  0.0009941507014756797\n",
      "--------------------------\n",
      "0 :  cross entropy loss =  57.212 ; reg =  24.3639\n",
      "100 :  cross entropy loss =  26.0509 ; reg =  25.0176\n",
      "200 :  cross entropy loss =  25.8279 ; reg =  24.8665\n",
      "300 :  cross entropy loss =  29.0086 ; reg =  24.7037\n",
      "400 :  cross entropy loss =  36.6414 ; reg =  24.4747\n",
      "373 : train_accuracy:  0.7522 ; validation_accuracy:  0.734\n",
      "374 :  learning rate =  0.0009941507014756797\n",
      "--------------------------\n",
      "0 :  cross entropy loss =  91.9069 ; reg =  24.5034\n",
      "100 :  cross entropy loss =  23.0513 ; reg =  25.518\n",
      "200 :  cross entropy loss =  19.2843 ; reg =  24.7562\n",
      "300 :  cross entropy loss =  46.3098 ; reg =  24.48\n",
      "400 :  cross entropy loss =  36.4827 ; reg =  24.5341\n",
      "374 : train_accuracy:  0.7618 ; validation_accuracy:  0.7216\n",
      "375 :  learning rate =  0.0009941507014756797\n",
      "--------------------------\n",
      "0 :  cross entropy loss =  36.5804 ; reg =  24.6245\n",
      "100 :  cross entropy loss =  30.8283 ; reg =  24.5544\n",
      "200 :  cross entropy loss =  30.5195 ; reg =  24.3832\n",
      "300 :  cross entropy loss =  39.2375 ; reg =  24.2565\n",
      "400 :  cross entropy loss =  34.3938 ; reg =  24.0541\n",
      "375 : train_accuracy:  0.7648 ; validation_accuracy:  0.7336\n",
      "376 :  learning rate =  0.0009941507014756797\n",
      "--------------------------\n",
      "0 :  cross entropy loss =  31.0603 ; reg =  24.0\n",
      "100 :  cross entropy loss =  21.9024 ; reg =  24.0926\n",
      "200 :  cross entropy loss =  42.0979 ; reg =  24.303\n",
      "300 :  cross entropy loss =  25.524 ; reg =  24.0139\n",
      "400 :  cross entropy loss =  34.8913 ; reg =  24.1125\n",
      "376 : train_accuracy:  0.7302 ; validation_accuracy:  0.6916\n",
      "377 :  learning rate =  0.0009941507014756797\n",
      "--------------------------\n",
      "0 :  cross entropy loss =  107.837 ; reg =  24.153\n",
      "100 :  cross entropy loss =  21.8719 ; reg =  24.953\n",
      "200 :  cross entropy loss =  32.2482 ; reg =  25.0178\n",
      "300 :  cross entropy loss =  55.5586 ; reg =  24.8913\n",
      "400 :  cross entropy loss =  31.6894 ; reg =  24.4755\n",
      "377 : train_accuracy:  0.7272 ; validation_accuracy:  0.6952\n",
      "378 :  learning rate =  0.0009941507014756797\n",
      "--------------------------\n",
      "0 :  cross entropy loss =  51.6488 ; reg =  24.347\n",
      "100 :  cross entropy loss =  20.0524 ; reg =  24.9285\n",
      "200 :  cross entropy loss =  17.633 ; reg =  24.7983\n",
      "300 :  cross entropy loss =  41.8306 ; reg =  24.8489\n",
      "400 :  cross entropy loss =  47.9092 ; reg =  24.707\n",
      "378 : train_accuracy:  0.7474 ; validation_accuracy:  0.7168\n",
      "379 :  learning rate =  0.0009941507014756797\n",
      "--------------------------\n",
      "0 :  cross entropy loss =  53.1268 ; reg =  24.534\n",
      "100 :  cross entropy loss =  41.6413 ; reg =  25.2552\n",
      "200 :  cross entropy loss =  37.3639 ; reg =  24.9461\n",
      "300 :  cross entropy loss =  36.9102 ; reg =  24.6886\n",
      "400 :  cross entropy loss =  43.5321 ; reg =  24.6105\n",
      "379 : train_accuracy:  0.7346 ; validation_accuracy:  0.724\n",
      "380 :  learning rate =  0.0009941507014756797\n",
      "--------------------------\n",
      "0 :  cross entropy loss =  49.8373 ; reg =  24.6407\n",
      "100 :  cross entropy loss =  25.724 ; reg =  24.555\n",
      "200 :  cross entropy loss =  25.4002 ; reg =  24.3951\n",
      "300 :  cross entropy loss =  24.1086 ; reg =  24.16\n",
      "400 :  cross entropy loss =  46.261 ; reg =  24.3233\n",
      "380 : train_accuracy:  0.7254 ; validation_accuracy:  0.6872\n",
      "381 :  learning rate =  0.0009941507014756797\n",
      "--------------------------\n",
      "0 :  cross entropy loss =  52.0046 ; reg =  24.3339\n",
      "100 :  cross entropy loss =  44.9622 ; reg =  25.2683\n",
      "200 :  cross entropy loss =  43.5989 ; reg =  25.0054\n",
      "300 :  cross entropy loss =  41.3581 ; reg =  24.6244\n",
      "400 :  cross entropy loss =  56.8989 ; reg =  24.4564\n",
      "381 : train_accuracy:  0.7502 ; validation_accuracy:  0.7112\n",
      "382 :  learning rate =  0.0009941507014756797\n",
      "--------------------------\n",
      "0 :  cross entropy loss =  70.7215 ; reg =  24.3752\n",
      "100 :  cross entropy loss =  29.3558 ; reg =  25.0175\n",
      "200 :  cross entropy loss =  33.8038 ; reg =  24.6386\n",
      "300 :  cross entropy loss =  30.3877 ; reg =  24.4695\n",
      "400 :  cross entropy loss =  23.5244 ; reg =  24.425\n",
      "382 : train_accuracy:  0.73 ; validation_accuracy:  0.6948\n",
      "383 :  learning rate =  0.0009941507014756797\n",
      "--------------------------\n",
      "0 :  cross entropy loss =  44.8674 ; reg =  24.4643\n",
      "100 :  cross entropy loss =  33.2365 ; reg =  25.4759\n",
      "200 :  cross entropy loss =  23.3931 ; reg =  25.1444\n",
      "300 :  cross entropy loss =  32.0082 ; reg =  24.9869\n",
      "400 :  cross entropy loss =  22.2527 ; reg =  24.657\n",
      "383 : train_accuracy:  0.7602 ; validation_accuracy:  0.7216\n",
      "384 :  learning rate =  0.0009941507014756797\n",
      "--------------------------\n",
      "0 :  cross entropy loss =  38.1355 ; reg =  24.4099\n",
      "100 :  cross entropy loss =  35.8308 ; reg =  25.0251\n",
      "200 :  cross entropy loss =  16.6319 ; reg =  24.7104\n",
      "300 :  cross entropy loss =  23.3346 ; reg =  24.6586\n",
      "400 :  cross entropy loss =  26.6119 ; reg =  24.4666\n",
      "384 : train_accuracy:  0.7654 ; validation_accuracy:  0.7252\n",
      "385 :  learning rate =  0.0009941507014756797\n",
      "--------------------------\n",
      "0 :  cross entropy loss =  40.1584 ; reg =  24.5028\n",
      "100 :  cross entropy loss =  27.2986 ; reg =  24.4014\n",
      "200 :  cross entropy loss =  14.8756 ; reg =  24.4874\n",
      "300 :  cross entropy loss =  51.8832 ; reg =  24.6062\n",
      "400 :  cross entropy loss =  37.5842 ; reg =  24.8241\n",
      "385 : train_accuracy:  0.7304 ; validation_accuracy:  0.6856\n",
      "386 :  learning rate =  0.0009941507014756797\n",
      "--------------------------\n",
      "0 :  cross entropy loss =  64.8036 ; reg =  25.0686\n",
      "100 :  cross entropy loss =  13.0356 ; reg =  24.8242\n",
      "200 :  cross entropy loss =  36.7678 ; reg =  24.7108\n",
      "300 :  cross entropy loss =  23.497 ; reg =  24.9575\n",
      "400 :  cross entropy loss =  29.3349 ; reg =  24.6756\n",
      "386 : train_accuracy:  0.6876 ; validation_accuracy:  0.6524\n",
      "387 :  learning rate =  0.0009941507014756797\n",
      "--------------------------\n",
      "0 :  cross entropy loss =  151.73 ; reg =  24.7119\n",
      "100 :  cross entropy loss =  22.9272 ; reg =  25.5078\n",
      "200 :  cross entropy loss =  37.4431 ; reg =  25.0239\n",
      "300 :  cross entropy loss =  29.7679 ; reg =  24.6966\n",
      "400 :  cross entropy loss =  32.9618 ; reg =  24.6776\n",
      "387 : train_accuracy:  0.7764 ; validation_accuracy:  0.7348\n",
      "388 :  learning rate =  0.0009941507014756797\n",
      "--------------------------\n",
      "0 :  cross entropy loss =  38.4126 ; reg =  24.6092\n",
      "100 :  cross entropy loss =  29.7878 ; reg =  24.9457\n",
      "200 :  cross entropy loss =  37.0906 ; reg =  24.5327\n",
      "300 :  cross entropy loss =  35.4507 ; reg =  24.5984\n",
      "400 :  cross entropy loss =  53.7691 ; reg =  24.6291\n",
      "388 : train_accuracy:  0.7706 ; validation_accuracy:  0.7316\n",
      "389 :  learning rate =  0.0009941507014756797\n",
      "--------------------------\n",
      "0 :  cross entropy loss =  18.6555 ; reg =  24.8741\n",
      "100 :  cross entropy loss =  24.3365 ; reg =  24.5587\n",
      "200 :  cross entropy loss =  30.7796 ; reg =  24.5035\n",
      "300 :  cross entropy loss =  24.2213 ; reg =  24.8111\n",
      "400 :  cross entropy loss =  26.3268 ; reg =  24.8809\n",
      "389 : train_accuracy:  0.7146 ; validation_accuracy:  0.6776\n",
      "390 :  learning rate =  0.0009941507014756797\n",
      "--------------------------\n",
      "0 :  cross entropy loss =  67.8057 ; reg =  24.832\n",
      "100 :  cross entropy loss =  35.786 ; reg =  25.0486\n",
      "200 :  cross entropy loss =  33.8476 ; reg =  24.7644\n",
      "300 :  cross entropy loss =  20.5197 ; reg =  24.6125\n",
      "400 :  cross entropy loss =  37.4653 ; reg =  24.2995\n",
      "390 : train_accuracy:  0.7114 ; validation_accuracy:  0.68\n",
      "391 :  learning rate =  0.0009941507014756797\n",
      "--------------------------\n",
      "0 :  cross entropy loss =  64.5653 ; reg =  24.3575\n",
      "100 :  cross entropy loss =  34.8201 ; reg =  24.7461\n",
      "200 :  cross entropy loss =  32.8507 ; reg =  24.8961\n",
      "300 :  cross entropy loss =  42.8512 ; reg =  24.6639\n",
      "400 :  cross entropy loss =  27.2938 ; reg =  24.4179\n",
      "391 : train_accuracy:  0.742 ; validation_accuracy:  0.7268\n",
      "392 :  learning rate =  0.0009941507014756797\n",
      "--------------------------\n",
      "0 :  cross entropy loss =  19.1763 ; reg =  24.4838\n",
      "100 :  cross entropy loss =  23.8165 ; reg =  24.6112\n",
      "200 :  cross entropy loss =  36.6911 ; reg =  24.7164\n",
      "300 :  cross entropy loss =  27.2905 ; reg =  24.3507\n",
      "400 :  cross entropy loss =  27.9349 ; reg =  24.2599\n",
      "392 : train_accuracy:  0.7294 ; validation_accuracy:  0.6936\n",
      "393 :  learning rate =  0.0009941507014756797\n",
      "--------------------------\n",
      "0 :  cross entropy loss =  53.6771 ; reg =  24.3428\n",
      "100 :  cross entropy loss =  25.8609 ; reg =  24.3261\n",
      "200 :  cross entropy loss =  47.5128 ; reg =  24.3406\n",
      "300 :  cross entropy loss =  34.7369 ; reg =  24.3523\n",
      "400 :  cross entropy loss =  38.7197 ; reg =  24.264\n",
      "393 : train_accuracy:  0.7624 ; validation_accuracy:  0.7064\n",
      "394 :  learning rate =  0.0009941507014756797\n",
      "--------------------------\n",
      "0 :  cross entropy loss =  57.8386 ; reg =  24.4639\n",
      "100 :  cross entropy loss =  30.7784 ; reg =  25.3956\n",
      "200 :  cross entropy loss =  31.9627 ; reg =  25.2964\n",
      "300 :  cross entropy loss =  18.4942 ; reg =  24.9776\n",
      "400 :  cross entropy loss =  42.4887 ; reg =  24.7187\n",
      "394 : train_accuracy:  0.7376 ; validation_accuracy:  0.7008\n",
      "395 :  learning rate =  0.0009941507014756797\n",
      "--------------------------\n",
      "0 :  cross entropy loss =  41.2402 ; reg =  24.7233\n",
      "100 :  cross entropy loss =  41.091 ; reg =  25.3462\n",
      "200 :  cross entropy loss =  51.154 ; reg =  25.1104\n",
      "300 :  cross entropy loss =  43.0884 ; reg =  25.024\n",
      "400 :  cross entropy loss =  27.5692 ; reg =  25.0368\n",
      "395 : train_accuracy:  0.751 ; validation_accuracy:  0.6964\n",
      "396 :  learning rate =  0.0009941507014756797\n",
      "--------------------------\n",
      "0 :  cross entropy loss =  51.8764 ; reg =  24.9267\n",
      "100 :  cross entropy loss =  33.0005 ; reg =  25.5452\n",
      "200 :  cross entropy loss =  38.9706 ; reg =  25.226\n",
      "300 :  cross entropy loss =  47.4937 ; reg =  24.99\n",
      "400 :  cross entropy loss =  33.253 ; reg =  24.5677\n",
      "396 : train_accuracy:  0.7284 ; validation_accuracy:  0.7128\n",
      "397 :  learning rate =  0.0009941507014756797\n",
      "--------------------------\n",
      "0 :  cross entropy loss =  62.8653 ; reg =  24.9317\n",
      "100 :  cross entropy loss =  33.4918 ; reg =  25.3005\n",
      "200 :  cross entropy loss =  15.3626 ; reg =  24.8901\n",
      "300 :  cross entropy loss =  42.2254 ; reg =  24.5779\n",
      "400 :  cross entropy loss =  44.7988 ; reg =  24.5813\n",
      "397 : train_accuracy:  0.7242 ; validation_accuracy:  0.7036\n",
      "398 :  learning rate =  0.0009941507014756797\n",
      "--------------------------\n",
      "0 :  cross entropy loss =  54.3224 ; reg =  24.7811\n",
      "100 :  cross entropy loss =  27.8702 ; reg =  25.1835\n",
      "200 :  cross entropy loss =  43.2073 ; reg =  24.909\n",
      "300 :  cross entropy loss =  39.2588 ; reg =  24.6999\n",
      "400 :  cross entropy loss =  58.4857 ; reg =  24.7966\n",
      "398 : train_accuracy:  0.746 ; validation_accuracy:  0.7132\n",
      "399 :  learning rate =  0.0009941507014756797\n",
      "--------------------------\n",
      "0 :  cross entropy loss =  42.4812 ; reg =  24.4179\n",
      "100 :  cross entropy loss =  22.4601 ; reg =  24.5928\n",
      "200 :  cross entropy loss =  25.8674 ; reg =  24.3401\n",
      "300 :  cross entropy loss =  22.0033 ; reg =  24.4495\n",
      "400 :  cross entropy loss =  51.3104 ; reg =  24.3617\n",
      "399 : train_accuracy:  0.7546 ; validation_accuracy:  0.7116\n",
      "400 :  learning rate =  0.0009941507014756797\n",
      "--------------------------\n",
      "0 :  cross entropy loss =  52.2326 ; reg =  24.3363\n",
      "100 :  cross entropy loss =  30.887 ; reg =  24.4187\n",
      "200 :  cross entropy loss =  31.99 ; reg =  24.3539\n",
      "300 :  cross entropy loss =  19.8205 ; reg =  24.2609\n",
      "400 :  cross entropy loss =  39.357 ; reg =  24.2547\n",
      "400 : train_accuracy:  0.701 ; validation_accuracy:  0.6608\n",
      "401 :  learning rate =  0.0009941507014756797\n",
      "--------------------------\n",
      "0 :  cross entropy loss =  74.5341 ; reg =  24.3973\n",
      "100 :  cross entropy loss =  34.1736 ; reg =  24.9438\n",
      "200 :  cross entropy loss =  36.4041 ; reg =  24.8557\n",
      "300 :  cross entropy loss =  23.6325 ; reg =  24.6323\n",
      "400 :  cross entropy loss =  17.6116 ; reg =  24.3616\n",
      "401 : train_accuracy:  0.7264 ; validation_accuracy:  0.6872\n",
      "402 :  learning rate =  0.0009941507014756797\n",
      "--------------------------\n",
      "0 :  cross entropy loss =  53.3147 ; reg =  24.2244\n",
      "100 :  cross entropy loss =  25.0907 ; reg =  24.5014\n",
      "200 :  cross entropy loss =  38.2111 ; reg =  24.2973\n",
      "300 :  cross entropy loss =  25.7004 ; reg =  24.245\n",
      "400 :  cross entropy loss =  37.533 ; reg =  24.1157\n",
      "402 : train_accuracy:  0.74 ; validation_accuracy:  0.7036\n",
      "403 :  learning rate =  0.0009941507014756797\n",
      "--------------------------\n",
      "0 :  cross entropy loss =  57.707 ; reg =  24.0457\n",
      "100 :  cross entropy loss =  29.2042 ; reg =  24.3782\n",
      "200 :  cross entropy loss =  21.4691 ; reg =  24.3641\n",
      "300 :  cross entropy loss =  33.6969 ; reg =  24.3286\n",
      "400 :  cross entropy loss =  24.2164 ; reg =  24.329\n",
      "403 : train_accuracy:  0.7688 ; validation_accuracy:  0.722\n",
      "404 :  learning rate =  0.0009941507014756797\n",
      "--------------------------\n",
      "0 :  cross entropy loss =  53.3867 ; reg =  24.0826\n",
      "100 :  cross entropy loss =  29.4008 ; reg =  24.401\n",
      "200 :  cross entropy loss =  39.3121 ; reg =  24.435\n",
      "300 :  cross entropy loss =  43.143 ; reg =  24.6575\n",
      "400 :  cross entropy loss =  44.0862 ; reg =  24.5981\n",
      "404 : train_accuracy:  0.7338 ; validation_accuracy:  0.7032\n",
      "405 :  learning rate =  0.0009941507014756797\n",
      "--------------------------\n",
      "0 :  cross entropy loss =  45.6622 ; reg =  24.5361\n",
      "100 :  cross entropy loss =  14.9673 ; reg =  24.6854\n",
      "200 :  cross entropy loss =  39.478 ; reg =  24.4662\n",
      "300 :  cross entropy loss =  34.8777 ; reg =  24.3872\n",
      "400 :  cross entropy loss =  40.5208 ; reg =  24.3426\n",
      "405 : train_accuracy:  0.722 ; validation_accuracy:  0.6788\n",
      "406 :  learning rate =  0.0009941507014756797\n",
      "--------------------------\n",
      "0 :  cross entropy loss =  56.0058 ; reg =  24.2093\n",
      "100 :  cross entropy loss =  24.6809 ; reg =  24.8394\n",
      "200 :  cross entropy loss =  28.2767 ; reg =  24.8134\n",
      "300 :  cross entropy loss =  46.9532 ; reg =  24.7348\n",
      "400 :  cross entropy loss =  39.475 ; reg =  24.8251\n",
      "406 : train_accuracy:  0.777 ; validation_accuracy:  0.732\n",
      "407 :  learning rate =  0.0009941507014756797\n",
      "--------------------------\n",
      "0 :  cross entropy loss =  28.025 ; reg =  24.778\n",
      "100 :  cross entropy loss =  34.9714 ; reg =  24.9089\n",
      "200 :  cross entropy loss =  33.8648 ; reg =  24.8712\n",
      "300 :  cross entropy loss =  40.3384 ; reg =  24.7313\n",
      "400 :  cross entropy loss =  19.8118 ; reg =  24.6929\n",
      "407 : train_accuracy:  0.679 ; validation_accuracy:  0.6504\n",
      "408 :  learning rate =  0.0009941507014756797\n",
      "--------------------------\n",
      "0 :  cross entropy loss =  115.521 ; reg =  24.7044\n",
      "100 :  cross entropy loss =  31.6738 ; reg =  26.5215\n",
      "200 :  cross entropy loss =  49.8357 ; reg =  25.7919\n",
      "300 :  cross entropy loss =  23.8585 ; reg =  25.4675\n",
      "400 :  cross entropy loss =  27.825 ; reg =  24.8636\n",
      "408 : train_accuracy:  0.7686 ; validation_accuracy:  0.7352\n",
      "409 :  learning rate =  0.0009941507014756797\n",
      "--------------------------\n",
      "0 :  cross entropy loss =  35.2826 ; reg =  24.9142\n",
      "100 :  cross entropy loss =  39.4602 ; reg =  25.1421\n",
      "200 :  cross entropy loss =  46.031 ; reg =  24.8761\n",
      "300 :  cross entropy loss =  42.9597 ; reg =  24.379\n",
      "400 :  cross entropy loss =  33.0487 ; reg =  24.1304\n",
      "409 : train_accuracy:  0.7228 ; validation_accuracy:  0.698\n",
      "410 :  learning rate =  0.0009941507014756797\n",
      "--------------------------\n",
      "0 :  cross entropy loss =  34.3446 ; reg =  24.1588\n",
      "100 :  cross entropy loss =  35.0333 ; reg =  24.1938\n",
      "200 :  cross entropy loss =  40.1924 ; reg =  24.4434\n",
      "300 :  cross entropy loss =  23.7596 ; reg =  24.3159\n",
      "400 :  cross entropy loss =  45.6972 ; reg =  24.2859\n",
      "410 : train_accuracy:  0.7572 ; validation_accuracy:  0.7256\n",
      "411 :  learning rate =  0.0009941507014756797\n",
      "--------------------------\n",
      "0 :  cross entropy loss =  32.3348 ; reg =  24.3742\n",
      "100 :  cross entropy loss =  30.8525 ; reg =  24.3421\n",
      "200 :  cross entropy loss =  49.1184 ; reg =  24.3387\n",
      "300 :  cross entropy loss =  52.8203 ; reg =  24.3584\n",
      "400 :  cross entropy loss =  17.3299 ; reg =  24.3082\n",
      "411 : train_accuracy:  0.7234 ; validation_accuracy:  0.702\n",
      "412 :  learning rate =  0.0009941507014756797\n",
      "--------------------------\n",
      "0 :  cross entropy loss =  65.8438 ; reg =  24.2311\n",
      "100 :  cross entropy loss =  37.161 ; reg =  25.2918\n",
      "200 :  cross entropy loss =  38.4266 ; reg =  25.0236\n",
      "300 :  cross entropy loss =  35.1336 ; reg =  24.7864\n",
      "400 :  cross entropy loss =  38.8236 ; reg =  24.5652\n",
      "412 : train_accuracy:  0.7516 ; validation_accuracy:  0.7288\n",
      "413 :  learning rate =  0.0009941507014756797\n",
      "--------------------------\n",
      "0 :  cross entropy loss =  38.6438 ; reg =  24.5219\n",
      "100 :  cross entropy loss =  38.5983 ; reg =  24.9028\n",
      "200 :  cross entropy loss =  34.3051 ; reg =  24.5258\n",
      "300 :  cross entropy loss =  49.4659 ; reg =  24.6651\n",
      "400 :  cross entropy loss =  31.2476 ; reg =  24.5041\n",
      "413 : train_accuracy:  0.736 ; validation_accuracy:  0.7056\n",
      "414 :  learning rate =  0.0009941507014756797\n",
      "--------------------------\n",
      "0 :  cross entropy loss =  66.767 ; reg =  24.5688\n",
      "100 :  cross entropy loss =  31.0967 ; reg =  25.9391\n",
      "200 :  cross entropy loss =  38.8006 ; reg =  25.5367\n",
      "300 :  cross entropy loss =  46.9324 ; reg =  25.429\n",
      "400 :  cross entropy loss =  32.6323 ; reg =  25.2614\n",
      "414 : train_accuracy:  0.7258 ; validation_accuracy:  0.7056\n",
      "415 :  learning rate =  0.0009941507014756797\n",
      "--------------------------\n",
      "0 :  cross entropy loss =  83.9661 ; reg =  25.115\n",
      "100 :  cross entropy loss =  23.6506 ; reg =  25.3919\n",
      "200 :  cross entropy loss =  44.6242 ; reg =  25.0101\n",
      "300 :  cross entropy loss =  20.8545 ; reg =  25.0359\n",
      "400 :  cross entropy loss =  25.7785 ; reg =  24.9722\n",
      "415 : train_accuracy:  0.7542 ; validation_accuracy:  0.7136\n",
      "416 :  learning rate =  0.0009941507014756797\n",
      "--------------------------\n",
      "0 :  cross entropy loss =  55.9879 ; reg =  24.7747\n",
      "100 :  cross entropy loss =  23.4685 ; reg =  24.7029\n",
      "200 :  cross entropy loss =  28.9284 ; reg =  24.4682\n",
      "300 :  cross entropy loss =  45.8334 ; reg =  24.3748\n",
      "400 :  cross entropy loss =  24.8647 ; reg =  24.3572\n",
      "416 : train_accuracy:  0.7334 ; validation_accuracy:  0.7096\n",
      "417 :  learning rate =  0.0009941507014756797\n",
      "--------------------------\n",
      "0 :  cross entropy loss =  59.8607 ; reg =  24.4562\n",
      "100 :  cross entropy loss =  37.1994 ; reg =  24.8646\n",
      "200 :  cross entropy loss =  29.7127 ; reg =  24.6566\n",
      "300 :  cross entropy loss =  37.9839 ; reg =  24.7096\n",
      "400 :  cross entropy loss =  38.5351 ; reg =  24.5123\n",
      "417 : train_accuracy:  0.7452 ; validation_accuracy:  0.6988\n",
      "418 :  learning rate =  0.0009941507014756797\n",
      "--------------------------\n",
      "0 :  cross entropy loss =  50.7489 ; reg =  24.4375\n",
      "100 :  cross entropy loss =  26.0626 ; reg =  24.528\n",
      "200 :  cross entropy loss =  29.6573 ; reg =  24.3726\n",
      "300 :  cross entropy loss =  36.431 ; reg =  24.4227\n",
      "400 :  cross entropy loss =  33.8592 ; reg =  24.4633\n",
      "418 : train_accuracy:  0.7278 ; validation_accuracy:  0.6912\n",
      "419 :  learning rate =  0.0009941507014756797\n",
      "--------------------------\n",
      "0 :  cross entropy loss =  74.5207 ; reg =  24.4039\n",
      "100 :  cross entropy loss =  30.5849 ; reg =  24.8546\n",
      "200 :  cross entropy loss =  32.2253 ; reg =  24.6302\n",
      "300 :  cross entropy loss =  44.6719 ; reg =  24.3734\n",
      "400 :  cross entropy loss =  52.0262 ; reg =  24.5871\n",
      "419 : train_accuracy:  0.7162 ; validation_accuracy:  0.6924\n",
      "420 :  learning rate =  0.0009941507014756797\n",
      "--------------------------\n",
      "0 :  cross entropy loss =  85.5249 ; reg =  24.5118\n",
      "100 :  cross entropy loss =  50.9048 ; reg =  24.8196\n",
      "200 :  cross entropy loss =  34.1857 ; reg =  24.5561\n",
      "300 :  cross entropy loss =  31.3464 ; reg =  24.5292\n",
      "400 :  cross entropy loss =  34.7242 ; reg =  24.6129\n",
      "420 : train_accuracy:  0.7336 ; validation_accuracy:  0.7096\n",
      "421 :  learning rate =  0.0009941507014756797\n",
      "--------------------------\n",
      "0 :  cross entropy loss =  41.8996 ; reg =  24.6472\n",
      "100 :  cross entropy loss =  31.2129 ; reg =  25.0842\n",
      "200 :  cross entropy loss =  39.3465 ; reg =  25.075\n",
      "300 :  cross entropy loss =  27.3229 ; reg =  24.4985\n",
      "400 :  cross entropy loss =  22.5042 ; reg =  24.3439\n",
      "421 : train_accuracy:  0.734 ; validation_accuracy:  0.6944\n",
      "422 :  learning rate =  0.0009941507014756797\n",
      "--------------------------\n",
      "0 :  cross entropy loss =  44.9891 ; reg =  24.4637\n",
      "100 :  cross entropy loss =  43.0881 ; reg =  24.4509\n",
      "200 :  cross entropy loss =  37.3808 ; reg =  24.4871\n",
      "300 :  cross entropy loss =  38.7192 ; reg =  24.5973\n",
      "400 :  cross entropy loss =  24.4024 ; reg =  24.4953\n",
      "422 : train_accuracy:  0.7576 ; validation_accuracy:  0.744\n",
      "423 :  learning rate =  0.0009941507014756797\n",
      "--------------------------\n",
      "0 :  cross entropy loss =  36.4447 ; reg =  24.486\n",
      "100 :  cross entropy loss =  39.6588 ; reg =  24.742\n",
      "200 :  cross entropy loss =  62.8286 ; reg =  24.6837\n",
      "300 :  cross entropy loss =  29.1361 ; reg =  24.5366\n",
      "400 :  cross entropy loss =  32.3839 ; reg =  24.6615\n",
      "423 : train_accuracy:  0.732 ; validation_accuracy:  0.7036\n",
      "424 :  learning rate =  0.0009941507014756797\n",
      "--------------------------\n",
      "0 :  cross entropy loss =  47.8333 ; reg =  24.6909\n",
      "100 :  cross entropy loss =  30.8652 ; reg =  25.0324\n",
      "200 :  cross entropy loss =  34.9977 ; reg =  24.6975\n",
      "300 :  cross entropy loss =  39.236 ; reg =  24.6276\n",
      "400 :  cross entropy loss =  49.0255 ; reg =  24.6909\n",
      "424 : train_accuracy:  0.7386 ; validation_accuracy:  0.698\n",
      "425 :  learning rate =  0.0009941507014756797\n",
      "--------------------------\n",
      "0 :  cross entropy loss =  47.1818 ; reg =  24.5987\n",
      "100 :  cross entropy loss =  22.923 ; reg =  24.824\n",
      "200 :  cross entropy loss =  33.8704 ; reg =  24.4667\n",
      "300 :  cross entropy loss =  51.9186 ; reg =  24.9006\n",
      "400 :  cross entropy loss =  25.8149 ; reg =  24.6564\n",
      "425 : train_accuracy:  0.7662 ; validation_accuracy:  0.7216\n",
      "426 :  learning rate =  0.0009941507014756797\n",
      "--------------------------\n",
      "0 :  cross entropy loss =  35.074 ; reg =  24.6281\n",
      "100 :  cross entropy loss =  38.5703 ; reg =  24.7039\n",
      "200 :  cross entropy loss =  29.8323 ; reg =  24.4139\n",
      "300 :  cross entropy loss =  26.4477 ; reg =  24.2225\n",
      "400 :  cross entropy loss =  47.262 ; reg =  24.2818\n",
      "426 : train_accuracy:  0.7134 ; validation_accuracy:  0.6844\n",
      "427 :  learning rate =  0.0009941507014756797\n",
      "--------------------------\n",
      "0 :  cross entropy loss =  65.9423 ; reg =  24.311\n",
      "100 :  cross entropy loss =  25.0362 ; reg =  25.364\n",
      "200 :  cross entropy loss =  35.8604 ; reg =  25.2266\n",
      "300 :  cross entropy loss =  24.7794 ; reg =  24.7006\n",
      "400 :  cross entropy loss =  39.9625 ; reg =  24.5113\n",
      "427 : train_accuracy:  0.7334 ; validation_accuracy:  0.7\n",
      "428 :  learning rate =  0.0009941507014756797\n",
      "--------------------------\n",
      "0 :  cross entropy loss =  32.9551 ; reg =  24.4368\n",
      "100 :  cross entropy loss =  21.2629 ; reg =  24.314\n",
      "200 :  cross entropy loss =  23.8719 ; reg =  24.2488\n",
      "300 :  cross entropy loss =  43.3677 ; reg =  24.4935\n",
      "400 :  cross entropy loss =  51.2637 ; reg =  24.3142\n",
      "428 : train_accuracy:  0.7252 ; validation_accuracy:  0.6824\n",
      "429 :  learning rate =  0.0009941507014756797\n",
      "--------------------------\n",
      "0 :  cross entropy loss =  57.471 ; reg =  24.3183\n",
      "100 :  cross entropy loss =  29.6725 ; reg =  24.8139\n",
      "200 :  cross entropy loss =  43.5447 ; reg =  24.6887\n",
      "300 :  cross entropy loss =  28.612 ; reg =  24.7187\n",
      "400 :  cross entropy loss =  26.4955 ; reg =  24.5422\n",
      "429 : train_accuracy:  0.75 ; validation_accuracy:  0.7156\n",
      "430 :  learning rate =  0.0009941507014756797\n",
      "--------------------------\n",
      "0 :  cross entropy loss =  48.1021 ; reg =  24.4589\n",
      "100 :  cross entropy loss =  34.7961 ; reg =  25.0686\n",
      "200 :  cross entropy loss =  57.1365 ; reg =  24.7096\n",
      "300 :  cross entropy loss =  53.7687 ; reg =  24.5043\n",
      "400 :  cross entropy loss =  22.1956 ; reg =  24.6706\n",
      "430 : train_accuracy:  0.7534 ; validation_accuracy:  0.7344\n",
      "431 :  learning rate =  0.0009941507014756797\n",
      "--------------------------\n",
      "0 :  cross entropy loss =  27.8082 ; reg =  24.5271\n",
      "100 :  cross entropy loss =  50.328 ; reg =  24.9262\n",
      "200 :  cross entropy loss =  16.6781 ; reg =  24.8185\n",
      "300 :  cross entropy loss =  26.5021 ; reg =  24.4089\n",
      "400 :  cross entropy loss =  39.8971 ; reg =  24.5478\n",
      "431 : train_accuracy:  0.7248 ; validation_accuracy:  0.696\n",
      "432 :  learning rate =  0.0009941507014756797\n",
      "--------------------------\n",
      "0 :  cross entropy loss =  85.6019 ; reg =  24.8439\n",
      "100 :  cross entropy loss =  28.027 ; reg =  25.3368\n",
      "200 :  cross entropy loss =  31.9397 ; reg =  25.1278\n",
      "300 :  cross entropy loss =  30.8961 ; reg =  24.9296\n",
      "400 :  cross entropy loss =  32.0381 ; reg =  24.8572\n",
      "432 : train_accuracy:  0.73 ; validation_accuracy:  0.7072\n",
      "433 :  learning rate =  0.0009941507014756797\n",
      "--------------------------\n",
      "0 :  cross entropy loss =  41.739 ; reg =  24.8392\n",
      "100 :  cross entropy loss =  23.1675 ; reg =  25.3417\n",
      "200 :  cross entropy loss =  30.6003 ; reg =  24.896\n",
      "300 :  cross entropy loss =  32.1175 ; reg =  25.0688\n",
      "400 :  cross entropy loss =  24.7969 ; reg =  24.9473\n",
      "433 : train_accuracy:  0.7276 ; validation_accuracy:  0.6916\n",
      "434 :  learning rate =  0.0009941507014756797\n",
      "--------------------------\n",
      "0 :  cross entropy loss =  64.6105 ; reg =  25.0236\n",
      "100 :  cross entropy loss =  29.2757 ; reg =  25.5408\n",
      "200 :  cross entropy loss =  26.6316 ; reg =  25.1067\n",
      "300 :  cross entropy loss =  37.1592 ; reg =  25.1293\n",
      "400 :  cross entropy loss =  46.8691 ; reg =  25.1743\n",
      "434 : train_accuracy:  0.761 ; validation_accuracy:  0.73\n",
      "435 :  learning rate =  0.0009941507014756797\n",
      "--------------------------\n",
      "0 :  cross entropy loss =  39.9371 ; reg =  24.9516\n",
      "100 :  cross entropy loss =  33.1654 ; reg =  25.2228\n",
      "200 :  cross entropy loss =  41.6521 ; reg =  24.9268\n",
      "300 :  cross entropy loss =  41.1063 ; reg =  24.8739\n",
      "400 :  cross entropy loss =  55.598 ; reg =  24.5842\n",
      "435 : train_accuracy:  0.706 ; validation_accuracy:  0.6756\n",
      "436 :  learning rate =  0.0009941507014756797\n",
      "--------------------------\n",
      "0 :  cross entropy loss =  38.0498 ; reg =  24.7909\n",
      "100 :  cross entropy loss =  23.805 ; reg =  25.0866\n",
      "200 :  cross entropy loss =  41.9854 ; reg =  24.5974\n",
      "300 :  cross entropy loss =  38.0031 ; reg =  24.4752\n",
      "400 :  cross entropy loss =  47.0135 ; reg =  24.5545\n",
      "436 : train_accuracy:  0.7408 ; validation_accuracy:  0.7208\n",
      "437 :  learning rate =  0.0009941507014756797\n",
      "--------------------------\n",
      "0 :  cross entropy loss =  47.953 ; reg =  24.3739\n",
      "100 :  cross entropy loss =  28.6875 ; reg =  24.3261\n",
      "200 :  cross entropy loss =  36.326 ; reg =  24.2811\n",
      "300 :  cross entropy loss =  32.5571 ; reg =  24.0194\n",
      "400 :  cross entropy loss =  36.7034 ; reg =  24.0612\n",
      "437 : train_accuracy:  0.7364 ; validation_accuracy:  0.6996\n",
      "438 :  learning rate =  0.0009941507014756797\n",
      "--------------------------\n",
      "0 :  cross entropy loss =  39.893 ; reg =  24.1978\n",
      "100 :  cross entropy loss =  25.1508 ; reg =  24.4254\n",
      "200 :  cross entropy loss =  25.5853 ; reg =  24.1919\n",
      "300 :  cross entropy loss =  48.3403 ; reg =  24.5394\n",
      "400 :  cross entropy loss =  37.9591 ; reg =  24.4194\n",
      "438 : train_accuracy:  0.679 ; validation_accuracy:  0.6384\n",
      "439 :  learning rate =  0.0009941507014756797\n",
      "--------------------------\n",
      "0 :  cross entropy loss =  102.566 ; reg =  24.2789\n",
      "100 :  cross entropy loss =  49.1567 ; reg =  25.954\n",
      "200 :  cross entropy loss =  14.4516 ; reg =  25.2012\n",
      "300 :  cross entropy loss =  34.5712 ; reg =  24.9318\n",
      "400 :  cross entropy loss =  33.0423 ; reg =  24.734\n",
      "439 : train_accuracy:  0.6932 ; validation_accuracy:  0.666\n",
      "440 :  learning rate =  0.0009941507014756797\n",
      "--------------------------\n",
      "0 :  cross entropy loss =  71.6329 ; reg =  24.4556\n",
      "100 :  cross entropy loss =  31.0288 ; reg =  24.4678\n",
      "200 :  cross entropy loss =  29.2155 ; reg =  24.7075\n",
      "300 :  cross entropy loss =  46.7986 ; reg =  24.5937\n",
      "400 :  cross entropy loss =  31.2845 ; reg =  24.4288\n",
      "440 : train_accuracy:  0.7548 ; validation_accuracy:  0.7108\n",
      "441 :  learning rate =  0.0009941507014756797\n",
      "--------------------------\n",
      "0 :  cross entropy loss =  42.9484 ; reg =  24.2965\n",
      "100 :  cross entropy loss =  42.5621 ; reg =  25.02\n",
      "200 :  cross entropy loss =  39.4412 ; reg =  24.8678\n",
      "300 :  cross entropy loss =  37.5837 ; reg =  24.8068\n",
      "400 :  cross entropy loss =  41.048 ; reg =  24.5856\n",
      "441 : train_accuracy:  0.743 ; validation_accuracy:  0.716\n",
      "442 :  learning rate =  0.0009941507014756797\n",
      "--------------------------\n",
      "0 :  cross entropy loss =  30.902 ; reg =  24.6517\n",
      "100 :  cross entropy loss =  76.1991 ; reg =  25.3314\n",
      "200 :  cross entropy loss =  22.1929 ; reg =  24.9001\n",
      "300 :  cross entropy loss =  29.8655 ; reg =  24.7705\n",
      "400 :  cross entropy loss =  34.4286 ; reg =  24.5887\n",
      "442 : train_accuracy:  0.7248 ; validation_accuracy:  0.7008\n",
      "443 :  learning rate =  0.0009941507014756797\n",
      "--------------------------\n",
      "0 :  cross entropy loss =  107.419 ; reg =  24.3624\n",
      "100 :  cross entropy loss =  25.0501 ; reg =  25.1527\n",
      "200 :  cross entropy loss =  32.5835 ; reg =  24.8065\n",
      "300 :  cross entropy loss =  34.8202 ; reg =  24.6784\n",
      "400 :  cross entropy loss =  31.7527 ; reg =  24.4742\n",
      "443 : train_accuracy:  0.7492 ; validation_accuracy:  0.7208\n",
      "444 :  learning rate =  0.0009941507014756797\n",
      "--------------------------\n",
      "0 :  cross entropy loss =  19.487 ; reg =  24.2851\n",
      "100 :  cross entropy loss =  43.9358 ; reg =  24.2992\n",
      "200 :  cross entropy loss =  42.8771 ; reg =  24.4094\n",
      "300 :  cross entropy loss =  23.36 ; reg =  24.4044\n",
      "400 :  cross entropy loss =  31.1259 ; reg =  24.4951\n",
      "444 : train_accuracy:  0.7528 ; validation_accuracy:  0.7152\n",
      "445 :  learning rate =  0.0009941507014756797\n",
      "--------------------------\n",
      "0 :  cross entropy loss =  79.024 ; reg =  24.5772\n",
      "100 :  cross entropy loss =  28.3811 ; reg =  25.4242\n",
      "200 :  cross entropy loss =  33.7331 ; reg =  24.8712\n",
      "300 :  cross entropy loss =  16.9007 ; reg =  24.7025\n",
      "400 :  cross entropy loss =  34.5071 ; reg =  24.55\n",
      "445 : train_accuracy:  0.7514 ; validation_accuracy:  0.6992\n",
      "446 :  learning rate =  0.0009941507014756797\n",
      "--------------------------\n",
      "0 :  cross entropy loss =  59.9682 ; reg =  24.4748\n",
      "100 :  cross entropy loss =  34.9259 ; reg =  24.9347\n",
      "200 :  cross entropy loss =  28.7423 ; reg =  24.7375\n",
      "300 :  cross entropy loss =  47.5786 ; reg =  24.7049\n",
      "400 :  cross entropy loss =  45.2836 ; reg =  24.6587\n",
      "446 : train_accuracy:  0.7084 ; validation_accuracy:  0.6884\n",
      "447 :  learning rate =  0.0009941507014756797\n",
      "--------------------------\n",
      "0 :  cross entropy loss =  55.048 ; reg =  24.73\n",
      "100 :  cross entropy loss =  21.0889 ; reg =  25.5314\n",
      "200 :  cross entropy loss =  33.76 ; reg =  25.136\n",
      "300 :  cross entropy loss =  16.4725 ; reg =  25.099\n",
      "400 :  cross entropy loss =  48.2549 ; reg =  24.9476\n",
      "447 : train_accuracy:  0.7002 ; validation_accuracy:  0.6624\n",
      "448 :  learning rate =  0.0009941507014756797\n",
      "--------------------------\n",
      "0 :  cross entropy loss =  74.3257 ; reg =  25.1221\n",
      "100 :  cross entropy loss =  32.7016 ; reg =  25.6408\n",
      "200 :  cross entropy loss =  36.1295 ; reg =  25.0672\n",
      "300 :  cross entropy loss =  23.3326 ; reg =  24.9251\n",
      "400 :  cross entropy loss =  31.8531 ; reg =  24.9535\n",
      "448 : train_accuracy:  0.6854 ; validation_accuracy:  0.6592\n",
      "449 :  learning rate =  0.0009941507014756797\n",
      "--------------------------\n",
      "0 :  cross entropy loss =  85.5712 ; reg =  24.7862\n",
      "100 :  cross entropy loss =  23.5966 ; reg =  25.3583\n",
      "200 :  cross entropy loss =  32.6074 ; reg =  24.9783\n",
      "300 :  cross entropy loss =  27.7157 ; reg =  24.6894\n",
      "400 :  cross entropy loss =  39.6355 ; reg =  24.6618\n",
      "449 : train_accuracy:  0.7486 ; validation_accuracy:  0.714\n",
      "450 :  learning rate =  0.0009941507014756797\n",
      "--------------------------\n",
      "0 :  cross entropy loss =  51.4288 ; reg =  24.6108\n",
      "100 :  cross entropy loss =  40.9163 ; reg =  24.9838\n",
      "200 :  cross entropy loss =  28.2412 ; reg =  24.9363\n",
      "300 :  cross entropy loss =  31.0336 ; reg =  24.8052\n",
      "400 :  cross entropy loss =  39.2124 ; reg =  24.4431\n",
      "450 : train_accuracy:  0.7864 ; validation_accuracy:  0.7336\n",
      "451 :  learning rate =  0.0009941507014756797\n",
      "--------------------------\n",
      "0 :  cross entropy loss =  38.407 ; reg =  24.324\n",
      "100 :  cross entropy loss =  22.3361 ; reg =  24.3528\n",
      "200 :  cross entropy loss =  33.4202 ; reg =  24.2726\n",
      "300 :  cross entropy loss =  30.4706 ; reg =  24.1209\n",
      "400 :  cross entropy loss =  20.704 ; reg =  23.9454\n",
      "451 : train_accuracy:  0.7256 ; validation_accuracy:  0.7028\n",
      "452 :  learning rate =  0.0009941507014756797\n",
      "--------------------------\n",
      "0 :  cross entropy loss =  52.5761 ; reg =  23.9756\n",
      "100 :  cross entropy loss =  40.089 ; reg =  24.7789\n",
      "200 :  cross entropy loss =  51.4298 ; reg =  24.5235\n",
      "300 :  cross entropy loss =  35.4166 ; reg =  24.4456\n",
      "400 :  cross entropy loss =  50.555 ; reg =  24.5087\n",
      "452 : train_accuracy:  0.766 ; validation_accuracy:  0.7292\n",
      "453 :  learning rate =  0.0009941507014756797\n",
      "--------------------------\n",
      "0 :  cross entropy loss =  46.8593 ; reg =  24.8498\n",
      "100 :  cross entropy loss =  60.4975 ; reg =  26.0015\n",
      "200 :  cross entropy loss =  41.7407 ; reg =  25.4307\n",
      "300 :  cross entropy loss =  36.3644 ; reg =  25.171\n",
      "400 :  cross entropy loss =  50.8912 ; reg =  25.1193\n",
      "453 : train_accuracy:  0.7562 ; validation_accuracy:  0.7192\n",
      "454 :  learning rate =  0.0009941507014756797\n",
      "--------------------------\n",
      "0 :  cross entropy loss =  73.4205 ; reg =  25.2189\n",
      "100 :  cross entropy loss =  37.4357 ; reg =  25.7856\n",
      "200 :  cross entropy loss =  35.1794 ; reg =  25.7751\n",
      "300 :  cross entropy loss =  26.5552 ; reg =  25.368\n",
      "400 :  cross entropy loss =  40.4216 ; reg =  25.1593\n",
      "454 : train_accuracy:  0.731 ; validation_accuracy:  0.7176\n",
      "455 :  learning rate =  0.0009941507014756797\n",
      "--------------------------\n",
      "0 :  cross entropy loss =  43.9911 ; reg =  24.9113\n",
      "100 :  cross entropy loss =  14.6936 ; reg =  25.4585\n",
      "200 :  cross entropy loss =  33.169 ; reg =  25.3898\n",
      "300 :  cross entropy loss =  30.6915 ; reg =  25.0039\n",
      "400 :  cross entropy loss =  27.1911 ; reg =  24.5981\n",
      "455 : train_accuracy:  0.707 ; validation_accuracy:  0.6824\n",
      "456 :  learning rate =  0.0009941507014756797\n",
      "--------------------------\n",
      "0 :  cross entropy loss =  33.2367 ; reg =  24.4878\n",
      "100 :  cross entropy loss =  27.2396 ; reg =  24.7088\n",
      "200 :  cross entropy loss =  33.4533 ; reg =  24.3047\n",
      "300 :  cross entropy loss =  26.2372 ; reg =  24.3612\n",
      "400 :  cross entropy loss =  21.6436 ; reg =  24.1328\n",
      "456 : train_accuracy:  0.7372 ; validation_accuracy:  0.7012\n",
      "457 :  learning rate =  0.0009941507014756797\n",
      "--------------------------\n",
      "0 :  cross entropy loss =  38.6275 ; reg =  24.2729\n",
      "100 :  cross entropy loss =  19.4175 ; reg =  23.992\n",
      "200 :  cross entropy loss =  25.834 ; reg =  23.9321\n",
      "300 :  cross entropy loss =  41.3987 ; reg =  23.9789\n",
      "400 :  cross entropy loss =  35.7065 ; reg =  24.2401\n",
      "457 : train_accuracy:  0.767 ; validation_accuracy:  0.7416\n",
      "458 :  learning rate =  0.0009941507014756797\n",
      "--------------------------\n",
      "0 :  cross entropy loss =  31.8365 ; reg =  24.2673\n",
      "100 :  cross entropy loss =  20.3428 ; reg =  24.6945\n",
      "200 :  cross entropy loss =  27.3157 ; reg =  24.472\n",
      "300 :  cross entropy loss =  48.6561 ; reg =  24.2765\n",
      "400 :  cross entropy loss =  17.6973 ; reg =  24.1899\n",
      "458 : train_accuracy:  0.7282 ; validation_accuracy:  0.6876\n",
      "459 :  learning rate =  0.0009941507014756797\n",
      "--------------------------\n",
      "0 :  cross entropy loss =  71.7207 ; reg =  24.0037\n",
      "100 :  cross entropy loss =  21.4428 ; reg =  24.7349\n",
      "200 :  cross entropy loss =  43.3338 ; reg =  24.4965\n",
      "300 :  cross entropy loss =  36.2681 ; reg =  24.4997\n",
      "400 :  cross entropy loss =  21.6778 ; reg =  24.2056\n",
      "459 : train_accuracy:  0.718 ; validation_accuracy:  0.6904\n",
      "460 :  learning rate =  0.0009941507014756797\n",
      "--------------------------\n",
      "0 :  cross entropy loss =  44.887 ; reg =  24.2352\n",
      "100 :  cross entropy loss =  26.2248 ; reg =  24.775\n",
      "200 :  cross entropy loss =  24.2236 ; reg =  24.8345\n",
      "300 :  cross entropy loss =  39.6552 ; reg =  24.5422\n",
      "400 :  cross entropy loss =  26.3707 ; reg =  24.3729\n",
      "460 : train_accuracy:  0.7422 ; validation_accuracy:  0.7028\n",
      "461 :  learning rate =  0.0009941507014756797\n",
      "--------------------------\n",
      "0 :  cross entropy loss =  38.7354 ; reg =  24.3377\n",
      "100 :  cross entropy loss =  36.6633 ; reg =  25.1543\n",
      "200 :  cross entropy loss =  50.8614 ; reg =  24.7629\n",
      "300 :  cross entropy loss =  46.9811 ; reg =  24.5226\n",
      "400 :  cross entropy loss =  20.5024 ; reg =  24.5188\n",
      "461 : train_accuracy:  0.7206 ; validation_accuracy:  0.6948\n",
      "462 :  learning rate =  0.0009941507014756797\n",
      "--------------------------\n",
      "0 :  cross entropy loss =  80.5532 ; reg =  24.4359\n",
      "100 :  cross entropy loss =  34.0483 ; reg =  25.3724\n",
      "200 :  cross entropy loss =  29.0589 ; reg =  25.0374\n",
      "300 :  cross entropy loss =  55.1693 ; reg =  24.9317\n",
      "400 :  cross entropy loss =  31.3575 ; reg =  24.764\n",
      "462 : train_accuracy:  0.75 ; validation_accuracy:  0.708\n",
      "463 :  learning rate =  0.0009941507014756797\n",
      "--------------------------\n",
      "0 :  cross entropy loss =  50.0653 ; reg =  24.3973\n",
      "100 :  cross entropy loss =  31.8018 ; reg =  24.7864\n",
      "200 :  cross entropy loss =  47.017 ; reg =  24.6573\n",
      "300 :  cross entropy loss =  38.5923 ; reg =  24.5808\n",
      "400 :  cross entropy loss =  42.0583 ; reg =  24.5825\n",
      "463 : train_accuracy:  0.7036 ; validation_accuracy:  0.6736\n",
      "464 :  learning rate =  0.0009941507014756797\n",
      "--------------------------\n",
      "0 :  cross entropy loss =  87.1855 ; reg =  24.7061\n",
      "100 :  cross entropy loss =  55.8062 ; reg =  25.4969\n",
      "200 :  cross entropy loss =  35.5002 ; reg =  25.0996\n",
      "300 :  cross entropy loss =  27.5741 ; reg =  24.8103\n",
      "400 :  cross entropy loss =  20.432 ; reg =  24.5141\n",
      "464 : train_accuracy:  0.6926 ; validation_accuracy:  0.6616\n",
      "465 :  learning rate =  0.0009941507014756797\n",
      "--------------------------\n",
      "0 :  cross entropy loss =  71.068 ; reg =  24.5486\n",
      "100 :  cross entropy loss =  28.3654 ; reg =  24.7936\n",
      "200 :  cross entropy loss =  49.8637 ; reg =  24.5593\n",
      "300 :  cross entropy loss =  41.8242 ; reg =  24.5789\n",
      "400 :  cross entropy loss =  30.1144 ; reg =  24.5834\n",
      "465 : train_accuracy:  0.7798 ; validation_accuracy:  0.7236\n",
      "466 :  learning rate =  0.0009941507014756797\n",
      "--------------------------\n",
      "0 :  cross entropy loss =  28.1185 ; reg =  24.5139\n",
      "100 :  cross entropy loss =  28.7248 ; reg =  24.8438\n",
      "200 :  cross entropy loss =  43.6969 ; reg =  24.7791\n",
      "300 :  cross entropy loss =  25.4529 ; reg =  24.4495\n",
      "400 :  cross entropy loss =  27.2044 ; reg =  24.4803\n",
      "466 : train_accuracy:  0.7462 ; validation_accuracy:  0.7084\n",
      "467 :  learning rate =  0.0009941507014756797\n",
      "--------------------------\n",
      "0 :  cross entropy loss =  48.5955 ; reg =  24.6286\n",
      "100 :  cross entropy loss =  45.0288 ; reg =  25.9452\n",
      "200 :  cross entropy loss =  40.0413 ; reg =  25.8258\n",
      "300 :  cross entropy loss =  33.6266 ; reg =  25.4267\n",
      "400 :  cross entropy loss =  35.6224 ; reg =  25.3136\n",
      "467 : train_accuracy:  0.736 ; validation_accuracy:  0.704\n",
      "468 :  learning rate =  0.0009941507014756797\n",
      "--------------------------\n",
      "0 :  cross entropy loss =  55.9963 ; reg =  25.1309\n",
      "100 :  cross entropy loss =  25.9015 ; reg =  25.2241\n",
      "200 :  cross entropy loss =  39.2723 ; reg =  24.6657\n",
      "300 :  cross entropy loss =  27.3308 ; reg =  24.3387\n",
      "400 :  cross entropy loss =  43.169 ; reg =  24.1056\n",
      "468 : train_accuracy:  0.6858 ; validation_accuracy:  0.636\n",
      "469 :  learning rate =  0.0009941507014756797\n",
      "--------------------------\n",
      "0 :  cross entropy loss =  101.046 ; reg =  24.2186\n",
      "100 :  cross entropy loss =  49.7299 ; reg =  25.1752\n",
      "200 :  cross entropy loss =  33.8073 ; reg =  24.9267\n",
      "300 :  cross entropy loss =  36.0045 ; reg =  24.6771\n",
      "400 :  cross entropy loss =  21.403 ; reg =  24.531\n",
      "469 : train_accuracy:  0.7312 ; validation_accuracy:  0.7176\n",
      "470 :  learning rate =  0.0009941507014756797\n",
      "--------------------------\n",
      "0 :  cross entropy loss =  53.6036 ; reg =  24.5398\n",
      "100 :  cross entropy loss =  34.7392 ; reg =  25.1586\n",
      "200 :  cross entropy loss =  37.3617 ; reg =  25.0571\n",
      "300 :  cross entropy loss =  39.0506 ; reg =  24.8802\n",
      "400 :  cross entropy loss =  31.529 ; reg =  24.5748\n",
      "470 : train_accuracy:  0.7184 ; validation_accuracy:  0.6816\n",
      "471 :  learning rate =  0.0009941507014756797\n",
      "--------------------------\n",
      "0 :  cross entropy loss =  94.6407 ; reg =  24.4404\n",
      "100 :  cross entropy loss =  20.6718 ; reg =  25.2288\n",
      "200 :  cross entropy loss =  26.9949 ; reg =  24.712\n",
      "300 :  cross entropy loss =  51.7806 ; reg =  24.3844\n",
      "400 :  cross entropy loss =  43.8457 ; reg =  24.0551\n",
      "471 : train_accuracy:  0.7296 ; validation_accuracy:  0.71\n",
      "472 :  learning rate =  0.0009941507014756797\n",
      "--------------------------\n",
      "0 :  cross entropy loss =  53.9241 ; reg =  24.1379\n",
      "100 :  cross entropy loss =  24.4888 ; reg =  24.7246\n",
      "200 :  cross entropy loss =  42.7998 ; reg =  24.5366\n",
      "300 :  cross entropy loss =  38.1291 ; reg =  24.5471\n",
      "400 :  cross entropy loss =  43.1105 ; reg =  24.4104\n",
      "472 : train_accuracy:  0.7614 ; validation_accuracy:  0.718\n",
      "473 :  learning rate =  0.0009941507014756797\n",
      "--------------------------\n",
      "0 :  cross entropy loss =  23.9664 ; reg =  24.1638\n",
      "100 :  cross entropy loss =  23.9684 ; reg =  24.2827\n",
      "200 :  cross entropy loss =  27.1751 ; reg =  24.3027\n",
      "300 :  cross entropy loss =  17.4391 ; reg =  24.4691\n",
      "400 :  cross entropy loss =  47.1152 ; reg =  24.2197\n",
      "473 : train_accuracy:  0.7368 ; validation_accuracy:  0.7388\n",
      "474 :  learning rate =  0.0009941507014756797\n",
      "--------------------------\n",
      "0 :  cross entropy loss =  87.4404 ; reg =  24.2585\n",
      "100 :  cross entropy loss =  20.3804 ; reg =  24.7061\n",
      "200 :  cross entropy loss =  41.9024 ; reg =  24.4495\n",
      "300 :  cross entropy loss =  43.1885 ; reg =  24.4571\n",
      "400 :  cross entropy loss =  34.5597 ; reg =  24.4539\n",
      "474 : train_accuracy:  0.7232 ; validation_accuracy:  0.7024\n",
      "475 :  learning rate =  0.0009941507014756797\n",
      "--------------------------\n",
      "0 :  cross entropy loss =  60.9014 ; reg =  24.5116\n",
      "100 :  cross entropy loss =  28.6774 ; reg =  24.8075\n",
      "200 :  cross entropy loss =  35.8506 ; reg =  24.4912\n",
      "300 :  cross entropy loss =  37.3819 ; reg =  24.6344\n",
      "400 :  cross entropy loss =  34.4064 ; reg =  24.4643\n",
      "475 : train_accuracy:  0.7458 ; validation_accuracy:  0.712\n",
      "476 :  learning rate =  0.0009941507014756797\n",
      "--------------------------\n",
      "0 :  cross entropy loss =  52.4123 ; reg =  24.2338\n",
      "100 :  cross entropy loss =  48.1093 ; reg =  25.0629\n",
      "200 :  cross entropy loss =  24.5434 ; reg =  24.6496\n",
      "300 :  cross entropy loss =  29.8267 ; reg =  24.4424\n",
      "400 :  cross entropy loss =  44.8252 ; reg =  24.4533\n",
      "476 : train_accuracy:  0.751 ; validation_accuracy:  0.7232\n",
      "477 :  learning rate =  0.0009941507014756797\n",
      "--------------------------\n",
      "0 :  cross entropy loss =  29.6087 ; reg =  24.3858\n",
      "100 :  cross entropy loss =  37.1811 ; reg =  24.4266\n",
      "200 :  cross entropy loss =  18.5722 ; reg =  24.2243\n",
      "300 :  cross entropy loss =  40.3701 ; reg =  24.1001\n",
      "400 :  cross entropy loss =  41.116 ; reg =  23.973\n",
      "477 : train_accuracy:  0.7286 ; validation_accuracy:  0.7072\n",
      "478 :  learning rate =  0.0009941507014756797\n",
      "--------------------------\n",
      "0 :  cross entropy loss =  58.7684 ; reg =  23.661\n",
      "100 :  cross entropy loss =  40.8752 ; reg =  23.972\n",
      "200 :  cross entropy loss =  22.3749 ; reg =  24.0018\n",
      "300 :  cross entropy loss =  50.1849 ; reg =  24.4042\n",
      "400 :  cross entropy loss =  47.7472 ; reg =  24.2914\n",
      "478 : train_accuracy:  0.7432 ; validation_accuracy:  0.7368\n",
      "479 :  learning rate =  0.0009941507014756797\n",
      "--------------------------\n",
      "0 :  cross entropy loss =  61.9937 ; reg =  24.4701\n",
      "100 :  cross entropy loss =  36.6884 ; reg =  25.515\n",
      "200 :  cross entropy loss =  31.1759 ; reg =  25.1099\n",
      "300 :  cross entropy loss =  43.2908 ; reg =  24.6034\n",
      "400 :  cross entropy loss =  20.2045 ; reg =  24.2203\n",
      "479 : train_accuracy:  0.6926 ; validation_accuracy:  0.6572\n",
      "480 :  learning rate =  0.0009941507014756797\n",
      "--------------------------\n",
      "0 :  cross entropy loss =  89.3197 ; reg =  24.4202\n",
      "100 :  cross entropy loss =  17.1874 ; reg =  25.1348\n",
      "200 :  cross entropy loss =  26.6169 ; reg =  25.0015\n",
      "300 :  cross entropy loss =  29.102 ; reg =  24.712\n",
      "400 :  cross entropy loss =  37.1894 ; reg =  24.5226\n",
      "480 : train_accuracy:  0.7218 ; validation_accuracy:  0.7056\n",
      "481 :  learning rate =  0.0009941507014756797\n",
      "--------------------------\n",
      "0 :  cross entropy loss =  34.3481 ; reg =  24.421\n",
      "100 :  cross entropy loss =  41.8105 ; reg =  24.1799\n",
      "200 :  cross entropy loss =  14.9659 ; reg =  24.2088\n",
      "300 :  cross entropy loss =  21.0176 ; reg =  24.0827\n",
      "400 :  cross entropy loss =  48.5785 ; reg =  24.1897\n",
      "481 : train_accuracy:  0.7448 ; validation_accuracy:  0.7148\n",
      "482 :  learning rate =  0.0009941507014756797\n",
      "--------------------------\n",
      "0 :  cross entropy loss =  29.685 ; reg =  24.0354\n",
      "100 :  cross entropy loss =  27.5638 ; reg =  24.3307\n",
      "200 :  cross entropy loss =  24.9878 ; reg =  24.2077\n",
      "300 :  cross entropy loss =  34.2253 ; reg =  24.0406\n",
      "400 :  cross entropy loss =  36.7422 ; reg =  24.2387\n",
      "482 : train_accuracy:  0.73 ; validation_accuracy:  0.6964\n",
      "483 :  learning rate =  0.0009941507014756797\n",
      "--------------------------\n",
      "0 :  cross entropy loss =  48.0909 ; reg =  24.4531\n",
      "100 :  cross entropy loss =  34.5142 ; reg =  24.7797\n",
      "200 :  cross entropy loss =  17.4723 ; reg =  24.4278\n",
      "300 :  cross entropy loss =  43.9151 ; reg =  24.6906\n",
      "400 :  cross entropy loss =  33.8644 ; reg =  24.5402\n",
      "483 : train_accuracy:  0.756 ; validation_accuracy:  0.7268\n",
      "484 :  learning rate =  0.0009941507014756797\n",
      "--------------------------\n",
      "0 :  cross entropy loss =  45.3649 ; reg =  24.5836\n",
      "100 :  cross entropy loss =  32.637 ; reg =  24.5317\n",
      "200 :  cross entropy loss =  32.7099 ; reg =  24.5136\n",
      "300 :  cross entropy loss =  31.4381 ; reg =  24.2451\n",
      "400 :  cross entropy loss =  33.285 ; reg =  24.0388\n",
      "484 : train_accuracy:  0.7494 ; validation_accuracy:  0.714\n",
      "485 :  learning rate =  0.0009941507014756797\n",
      "--------------------------\n",
      "0 :  cross entropy loss =  57.4671 ; reg =  24.1907\n",
      "100 :  cross entropy loss =  31.1101 ; reg =  24.6142\n",
      "200 :  cross entropy loss =  37.4366 ; reg =  24.5992\n",
      "300 :  cross entropy loss =  31.5568 ; reg =  24.6072\n",
      "400 :  cross entropy loss =  35.4482 ; reg =  24.6646\n",
      "485 : train_accuracy:  0.7256 ; validation_accuracy:  0.698\n",
      "486 :  learning rate =  0.0009941507014756797\n",
      "--------------------------\n",
      "0 :  cross entropy loss =  56.5406 ; reg =  24.7146\n",
      "100 :  cross entropy loss =  30.6005 ; reg =  24.6117\n",
      "200 :  cross entropy loss =  26.3612 ; reg =  24.4202\n",
      "300 :  cross entropy loss =  41.2802 ; reg =  24.1861\n",
      "400 :  cross entropy loss =  48.0663 ; reg =  23.8639\n",
      "486 : train_accuracy:  0.7542 ; validation_accuracy:  0.724\n",
      "487 :  learning rate =  0.0009941507014756797\n",
      "--------------------------\n",
      "0 :  cross entropy loss =  121.211 ; reg =  24.0782\n",
      "100 :  cross entropy loss =  30.4978 ; reg =  24.9579\n",
      "200 :  cross entropy loss =  21.0852 ; reg =  24.9471\n",
      "300 :  cross entropy loss =  36.46 ; reg =  24.7694\n",
      "400 :  cross entropy loss =  58.3781 ; reg =  24.878\n",
      "487 : train_accuracy:  0.7174 ; validation_accuracy:  0.6748\n",
      "488 :  learning rate =  0.0009941507014756797\n",
      "--------------------------\n",
      "0 :  cross entropy loss =  33.1741 ; reg =  24.9845\n",
      "100 :  cross entropy loss =  34.2654 ; reg =  24.9585\n",
      "200 :  cross entropy loss =  46.7912 ; reg =  25.085\n",
      "300 :  cross entropy loss =  27.8982 ; reg =  24.8604\n",
      "400 :  cross entropy loss =  21.864 ; reg =  24.6082\n",
      "488 : train_accuracy:  0.7566 ; validation_accuracy:  0.7292\n",
      "489 :  learning rate =  0.0009941507014756797\n",
      "--------------------------\n",
      "0 :  cross entropy loss =  29.0269 ; reg =  24.5542\n",
      "100 :  cross entropy loss =  32.9277 ; reg =  24.8212\n",
      "200 :  cross entropy loss =  42.9351 ; reg =  24.8221\n",
      "300 :  cross entropy loss =  32.7616 ; reg =  24.6614\n",
      "400 :  cross entropy loss =  46.7707 ; reg =  24.5885\n",
      "489 : train_accuracy:  0.6998 ; validation_accuracy:  0.6672\n",
      "490 :  learning rate =  0.0009941507014756797\n",
      "--------------------------\n",
      "0 :  cross entropy loss =  66.7776 ; reg =  24.6509\n",
      "100 :  cross entropy loss =  22.7873 ; reg =  25.043\n",
      "200 :  cross entropy loss =  32.1042 ; reg =  24.7135\n",
      "300 :  cross entropy loss =  35.578 ; reg =  24.5687\n",
      "400 :  cross entropy loss =  33.9055 ; reg =  24.4619\n",
      "490 : train_accuracy:  0.7144 ; validation_accuracy:  0.6868\n",
      "491 :  learning rate =  0.0009941507014756797\n",
      "--------------------------\n",
      "0 :  cross entropy loss =  63.5336 ; reg =  24.5453\n",
      "100 :  cross entropy loss =  28.8398 ; reg =  24.8508\n",
      "200 :  cross entropy loss =  50.1963 ; reg =  24.7544\n",
      "300 :  cross entropy loss =  50.6142 ; reg =  24.5061\n",
      "400 :  cross entropy loss =  31.6237 ; reg =  24.7109\n",
      "491 : train_accuracy:  0.7132 ; validation_accuracy:  0.6732\n",
      "492 :  learning rate =  0.0009941507014756797\n",
      "--------------------------\n",
      "0 :  cross entropy loss =  88.2074 ; reg =  24.921\n",
      "100 :  cross entropy loss =  35.8133 ; reg =  25.7367\n",
      "200 :  cross entropy loss =  13.9449 ; reg =  25.3097\n",
      "300 :  cross entropy loss =  29.7707 ; reg =  24.9234\n",
      "400 :  cross entropy loss =  33.6585 ; reg =  24.7536\n",
      "492 : train_accuracy:  0.7384 ; validation_accuracy:  0.7044\n",
      "493 :  learning rate =  0.0009941507014756797\n",
      "--------------------------\n",
      "0 :  cross entropy loss =  48.6615 ; reg =  24.6788\n",
      "100 :  cross entropy loss =  49.745 ; reg =  25.3557\n",
      "200 :  cross entropy loss =  39.2924 ; reg =  25.2602\n",
      "300 :  cross entropy loss =  25.0657 ; reg =  24.7913\n",
      "400 :  cross entropy loss =  47.7459 ; reg =  24.6623\n",
      "493 : train_accuracy:  0.6924 ; validation_accuracy:  0.656\n",
      "494 :  learning rate =  0.0009941507014756797\n",
      "--------------------------\n",
      "0 :  cross entropy loss =  68.8959 ; reg =  24.7437\n",
      "100 :  cross entropy loss =  27.5179 ; reg =  25.4952\n",
      "200 :  cross entropy loss =  50.3391 ; reg =  25.1264\n",
      "300 :  cross entropy loss =  26.8041 ; reg =  24.676\n",
      "400 :  cross entropy loss =  28.9086 ; reg =  24.8462\n",
      "494 : train_accuracy:  0.7384 ; validation_accuracy:  0.714\n",
      "495 :  learning rate =  0.0009941507014756797\n",
      "--------------------------\n",
      "0 :  cross entropy loss =  62.874 ; reg =  24.759\n",
      "100 :  cross entropy loss =  41.7783 ; reg =  25.1577\n",
      "200 :  cross entropy loss =  35.5549 ; reg =  24.8637\n",
      "300 :  cross entropy loss =  29.5144 ; reg =  24.7566\n",
      "400 :  cross entropy loss =  42.5048 ; reg =  24.8049\n",
      "495 : train_accuracy:  0.7216 ; validation_accuracy:  0.6924\n",
      "496 :  learning rate =  0.0009941507014756797\n",
      "--------------------------\n",
      "0 :  cross entropy loss =  62.1978 ; reg =  24.6607\n",
      "100 :  cross entropy loss =  28.3402 ; reg =  25.0352\n",
      "200 :  cross entropy loss =  40.0877 ; reg =  24.9498\n",
      "300 :  cross entropy loss =  37.676 ; reg =  24.4577\n",
      "400 :  cross entropy loss =  70.5185 ; reg =  24.2553\n",
      "496 : train_accuracy:  0.7394 ; validation_accuracy:  0.6932\n",
      "497 :  learning rate =  0.0009941507014756797\n",
      "--------------------------\n",
      "0 :  cross entropy loss =  59.031 ; reg =  24.5379\n",
      "100 :  cross entropy loss =  31.5454 ; reg =  24.8784\n",
      "200 :  cross entropy loss =  41.6554 ; reg =  24.8233\n",
      "300 :  cross entropy loss =  25.9132 ; reg =  24.8613\n",
      "400 :  cross entropy loss =  35.3682 ; reg =  24.9526\n",
      "497 : train_accuracy:  0.729 ; validation_accuracy:  0.6872\n",
      "498 :  learning rate =  0.0009941507014756797\n",
      "--------------------------\n",
      "0 :  cross entropy loss =  34.0792 ; reg =  24.9305\n",
      "100 :  cross entropy loss =  23.6628 ; reg =  24.9454\n",
      "200 :  cross entropy loss =  31.0975 ; reg =  24.7564\n",
      "300 :  cross entropy loss =  36.9973 ; reg =  24.5664\n",
      "400 :  cross entropy loss =  41.6567 ; reg =  24.5551\n",
      "498 : train_accuracy:  0.7484 ; validation_accuracy:  0.7124\n",
      "499 :  learning rate =  0.0009941507014756797\n",
      "--------------------------\n",
      "0 :  cross entropy loss =  34.1239 ; reg =  24.5085\n",
      "100 :  cross entropy loss =  18.5188 ; reg =  25.0087\n",
      "200 :  cross entropy loss =  35.4518 ; reg =  24.981\n",
      "300 :  cross entropy loss =  32.1177 ; reg =  25.0221\n",
      "400 :  cross entropy loss =  33.0091 ; reg =  24.6747\n",
      "499 : train_accuracy:  0.7478 ; validation_accuracy:  0.718\n",
      "Model saved in file: ./tmp/one_layer/model_image64_adam_reg01.ckpt\n"
     ]
    }
   ],
   "source": [
    "train_losses = []\n",
    "train_accuracies = []\n",
    "validation_accuracies = []\n",
    "\n",
    "#Initialisation\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "\n",
    "    lr = 0.03\n",
    "\n",
    "    for k in range(EPOCHS):\n",
    "        \n",
    "        if lr > 0.001:\n",
    "            lr *= 0.99\n",
    "            \n",
    "        print(k, \": \", \"learning rate = \", lr)\n",
    "        mini_batches = get_mini_batches(X_train, y_train, batch_size = BATCH_SIZE)\n",
    "        \n",
    "        i = 0\n",
    "        print(\"--------------------------\")\n",
    "        \n",
    "        for mini_batch in mini_batches:\n",
    "            # load batch of images and correct answers\n",
    "\n",
    "            batch_X, batch_Y = mini_batch\n",
    "            \n",
    "            sess.run(train_step, feed_dict={X: batch_X, Y_: batch_Y, learning_rate: lr})\n",
    "\n",
    "            if i >= 0 and i % 100 == 0:\n",
    "                cross_entropy_loss, reg_loss = sess.run([cross_entropy, reg], feed_dict={X: batch_X, Y_: batch_Y})\n",
    "                loss = cross_entropy_loss + reg_loss\n",
    "                train_losses.append(loss)             \n",
    "                print(i, \":\", \" cross entropy loss = \", cross_entropy_loss, \"; reg = \", reg_loss)            \n",
    "            i+= 1\n",
    "            \n",
    "        #saver.save(sess, \"./tmp/one_layer/model_image64_adam.ckpt\")\n",
    "    \n",
    "        train_accuracy = sess.run(accuracy, feed_dict={X: X_train[:5000], Y_: y_train[:5000]})\n",
    "        validation_accuracy = sess.run(accuracy, feed_dict={X: X_validation, Y_: y_validation})\n",
    "        train_accuracies.append(train_accuracy)\n",
    "        validation_accuracies.append(validation_accuracy)\n",
    "    \n",
    "        print(k, \": train_accuracy: \", train_accuracy, \"; validation_accuracy: \", validation_accuracy)\n",
    "   \n",
    "    # Save the variables to disk.\n",
    "    save_path = saver.save(sess, \"./tmp/one_layer/model_image64_adam_reg01.ckpt\")\n",
    "    print(\"Model saved in file: %s\" % save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.text.Text at 0x7f8f4131b630>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYEAAAEWCAYAAACAOivfAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f8f4129d400>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(np.arange(0,500,0.2), train_losses)\n",
    "plt.title(\"train loss\")\n",
    "plt.xlabel(\"epoch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.2, 1)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAEWCAYAAACdaNcBAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAIABJREFUeJzsnXeYFdX5xz/v7duXhaVXKQIKNiyxGzFqLGgssUaN5Wdv\nSYyxl2hMjMaaGFs09t5RjBVREVABQar0vuyyfW8/vz9m5s7MbXvBBWU9n+fh2blnzsyce5f9nve+\n5z3vK0opNBqNRtO58PzQA9BoNBpNx6PFXaPRaDohWtw1Go2mE6LFXaPRaDohWtw1Go2mE6LFXaPR\naDohWtw1WRGRB0Tk2h96HIUgIktEZKx5fJWIPFxI3014zj4iMm9Tx6nRbEm0uHdCvo+AWSilzlVK\n3fw9xnCKOY5GEflCRPp+n/EUilLqVqXUWR1xLxFRIjLEce9PlFLbdsS9NZrNje+HHoBmyyMiPqVU\nfDPevxT4D3AY8B6wCxDeXM/TfD829/8HzQ+Dttw7GSLyBNAfeENEmkXkChEZaFqhZ4rIMuADs+8L\nIrJGRBpEZKKIbOe4z2Mi8mfzeH8RWSEivxORdSKyWkTOyDMMBcSBxUqppFJqqlJqfZ4x9xaRNhGp\ncrTtJCLrRcQvIoNF5AMRqTXbnhKRyhz3ukFEnnS8PlVElprXXp3WdzcR+VxE6s33dJ+IBMxzE81u\nM8zP8dfW5+C4foSIfGReP1tEjkz7/O4XkbdEpMn89jI4z2eQ73dRJCJ3mO+jQUQmiUiReW5vEfnM\nHMNyETndbP9IRM5y3ON0EZnkeK1E5AIRWQAsMNvuNu/RKCJfisg+jv5e0+X1nfl+vhSRfuZ7vCPt\nvbwuIpfleq+aLYMW906GUupUYBlwhFKqVCn1N8fp/YARwMHm67eBoUB34CvgqTy37glUAH2AM4H7\nRaRLjr5RYDrwvFOw84x5FfA5cIyj+STgRaVUDBDgL0Bvc/z9gBvau6+IjAT+BZxqXtsVcLqHEsBl\nQDfgZ8CBwPnmmPY1++xgfo7Ppd3bD7wBvIvx+V0EPCUiTrfNCcCNQBdgIXBLnuHm+138HePbz55A\nFXAFkBSRAeZ19wLVwI4Yn3uhHAXsDow0X08171EFPA28ICIh89zlwInAL4Fy4LdAK/A4cKKIeABE\npBsw1rxe80OilNL/Otk/YAkw1vF6IIY1vU2eayrNPhXm68eAP5vH+wNtgM/Rfx2wR457PWD+uwL4\nEqgy2/8M3JHjmrOAD8xjAZYD++boexTwdbb3iyH6T5rH1wHPOvqVYEw8Y3Pc91LgFcdrBQxxvN4f\nWGEe7wOsATyO888ANzg+v4cd534JzC3w95f6XWAYYG0Yk0x6vz85x5t27iPgLMfr04FJae/t5+2M\nY4P1XGAeMC5HvznAQebxhcD4H/pvQP9T2uf+E2O5dSAiXgxL8jgMqy9pnuoGNGS5tla5/bKtQGl6\nJxEpwbDs+yulVpuW+3vmAu9eGFZoNl4C7hWRXsAwczyfmPfsAdyNIahlGIK3oYD329v5npVSLSJS\n6xjrMOBOYAxQjLEG9WUB903dWymVdLQtxfhmY7HGcZz18zLHke93EQRCwHdZLu2Xo71QljtfiMjv\nMX53vTHEv9wcQ3vPehw4Bfif+fPu7zEmTQeh3TKdk1ypPp3tJwHjML5CV2BY92BYzd8HD+AF/ABK\nqSsxvu5Pxvi6/3bWgSm1AcPF8WtzbM8q0xQEbjXHPkopVY4hIIWMczWGKAEgIsUYrhmLfwFzgaHm\nfa8q8L4Aq4B+ljvCpD+wssDrneT7XazHWIzO5q9fnqMdoAVjwrLomaVP6v+D6V+/Ajge6KKUqsSY\n5K3PI9+zngTGicgOGG6zV3P002xBtLh3TtYC27TTpwyIALUYInBrRzxYKdUEvAP8U0R6mAuUH5jj\naSR/hNbTwG+AY3H7bMuAZqBBRPoAfyhwOC8Ch5uLjgHgJtz/58vMMTWLyHDgvLTr832OX2BY41eY\ni777A0cAzxY4Nic5fxfmN4NHgTvNhWeviPxMRIIYfvmxInK8iPhEpKuI7GheOh34lYgUixHOeWYB\nY4gDNYBPRK7DsNwtHgZuFpGhYjBaRLqaY1yBMYE/AbyklGrbhM9A08Foce+c/AW4xoyg+H2OPv/F\ncCOsBL7FsKw7ilMwhHEGhuV5BoZLxoMhVLl4HWNRcY1Saoaj/UZgZwxL8i3g5UIGoZSaDVyAMVGs\nxnDlrHB0+T2G1dwEPAQ8l3aLG4DHzc/x+LR7RzHE/FDzPf4T+I1Sam4hY0ujvd/F74FvMAS0Dvgr\nhq9/GYYv/3dm+3RgB/Oaf2CsL6zFcJvkWywHmIAxKc83xxLG7ba5E3ge49tVI/AIUOQ4/zgwCkPg\nNT8CxP7mq9FoNJuGiOyL4Z4ZoLSo/CjQlrtGo/lemGGhl2BEB2lh/5HQrriLyKNibFyZleO8iMg9\nIrJQRGaKyM4dP0yNRvNjRERGAPVAL+CuH3g4GgeFWO6PAYfkOX8ohp90KHAORgSCRqP5CaCUmqOU\nKlFK7amUavyhx6OxaVfclVITMRZrcjEO+K8ymAxUmrHKGo1Go/mB6IhNTH1wr6qvMNtWp3cUkXMw\nrHtKSkp2GT58eAc8XqPRaH46fPnll+uVUtXt9duiO1SVUg8CDwKMGTNGTZs2bUs+XqPRaLZ6RGRp\nIf06IlpmJY5dgBiJmTZll55Go9FoOoiOEPfXgd+YUTN7AA1KqQyXjEaj0Wi2HO26ZUTkGYxseN3M\nXNbXY+cNeQAYj7FLbiHGdux8eb41Go1GswVoV9yVUie2c15hbPHWaDQazY8EvUNVo9FoOiFa3DUa\njaYTosVdo9FoOiFa3DUajaYTosVdo9FoOiFa3DUajaYTosVdo9FoOiFa3DUajaYTosVdo9FoOiFa\n3DUajaYTosVdo9FoOiFa3DUajaYTosVdo9FoOiFa3DUajaYTosVdo9FoOiFa3DUajaYTosVdo9Fo\nOiFa3DUajaYTUpC4i8ghIjJPRBaKyJVZzncRkVdEZKaITBGR7Tt+qBqNRqMplHbFXUS8wP3AocBI\n4EQRGZnW7SpgulJqNPAb4O6OHqhGo9FoCqcQy303YKFSapFSKgo8C4xL6zMS+ABAKTUXGCgiPTp0\npBqNRqMpmELEvQ+w3PF6hdnmZAbwKwAR2Q0YAPTtiAFqNBqNZuPpqAXV24BKEZkOXAR8DSTSO4nI\nOSIyTUSm1dTUdNCjNRqNRpOOr4A+K4F+jtd9zbYUSqlG4AwAERFgMbAo/UZKqQeBBwHGjBmjNm3I\nGo1Go2mPQiz3qcBQERkkIgHgBOB1ZwcRqTTPAZwFTDQFX6PRaDQ/AO1a7kqpuIhcCEwAvMCjSqnZ\nInKuef4BYATwuIgoYDZw5mYcs0aj0WjaoRC3DEqp8cD4tLYHHMefA8M6dmgajUaj2VT0DlWNRqPp\nhGhx12g0mk6IFneNRqPphGhx12g0mk6IFneNRqPphGhx12g0mk6IFneNRqPphGhx12g0mk6IFneN\nRqPphGhx12g0mk6IFneNRqPphGhx12g0mk6IFneNRqPphGhx12g0mk6IFneNRqPphGhx12g0mk6I\nFneNRqPphGhx12g0mk6IFneNRqPphBQk7iJyiIjME5GFInJllvMVIvKGiMwQkdkickbHD1Wj0Wg0\nhdKuuIuIF7gfOBQYCZwoIiPTul0AfKuU2gHYH7hDRAIdPFaNRqPRFEghlvtuwEKl1CKlVBR4FhiX\n1kcBZSIiQClQB8Q7dKQajUajKZhCxL0PsNzxeoXZ5uQ+YASwCvgGuEQplUy/kYicIyLTRGRaTU3N\nJg5Zo9FoNO3RUQuqBwPTgd7AjsB9IlKe3kkp9aBSaoxSakx1dXUHPVqj0Wg06RQi7iuBfo7Xfc02\nJ2cALyuDhcBiYHjHDFGj0Wg6nmW1rdQ2R37oYWw2ChH3qcBQERlkLpKeALye1mcZcCCAiPQAtgUW\ndeRANRqNpiPZ9/YP2e/2j37oYWw22hV3pVQcuBCYAMwBnldKzRaRc0XkXLPbzcCeIvIN8D7wR6XU\n+s01aI1G07G8Nn0lUxbX/dDD2OI0R/LHfcxYXs/kRbVbaDQdi6+QTkqp8cD4tLYHHMergF907NA0\nGs3Gsv/tH7LP0GpuPmp7V3tDawyPB8pC/qzXXfLsdACW3HZYh46nORLnoYmL+L/9tqE40L7cvPL1\nCja0xPjt3oM6dBzpKKUK6jfu/k+Bjv9ctgR6h6pG04lYUtvKE5OXsqYhzJOTl6bad7jpXXa95b3U\na6UUyWRhAvd9eHf2Gu5+fwG3jp9TUP/LnpvBTW9+u5lHBW2xxGZ/xg+NFneNphNy8bNfc82rs1hW\n25pqC8fs6OS/vjOPba4anyHw05fXd+g4igNeAF75Kj0Gw83yulbemLEq9frd2Ws6dBzpNLZ1/m04\nWtw1mk5IW9SwTOtao652pRTrmyM88PF3AKxPixY56v5PaWiNccy/PmNpbUtBz1q8voVPF7qX2GKJ\nJHe/t4AVG9oAaInmt5SPvG8SFz3zder1OU982a4/PBxLcOKDk5m1sqGgcTppCsdSx//433wA/vji\nTAZe+VbW/rFExradHz1a3DWaToLTj1xRZPjW61ujJBzW+ctfrWTMn233zIr6tgzr/ZuVDXy5dANf\nL6unORJPieyc1Y18vWyDq29TOMYBf/+Ikx/+wtX+7JRl/OO9+alJxOKtmau5fcLcjLFvaI1ltLW1\nMyHMWF7P54tquf712a72VfVtzGjnG0hj2J447n5/AeFYguemLc/dv80YX01ThJe+XJH33j8WtLhr\nNJuZD+au5a2Zqzf7c6IO69IS9/XN0ZQwAXz13UqO8UzEyBgCKze0EYm7rdJ5a5sA2NAaZfvrJ7DL\nzf8D4NC7P+Hof34GGFYzwKQFtsXunFze/XYtAEGf13XvC57+ivs/dAt+Lt6ZtTrnwueH89axYF0z\nAEV+9zP2vO2D1EJoLpyWO8DMFbb1H41nWukN5md40TNf8bsXZrCqvi3rfa2JsiUSpz7tW9OWRou7\nRrOZ+e1j07jg6a82+fovl25gyfr2XSROkS4JGoL3+oxV1LbYInPEuge4I/AAe3oMa3fFhjYicbeF\nPG9NIwD1pjUdiSeJOyaOD+auZfi17zBrZQNTl9iWvHOR0rq2tiX7JiHrmfd/uDCnlX3ta7N5+auV\nxBNJHvj4O1qjhrWtlOKM/0zlmldnARAyxf3LpXW0ZHHl3PHuPCYvqiWeSLKuKQxAU9jdb/py+31k\ncwdZ4r6mIex6f07OenwaB975MeO/Wc12109gx5v+l/V9bSkKCoXUaDQ/HMf8y7CW2wvHizgWTC0f\n98T5NTw7ZVmqvajFWNgswRCplfWtroVWgK+WGWK7wWF5zl/bnDp+cKKxP3F5XStfOtw0DW0xigM+\nPp5fw6xVhiXsvLfTb33JM9O59oiR3D5hHk85onrS+d0LM/h04Xpe/nolG1qi/OmXI2hKE9+igJf3\n56zlzMencenYoRn3uPeDhdz7wUKOH9OXF75cwb0n7pQh7msa7EmoKRwjkVQsqrHfsyXuPq9hD9c0\nR/jL23P45fa92KFfJQDvzTG+rZz/1KZP5B2Jttw1mh+QqUvqaMhiBW4KTgt8u7VvsCR0EkGifLu6\nMdUeNq3fJAIYbplwWljgQtPdsdjxbeHUR2yf+uRFxmangM/Dd+ua6VYaBOChiYt5+asVnPboFLJ5\nU5yW/Tuz1/Dfz5cA0L08lPd9vfy1MSEt39BKMqmoaXJ/Gwj5PDxv+sudUTDxRNK13vD8tBUoBR/O\nrclwyzgXlpvCcY66/1N+/eDkVFtK3D3G5/ba9JX8++NF3LyRYZv/+XQxo26YwEMTN/8Gfi3uGs0W\nIt1/HE8kOe6Bzznxock5rtg4nG6ZoxufBKBa6vnsO3uHZSRqWONBDLFasK6Zmhz5VeasbkodO107\nFqvq22iOxNmxXwUAj366mMufn5FzfM6wTIAP564DoHdlfnG3GP/NGm4ZP4d1jRGu9z3OgZ4vAUgk\nFetMwXe6ZdpiiYyJy+rTmrZYu6q+DS8JgkRZuK6ZlWk+9dWmOybgMyTzZTO0c9aqBj7/rpbtrnun\noPfwxaI6msJxehX4nr8PWtw1mi3EvLVN1DlE0rJknZZ1Ok7L89pXZzFzRe4oEKdbJpwwLEw/xjMu\n9r7M6d53EHMh9c6jBnPGXgNZsaGN4x74POv90sMk05m7xhD/HU23RHscfu8k12vL1WMJZyE8Mmkx\nNc0RzvBN4JHAHQC0RhOpsTonob1u+4ClaRNKwOuhORInnLbOsGBdM4/4/8680Olc+tz0jOdON11V\n0XiSIbKC3hgLyeFYkhMfmpw31HP+2iaWrG/hvW/X8s7sNewztBuHj+5d8HveVLTPXaPZQhxy1yd0\nKw0w7ZqDgMJ2SVqLiABPTF7KK1+vZNaNB2ft63TLJDAWGSv8SYjC5f4XAfgsYRRRCyVaU+4UJ36v\nEEsUtnN13pomQDG6T0Z279S94okE28sSvlGDwHQFpbPA4c8HKKeF4bKMKWpERt/ykI+19e7+rbEE\n65uidKGRtfUlqfbGcJzb3nGHXQ7sVkxzJO6aCMFwu+wfyv2t4ytzbaGhLcbnwSuMe4Wf5o++Z2hU\nJfwrcWTW696auTpjMb13RVHO53Qk2nLXaLYg65tty9IpMPe8vyDlB567ppFHJy0GyHAfJLKkDEgm\nFd+saKChLcZffQ9ymncCCfNP++5jhvHgqbuk+nrFfObC/3Hg0n+k2reXRfSVdVQWu6tjfhy4lI8D\nl6ZeHz66F5ccaCxarm4IMzl4Ifs+M4zXA1dnjKt7WYifeb7ljeA1/NH3rOseTpzRKaNkEdOD5/B8\n8GaKybTomyNx7n7bLcIT59fgjzXwdehcjqx71HVubtq3ov5VJbRE4oRjCd4IXMW1vicynmGFiVoU\nB7ysa4owZXFdRnjjeb43+KP/WXLx+aLM/IndyrZMBVJtuWs0m5ldZB5BifFZ0p3My2m53/m/+dQ0\nRbj5qO055K5PADhjr4EZoX3JNL/9Y58u5oY37EW9JaGPAPg2OQCAAWUwYEDX1PkiTFfLdx8wHDhl\n1PE8+U0zbwavAeAXxa9T0xRhZ5nP12oIAzzrUtf2pJb7DhlJQ1Ff7n5/AXUtUXp6DYt2tGdxxvvu\nUR6krNFwi5zne4M74sehEHbsV8lXSzewyuGOCfk9ROJJ3jDHAVBCG2d5x/NWcne+U33YtkcZ89Y2\n2e8BUm19xRDRA9RUAr5WDvR+zd6Re6hvcy+cdin2M3tVnHA0zijPEkZ5lhDxlfC38K9SfYLEiBBw\nXBOgNdrG8f/+nHKawR1Wn8JLAg+KGD7GeSbRXepZ1XJ2Rr/06KTNhbbcNZo81LVEU5ESTpRSPDpp\nMesa2/cXvxS8kacDt2a0py/2PTF5KbNX2Ztp4kmVYblb4h6OJfhg7lqXsDuJW3/ar14AtQtT7eW4\nfdCnDHW/t16hOMNkOS8Hb2B/j9tCnhy6CO7ZkaC5qNgWS48HV+wuc/AR586jh3Kj/3EGi50vZmHo\nN7wWuokTd+tPdZnbJTS4upTB1aWutneDf+Ry/4v8N3AbAOcfMBiAErE/c8sK7ilGBE8jxZzm+19K\n7NM3JJUEfTRH4kjEXiw+nxcJYlvkzm8M5TTTL2h/Zr3ETov8y2Hu8U7tegNTg+cxTJZzd+CfXO1/\nOiPOvyzk4+x9tmFLoC13jSYPe978Jogw9y9Hu9q/WlbPTW9+y5dLN3D/yTtv0r2zba8/7B570TEa\nT2ZY7rGE4t73FzBlSR2fLMhdMiFpiXvjCvjnHqn2bv4wOB7bK74c6J56/fjaX/Gmb3cA+oltte/T\nR8AMurHEPX2imHJ4Ld3fu5n3B/2eA9dPhJXPUert4eqzPQsg6OPaw0dy7AOfM7R7KQvWNTM2MJs9\nEh+7+nYRw7few9fKU6funpoQih2W+69r7mOZ/JzeYg4uVIGl00d6PuX15F4A7NC3gjNKPmWZtyct\nkTjFYXdisu1kSeq4iChW9P6k4CWUN7QxkKcB6Cl2XP+9K45JHXs9QlXLdyDGpGSxocU9eT5z9h70\nrNj8kTKgLXeNJi9zQ2cw0eFztli4zrD8vB57kVApxWvTVxJLJFMpdd15W4zj/3y6mD+/+S2vTs+f\nKTEST9IaTXCL7xFO8H6Qar/jf/OzCrsH20otJ/uO1hLlFuTyliUZfX7uMaJFuosdmTMybi9MytpZ\nzA2exiiPO1a7+8IXADhwcBmsNMIU/ZJ90XjMwCq+u/WX7DWkGwCXrfkjP2sYn7WvjwR7DenGNt2M\nxVKnZX1k5A1u8z3MxTsbwr/zoJ6pc/cE7jdFW/GP3Zo4aumtHLPgCpIKgq2rnI/gEt/LqeMisSeP\ncjFCIosIc7nvefrL2tQ5b9IW7sqi7HnynZb7gK7FbNujFP57FHz5WNb+HYm23DWadnCKnIUVA+6M\nOJkwew2XPDudJetbeeubVSSSircu3gfLTiunlUZKuNHhSvGQpCd1rKJbxjOiprif7HsfgGcTP0+d\nO3n3/sxb08S0pYYlGSDGnf5/ps5v48meMleSbktSPr2L5wLucsfFprh1x37fI+KOfOwTbyckMX7p\nmWK8Hn0CzHwWlpkhlSoJYWMhsxs5MjbWL8PbWsvwlhmQ5b27SEThs/vw7XwqAF39ae8BRXnMFN1o\nk+tctdRzvmcG24x/DoA+9dMARVGrO9fPft6ZqePDh1dw/Lifs9dt9oR6pe8ZTvPlTifwV3VX1nZr\nAf0c7xscd9jZ+FdNg0UfwnZHZ+3fkWhx12g2gWV1hgXsdJtYW9qX1rakYrijiWRK3LtKI42qxHWf\n1wLXMMqzhB3D/6aeMte5aDxJa9jeTLOvZwYzk9tQTxnn7T+Yhz9ZzDdL11JOC3t5ZnO4152Z0UVR\nF2gzXQoePzhEfndPZpZGgF5ib346quV5+8S3rxm3sb4pdBlo/Eyan8X7N6a6hiTH7tu7RgFGQeav\nvZmLjhm8ezUs+5zp1z5K8RvPg2PIYQIEo+ZEFHZHx5TTwhX+51xt1TQQiNbZfouK/tBgp2i4bL++\ntKRZ4kPEbemnMzY5KccZRZAYV/mfIfHa27DTSeArgu2PydG/4yjILSMih4jIPBFZKCJXZjn/BxGZ\nbv6bJSIJEanq+OFqNN+PRFJl7JQslFkrG1K5w62YciuM77uaZu56bwGAa4PMBsemmircwhMkyijP\nEgCmh/6PUeJ2c7TFEjzwip2e97+Bv/KwuXGnqiRAVUmA+/z3MDV0AXcF/klOLpsNY23B5eBbYMyZ\n0GvHvO93oMMFkY2Un7tqI0vizXze9dLp787LgnepXPcFgbmvupp/vn1/JGyK+2r3BqQe4k5RDDBA\n1uBPOhY6i9OkKtZK8Sd/YXbwDKLKCI0Z7rHFn367FzZejE1kVnSPN1IPjSuhsh8ES9u58vvTrriL\niBe4HzgUGAmcKCIjnX2UUrcrpXZUSu0I/An4WCn106u2q/nRc/f7C9j39g83SeAPv3cSh987iVe+\nXpHasdhoxqYffs+k1JZ150LpGY9NTR2/FLyRP/seSb0uxb3F/Xr/f12vl65aw/vBP7jahoshMkV+\nL1UlAQ7yZiapivnd3wCo6AsBxzeGkmo4/E7ot1vGtcqx0aifpybjvJOUu6qiX95+GbzsttQHSYHp\nkBNRWPxJZrsItGXfuZttghroWesKp8wQ99mvIpP+TolEaMD43LqKw93T1UxOtmv73zh28cxneuj/\n7IbmGijpnvuCDqQQy303YKFSapFSKgo8C4zL0/9E4JmOGJxG09FMNvOsrGrIno/bSa5c4pc9NyO1\n3bwpHEcp5YpZ/3CeLYqLatwLm6eY/nNwh/QBjPHMpye19GY9B3umUFOT6TcXFN1oQFrWUx6ANpW5\nISZelMWH7RT3oCX+mTtGG4I9M9pyUSXmN5HSHvk75qLrENp85Qz35C6SAQKXfQvnmf78Oa9ndvn2\nNdiQGWcPMMSTtmgtXsaUbbDFvesQ2OsSd5/pT6YOqyVLaogBexoT2q5nsmS78/OMHU4IpbnKWtZB\naXXeazqKQsS9D+D89FeYbRmISDFwCPBSjvPniMg0EZlWU5PfKtBoNgc+ryFo8YRi1soGl9skHec2\n/PSc50d5JnGe93WawjHXrlOAwz2f04caDhnehSoaaVXumO6e5SEGdi3OsNwBHg7cwf+Cf+DfgbtY\nvyHTGi2RCNNC58Hfh/CzyeemLEsnyfbE3TpWmZEsDYHCxb0LTUQ9ISP8MAvKX5z/Bic8Q6jbIKol\nx6Jrj1FwXR1U9IHq4ca6QY3D2R5o37XRy4rf3OtSOH8yVG3D6MAqiiRKbbAfXPSl8Zw8JD1pE2jP\n7eGyWdB9BAlJO5dmlY8dlJbi4UdmuW8MRwCf5nLJKKUeVEqNUUqNqa7eMrOXRuPEyscdTSQ4/N5J\nHHm/vRCWTCqueHFGaiORU9BX1but7LsC/+SP/meR9fOYs7KWucHTONn7Hl4S3Be4l09Dl3DbmrP5\nKnQuAexFxYjycdjoXnz0hwNSOdVT7HEB23uWUGJGq2zYkN+zWb3uM1fctYUqcfxtHfo346dL3E1R\n3OUM8LrFpzFYQEKrUmMCCEiCuLcot/94wJ7Z2/e4APb/E3QdgljjEg90G5bqslq6w1nvgceUKI8H\nhox13+f386HakX9mp1MyHtXNsrz77ALdR0CfnRkQnksxEaJivvcck5NFPJB2vtje8ZvwGPdo9lYY\n7+myWTDCzjNTmrYXgEjDj8pyXwk4nWp9zbZsnIB2yWh+xPglSX9ZS3PEEO7ldbb1XNMc4flpK/jN\nI0aInzOF7uIa2+d6hOez1PF+nplc9thHhCTGLf5HGWumoQWoDBu1Nn1i36eWckJ+48+uRNIs9xGH\nu14uXp4/Dj4nprh/5dkedjf9vSFH5kZLUHtuD9euc10a9qX569Mp7wtH3pt6qfwl4AvBgL3tiQTg\n6jVIj+0zrz/oJvjFn2H/Kw3BtsZS3sewzE3apAj8aZt9Rp8AgTI4/B/wpxXGtXHHZ9h/T7jYvaBq\nhXWmJqAGdl9aAAAgAElEQVTeO1MSXc9AWUOL5dLyZgkaPPrB1KG/NM0n7xD3wb2Mc0VdehnvyRd0\n+/DXfJN5b0/2mPiOphBxnwoMFZFBIhLAEPAMx5eIVAD7Aa917BA1mo7jqIanmBi8jMT6zDqe1lZ1\nK21sxOFHf2mKHclyb+C+1HGVNFLqEOl/B7LHO68acSYt/i4Idl3R0nTLvcRt0VVghFNuKNs2+5sZ\nfKB9/NsJqUNfmfG1vyLgWDOo6GsfB3MLuJJ2JOHy2TDsF0SUIYi+UImxoHnGW8ZEssNJRj9/EYSy\nZIsccYRtjQMETNdNRT/3t4tsDB0LV62AMb+130OLYzOXShqROxdnpuwlYPavNj7LoZ6VhCXLTtHq\nEbDL6dBrh1STOCad1Hsz8foNy93rcwq2Yy2jxeF+PvR242fv/FFKHUW74q6UigMXAhOAOcDzSqnZ\nInKuiJzr6Ho08K5Sqv1ijxrND8SwiLFZJVG3lIu9L7Ob2Jtz0v3q4YgdUbFq7hSuGNXCLn53SbgL\nfK9zg+/xdp+bKOvDyr6HU0Yrw3saQnPTIQPcndLEvdLcfh8L5nAbdDGvH30C9LdTDIS6GG6TQV0c\n/mCfw/3Snojucjrs+4e8XazEWoGiNAEfdz9cYwqa5e7wOVLcBtImFr85ltLuBfnQMzjmYfvYmkyy\n+foty73U9ncP6Z3FPXL+53DE3W5XTbq4O7E+V6c1nmuC3GZ/uGq18XMLUNAmJqXUeGB8WtsDaa8f\nAx7rqIFpNN+XaUvqOPaBz3n1gr1SBSXipp+1tbUlleMcfg9AW9SdZCrqEPdXgtfDAjg/S0bAA7y5\n84Cn7oWfof17I4vD/GJENTx1PFULJrg7hSrAGzBC/oBKM4VALJBWDKPrEDj/C/joL8Zrf1p+cHOS\n8CRyLBbnWehUeAxxA+g5CtYvgA9uNl5fOM2+RagYIq1IIO1eHg9YC5CWK6ioEpra7PfoxJpoSqqN\nqBszGqbIX+By4LaHGouu88bDcNOtlT4msCcOxwQaKnZMNOdOMj4XEXvMFk5x/9VD7vt6zffqcUip\nZM9bj78o+9g2E3qHqqbTMnG+YUF+NG9dStwj5n/5tla74MPAK9+id0WIk3bvn2r7cO463vhiHnd2\n0FhCRcVIyJwZbkrz4Z73mfGHL2JEUjQavnrLcg/70gSxpNrwE1u+XZXMPA+pSSJFWW9oWpUpPt4g\nJMyJzHlu5DhorTMmkVNehm528emiomKIkN/atoS8vDcMOxhGHQe+tOgSy/ItqYZ9LofWWvjiX/Ro\np66qC4/XcPdYZJu8rEmkqArDbaLck2LPtIgZ5zmn0I8+Pm385ji9Dsu9NEfEUXvRQx2MThym6ZSs\naQjz7rfGBhbnwmhYGX+E3lZ34q1VDWEe+2xJ6vUZj03l4zmbuKCZhT7dKiHocGFUOlwyPbaDKjMN\nrCOSosIU92ZPmiuj/8+Mn9b9EmY0jiW0lqWZlkeGcz+Bc9yZFwE4yrm7NU34i6vgulrYZj93u+Vq\nyWeJptwyIePbwMC9M/vEWu0xe7ypReXvJUweL1wyE352od1m+eidk2J77imLgtwyjq90e18K+1+V\n2Tf9G9ZmRou75gfhw7df4psZ09rvmAUrI6OTLxbV8sWiWmOB7c3LePqzeakan86KR23mQmAw7N5n\ncb3vcXZtde9+9JEWB37lMjYZbwDCZjz3gL2MmOts7HgyicFGGb4K0y0T8TkmhV89BD+/1hygKSyW\n1X3mu3DA1bYYOaI6ACjpln0xb9SxfD7wAqCABVUL69n5BNKafLx5Kg/FTHeNNUmkhWZuMl0GOCxu\ncVvW1jeb9sQ25VbKI+7WRjenz93rN9YtwJ3iwbdlUv1aaHHXbF7qFsPrF9vWpckBX/yWUa8cmOOi\n3Lz37VrG3jmRN2a4Ezl9/MiVTHrkCqITroNpj9J96Xh2lvksCZ1EYN3MVHm6ZMIQ+kCbLe5CkjN8\nE/hX4G7XPf1iJwVr9pS3Gw+dTmOvvewFQ18QBptZHX95uyFmxz9hxEY72e1sZG8jxXCltADCTsMM\nK18VdzXcAla0iSVYlvulx3aw3xVQ1tNY2Pz1UwWP1UqnO7BrgdasJYz+PP1TlnsewY6a8ReWyyLl\ntimsjmterHt600IPLdFuz01iuaE8ebzX1rej9GeU9YDD74KTX7DbPFtWbrW4azYvTx0HXz1uLMx1\nAIvXG2JgFSy2uML/PL/zv8g3y42NP0NXvcrLwRsA8Cz6gL+/Ow8Ar5kwyplQ6l/DMvOzgBHDbuHt\naaZTslwiwLeDToOB++Qc6/rBR9tuFl8QeoyEGxoMEQYYeaQRG52GxxTOSprBX4w/aAiopG/z726O\naZsDMh++0ylQ3iuzPQc9+hlVjnoOGNZOT5NCLHdL3PNZ7tY3ia7G8/MK6cZiWcrpY7Q+/3g7VbT2\nvNj4WT08dx8rE2a2cY85wxWds6XR4q7ZvNSaoi7uohbtUd8a5Z73F6Qs7mg8yWH3fMK0pYZ4VzbM\nhUhzxnV1rcYf2yixc41c4X+Olm/eYt6aJpJRw8frzNF+yDJ72dQot6Yoo5Vb/Hax5aKjzY07p7zE\nspARK50IVmZNvmXRt0e3wgQuHVM4K6TFsPAtazZdKKq3hT98Z8R9f19G/xpOegF2Pq2w/q3m5Jgn\nZh5/keGuyGe573WpsaCciivPEWmyKViWefqi7y9uhp1/A9sfm//6kUfCdRtSsfFZsZKmOUJRfyxo\ncddsEeoabT95NJ69Oo+Ta1+bzZ3/m8/kRUZukPXNEWavamTC7LV0pYFLFv4W3r4i47qImf+8WNy1\nKy9vu5eD75pIkVmDzZmr3CKmvEwJns+Rns9cmQqbjn4Cqk2LNlBCTcDYEKS8QVs4nD5X05oPhMps\ncY+1n6gshSmG1dJg+KDNCSlrgq6SbrlD7zYGERj2i8Lv1WS6xZxRKtnuGarIL+4er21JW9d0FNYO\n13T3S6DE2GXbY2TmNRnj8+T3lffZGS6Y6l68TeesD+C4x9p/VgejQyE1m4+kvZBZU9+IFQAYDkdp\nb9kstm4+j/r/SbTlUaBbyoL3E+cOv7HFQjWszLDzKpMbslanTyrYxzMzlYnRqs/pxEOSCmlld98C\nPHH720VZcfrCm/HUhCdoW65dBtiFqK1vJr6AkZRq8cRNstwBIyzS8ks7d5n+0Jz4nFHgor1c7ofd\nYUcCFYI1ge1w4qaPzaKQiJ6C7tPOQmh1O66svrsY/7YwWtw1mw9HKJ4/aS9OtoVbaG9p8jeND7Gn\ndzq8sjM03UBk2FkA7CwL2N/cNPRluDdjwBZTspfEA6hSG3gicFv2h+1xPq3TX04VTR7uW0Vt0iEI\n6YtlpnUZ9wRsf26XQba4p/p5YOwNhusmWxhgLpxisuvZsNPJsGEJ7H154ffY3PTb1fjXHtsdtXH3\nLa6Ca9Zt3GSYC+v3tik7X51s4YXQjmLrHLVm68CxiSYWa2N1Qxut0TjhtvYLZcQTjo05791AOGLc\nq1Tsa1fWG8fROjsjdbbKO+0y+njqBhySejlIraSfOEIl0xI9WVNJUjy2399lVVs9xLDetztq49wN\nTmE7+BZjAjnk1i1SvedHgS/YMe4Za8F0C28e+rGgxV2z+Yg7xD0S5md/+YATHpxMJGwLdF1TK+99\n4U70pJQinnTvulQNxq7NYkcFne7Fxn/fwL327sIKKaDCkmMzUcO4x6H3TnTpYu8arVIb3OKeYUWa\nfzYKW9Sdm3wO+Qv02wN679T+WLLhtNzz+as1+bFCHh1JwH5KaHHX8PQXyxh3/6cdf2OH5R6PGlbU\nzBUNRB3i/sW/z2Ps2/uxdq1ddSgST7pcLQDhNfMBKHIslAaIoRI5CjDnw7E71NvFEOeSMvdGFWe0\nTbpbxuMxrEpBwfBfGnlenNXse+0AZ07YdF+vFvSOYeBecOqrWcNNN5pAKYw6vv1+PyK0uGu46pVv\nmLHc9lU3LPgcbqigZVmW1Kkbg0Pcl6zdwBuBqzjL+xbRsB05sn2rUWO0tW4VU5fUcejdn7C0ttVV\n4AIgsvpb3gv8nnFmLvVmiiARc+WIKZiwXfknEDQX3dJcHkFxPD9N3LfvY6wY7NTP/AbQPU8c9KbQ\nkREjP3UGH+BODbCpXLUSjnmo/X4/IrS4a1LETD937fQ3jJ9TX0yd+3h+DbXNptWcTMLab7Pe48nJ\nS7n21VnGC4e4fzZ/FaM8S7jG/xSxiG25x801/XBrEy99uYK5q+v52ztzqXJEs7SoIOXrpjLEs4q9\nvLMBaJZSJBmluaVwcW8lZMQ3H/rXVJs/YEVU5InXTnPLBHzGmP2eNBH2+N3bzTWaHxAt7j9RlFLu\nRUtIFXlu8xv+52Sz4XeOJZKc9ugUTnrILPY78W/wr5/B2tmpax//bAnrmsJc8+osnphs5DyPRe0d\ngKd7300dxyO25R42k3otWr6aoTUTWBw6hWVLFtDVUVdznupH35ZZqdcJJbRJMZ5ElNbmwsW9yVNu\nxDcPsxdPxYqFdmzGaS5KKxGcbvlZO0Kd8dlgRHmc/WHB42mXLgM77l6anxxa3H+iXP38FPa4+lkA\nymlhiKygLWqIe8Rj+Iql1djo0xZLcJH3ZbzrzJJhiycaP80qOItqmrn+9dlc+NTXrmeEw7a4j/TY\nRS5iEeeGHsP6fWPKHLZZaXxjGBOblkqaBfAt21CVdEfBxMWPJKO0OtwyYX/+AMtWr+lGcYa2WYmq\nKu1Kki1VaeXh4mmpc0cfZ+wM7ZMWu+zxdFzY3CUz4f8mdsy9ND9JtLj/RDlg1p+YFjqPVR89wszQ\n2bwXvCIl7sm44X7xthniHm5t4Xf+FxkfNNOYWptqfEFoWkMiYrze0BrlCM9nXOx9mSP+8T5/eS17\n1sd41Bb3bqaFXiEtxEwXzd6ebwhJjDXbngq/X0h9hXsnoVcUSY8fTzJG2CHurdU7253SMyICUX+W\nsm/W1v4u9macUG/jec2Df2lEXFT2y7yupFvW99ZhdBmw0YnKNBonWtx/ouzrNZJi9f7I3hjTGjEW\nEZVpqQaihrUcaXFsDGpaY+fgjofhjm3pM/43qdP3Bu7jcv+LvFZ/DLc2ZI9SSDrEvdoU94Gyhp1K\njMmkv5hFmyv6QWk1A4aMyLhHwuNHxaP85TUj6dfJ0T+RqHb0K++TcY03W9X5VHIpO7Klop/hbind\n9RS4cmnheb81mh8RBYm7iBwiIvNEZKGIZP2LFZH9RWS6iMwWkSwVATRZWfSxnTtkYxl/BUx9xN2m\nFLRk5k1JZ3EgMxlSpM2wwJVpuQfiTTD5Afo94ogT3rDEzpNSZxSNLl41mbnB09gr9nmqm0dyJwdr\nackss3ue7w2qw0sAW/C9ITMNbe/eGf2T4icRixASYyI6Yc9hVA/b3e5w5L0Z10ifLHHn2bL5Ddwb\n+u6qF0c1WzXtiruIeIH7gUOBkcCJIjIyrU8l8E/gSKXUdsBxm2GsnY/mGvjvkTD75U27fsq/4a20\nLelTHoLbt4Ha7/JeGkhmpjsd8fKB0LYBYsY5bzIK7/zR1UdtWEosbLhCFnz1Uao9JDFOjzxZ0LAb\n8iyCNlZsS3eMbwoBU9zLumS6QJLeAMWEuc73BAAHbD/AnaHRSiVb2oP1VYa7pue2WTI4OsMOT30F\nfvWwkSr3rPegItP612i2FgrJLbMbsFAptQhARJ4FxgHOWLiTgJeVUssAlFLrOnqgnZJIo/kzs7LQ\nJjP3TeNn/TLoOph3Zq2mvjXGCbv1d3UrSmYKbKh1NdE5b6dCGMuSjRl9onVLwfSxB9YaC6gthCih\nndzYDpobDfGeW30Iw2vesU8cdgetC6dT3mDkXvcXGeLepSrTnaI8Abb1rEi9Li4uNWp1dhsGuxp5\naLh0FgRL6bZ+IXz8V0qGOPK7HPMIzHvbfVOrmIZG0wkoRNz7AMsdr1cAu6f1GQb4ReQjoAy4Wyn1\n3/Qbicg5wDkA/fv3Tz/908NamMxVpT4fuXKipxUPOPdJwyedLu4lyWYmlR9Gv8QyBrR8k2pfsLLG\nLtuWhboVC+llps3tkTB2lcaVBwQ8qv1UvgAXxR8DIOF3xJaf9xn02I7itdekmoKW5V6RVlAaSHrc\nsecey2d+4VS70VoI7bcrnPKiqz+jjjX+aTSdlI5aUPUBuwCHAQcD14pIRh5MpdSDSqkxSqkx1dVZ\nFrd+apgLk5aPe6PIVUXGEndzwjjJ+z6X+dKELZmklBZaA11Z0NVd6s4Xb0NyjCemvDSus0MaQ6bI\nW1Z7mSo85jwpXhLmxqEFnkGpmPHySjvKxWtWIPJ40+LMB+yVmalxCxcf1mh+7BRiua8EnLFgfc02\nJyuAWqVUC9AiIhOBHYD5HTLKTkp9QwOVwDdL1zF6Yy/OUoUIoC0cpgiItjUR2LCEW/3Ggms88Qg+\nrzmXR5vwoIgHyvGnhdvFW9bTFs6+wFsnlagsz/WJsRGpQjUVXEhHFXdNCbJy2hjOyj7ZsvldUwMe\nL+pfZ7rbtbhrNC4KsdynAkNFZJCIBIATgNfT+rwG7C0iPhEpxnDbzOnYoXY+1tUaJeMWrdn4NLUq\nh59+ZZ0hvivXrYe77SiX+jZHrpQ2w+edCFTgSQvz+2buAtbUZvraARq8XShK5F4fyBchY3F//EgA\nvMFSR4Isx3WOjI1ZBdsXAI8X8aVlavRpcddonLQr7kqpOHAhMAFDsJ9XSs0WkXNF5FyzzxzgHWAm\nMAV4WCk1K9c9f3J88yIseA+ALxbVMnWJIeqxsOFzD3kK81U7iba6BTieSHLP+wuImzHqoaalrvMb\nzDJ3rZP/Q3L+BABUsBy/z/1foLvUu5JmJZVtirf6qyjOshC7MRx+2DjjYJfT8Zhb/z040iC0Z7lb\npKfh3UoLKmg0m4uCKjEppcYD49PaHkh7fTtwe8cNrRPxkulCuKGBdx65niQedr3lHhKmi6PIE89z\nscHi9S0kkknKXjiexKD9KRu8h6tU3Ydfz+XIj49ioGctAKF17oyO7zz3L4a03E9x3IhRX5Tsybpu\nu1MVnunqt6NnoStpVxQfITNDYzRQSWl4EyJ7PP5UVSZfl4FwxWIo6oLnjbsBEFWA5d51qF1sGzIt\nd41G40KbOx1EYsXXRN/PUcbNwfX+J7jR/zhgbxoK5RJ3pWDGsxBr44C/f8TYOyfSo+Yzek+5lYjT\nck8m6VIzJSXsACXr3aJ9UcPfkbi9M/TJxEF4iytZ13NfXojvm2qvSqstGsVeuFT+4lSB6Vw0emwf\nfhOm5f0nO2TRFwwZpdREUkm7xOWWcVruDnE/7zO42n5/Xr9jajsmbSOXRqPRNVQ7Cu/D+xt1mX/+\nx4LzccfaDCENSg63zNw34ZX/g9qFjPO0EBB7Eoi3OSzoWCtJn9uFEYhmryVqMSW5LdsGvHj9Xv4Q\nP5vjfNmTVCU9/pRLXBVQrqymeDDlzUb4ZVnX3rBhqV2FHvAFbMH2ZhN3ZzFl5/PSLPWSYuNcnSql\nSoc0ajQZaHHvYJKxCJ5AyI5DzyP0cXOnpzcZhUQcHjkIDrgaho4FoG7pbKqApWvWc3fgP65rY222\n5Z6MtpJoy74Imo0vk0OZrQYS8nvxiJAgdzGD0q69OXblJQzyrOEYf+Yk1OaroChup+eNBBwx6SXd\noXG1q78/aAu9N2iJu9Pn7iiakadIcmmJIe7ObxYajcZGu2U6mJiVd+XGSiO1QB4SZk6ZZCzCnW9O\nhVVfwUo7k2LNmmUAzK7PFLCEw3KPtrUQd4h7swpl9HdyU+xUFB42tETZsZ9RZ3J1v8OoKxni7jj2\nRnynPM+fzv0tex57SUZkDUCypIfrdSTUDcp6w7j7jcyJafHoqcpHOC33HOSZGMvLDfdPqB03kUbz\nU0WL+6byyMHwVGZNxXjEsblocTv5uE1xb2hq5rnPFxptjhJw/lYji0N1bFXGpUnHwma4tTEVGnlY\n5BaWK/cGsRrc9UGvP35PTti1H0fu2Id+VcUsue0wep35NGuGnuh+yN6XQmV/dhnQhaN36otkqQnq\nD7nbVKAUfjcHdjoF+o7JKGjhd4i7P5vlDnDuJGNyyENoqFEwo1Iyk5BpNBot7pvO8smwYEJGcyxS\neI4VX8Kw8v0k7PDDsG2BB9uMBcQ+bXMzrg00Lkkdh969IpWfZq7qTxvuAsstHncJuW5dq7ntmNFU\nlbjdHoP69so7Xm/QFvK/xk4AQPVzZ6IQp598r0vgDFeQFV5H8WfL/+5JT6XQc5QxOeSj62AAJiW2\ny99Po/mJosW9g0lE29rvZOI3MzMGJEbQDDdsbapLnbfSAHQPL864tu+y11LHoVWTIdJEmwpw9n7D\nWKncWRRTFYhMisrclnyqvTR7u4WVDgBgl/3HMeeMeXh3OD6tTzubiRzx6J5sC6obwcJzvmPAJZkT\nrEaj0eK+cdzSGx53+9Fj0Uja6zTLPWm7HGJpNUuttLsBYgRN3/GcJXbYYNLMD+PDWMiMKvfC5ypV\nRY0yhLsosp42TzFXHjqcm2Kn8kVyODUewz0T9rnFvay0lKyE7H5Xxc7MOO0P2dcN6d2VEQN64iuq\ndPVxTgBOYuMeIDZwf3ejuWC6qeI+pHc3+nXLU9hao/kJo8V9Y4i1wOKPU/nOAZYuXeTqkoyGjciX\nVIO927O+yc7Zkkwk8CvjPn4SKcs9mDB8yE9OXkos6l4sDKSFTNarMv4cM9wX1ZEltInhEqmhC7+O\nXsfioFGQIxZw548J+nL82k2xnZ7chufU2IzTvpAt3Kmol5B74vDlEHf/TifiP/01V1vXCmNc3Up0\nxItG09Focd8EEm12DPm6lW6XSSIWdqfMdaTzbWyy/enRSBtB03IvlTYu9r1iHKtWWqNxrnl1FpLM\nv3M17gkSLTb85L2iy4h43YubCTEiXWMBt3UtuaJQzGiY/kO25+M/7J9xurjYttwD1kJq0G05py+w\n5sPjM8YX8BaYbUyj0RSMjnPfBFoaarHsVWla4zqXiLWRjIZTs6aKR1KhfvUNdoKwSLiNoDImgb6y\nnr7e9QCU0srqBtOil/ziXhIQjt53V3gPgkSJeQ1xvvHI7YglkqjJphvHETu+a/ifTM12MzAiW479\nD1VDD6IqmCnSVVV2Ot6gtRnJ5w67DIQ2ot6omJ+SSubvp9FoNpqt03J/5Vx47tQt+0xHREdLo12j\nVEXd2/WT0TDNjhqhiZhtxdfXrk8dxyKthLJULyqjlbUNtrvGYnGyR0bfYh8UdbWzMcfN/Oin7TmQ\ns/bZJjWp+AK2AF934gHZ35/F9r/KsMZTz6uyy84FikzxT/sWsFHiXmQu4O5yeuHXaDSagtg6xX3G\nMzAnLetw7XdwQwWs+Sb7Nd8XR3GMsEvczY1IZvbEZDxCc4st+PGIHT1Tv8pObx+L2Ja7k6DEWFvX\nyOGez+kutvtnhX9gRt/SgFBVUUatMsQ4UpQ+ARhj8vpsn/YRO2QWmy6YYnv3aTCYfaNUoGgjxD1Q\nAtfWwn5XbPqYNBpNVrZOcc+GVQ9zxrObdv2q6fDwQVC3KOtpFbWt8UizI/961L2J5p4Js/hutcNC\nb7EF+ph5v7cvi4QpInvFo7Z133Ff4F5XW1PF0NTxA/HDASjzC11LgtQrw+2SKOmZ9X54A5wS/RMH\nRf6W/XyhOKx08WUX92BRjkicXHh9Befi0Wg0hdN5xN3KQ7IpJesAln8BK6bAS0Zx5VjdMqI1ttBb\nGRwB4k019nVWqTzz5T2B+/lsqp1CwLn46iTc0kgwh0+9eOUnqeM2FWBWciAlXewNRvvuMNw4UAmq\nSgIkzV+jKnNvQrI00+P1Myk5igWqb9bnbRI58qcXFW+kuGs0ms1C51lQtbIG5inunBdrcW/DUnjr\nd/inPmy8vsFIBxBpbcKyVb0Ny2hVQQSFpMRdsCR+TO3rWLm4kq3ZqyxFmw3rPuwrIxR350ivWP1p\n6vjl4JHM2+4yfhl5J9WmrBwvyQQBnyc1sfgr++BEWV53X4Bjd+nLqD7ukMgOw1+cmuR0uTuN5sfB\n1me5J7Onx62PGm+lqSVL/c8c17hImPHorevBEnYHkTbbjx5oWsZyVU1Ygnjiray/a59UHVGAA71f\n24/OYbnHmwy//erqvTPOjVbzUscDqiu4adz2jpJ00FYxGHY8BY5/3GwxRDxU6c4pk7LcPT7+ftwO\nnLbnwKxj2ShOegH2utTd9jt7vM5xajSaH46tT9xz1A6dvsoQ9aVr69wn1n4LN1XBwvfguw9g9ivw\n6T22j97EKk+XwayXAYg6xL2yeRFrPD0ISwhvvI1u9TOzXwskWgzLfX2gb1q7Mc6a7pni3lUc79Fj\nfrlypL8VfwiOut/IwQL8LXAe05LDCPQY7rqP5cmWAmqbFsywX8BBN7rbQuWw35XGsX8jFlQ1Gs1m\noyBxF5FDRGSeiCwUkSuznN9fRBpEZLr577qOH6pJJHve8mDAED+VLtKT7jQu+/p5eOJoeOF0+N+1\n8MwJrm6rcxSF5sUzIB4l5vC5d42uoC7Qi4iE8Mfz1xS1/PNTRt/MBuXwR7cZ4i7BUiNiJAdipsz1\nOIpVeNKs49N/fQLXVN1Bn+oqspGel2uzcMCfDBeWLn+n0fwoaNfnLiJe4H7gIGAFMFVEXldKfZvW\n9ROl1OGbYYxuHFkTUSrleyi2ov3Sfe5rjWEGZz+X97YNzS30y3Uy2kw84hbxxmBvosn5dI8sy3vf\n4HojNLO6SykRR2GJhlpj85MnWGxEjOTCyr/iEHRPWo70vYd2451L9yUDcx1BbRF112g0PyYKsdx3\nAxYqpRYppaLAs8C4zTusPDgtd8f2fJ+ZE1wc2/1RCuqX5r/f+gVwQwXV6yfn7LL3za+zdM16V1u4\nrB9RT4je8eV5b19RZ7hsfL4QEWWLsuXK8WTJke5ETOF3FoT2+DfOOlabmJhLo9FsvRQi7n0Ap4Kt\nMNvS2VNEZorI2yKSNcm2iJwjItNEZFpNTU22Lu3jKGZBPAJf/BsmXE3STNYliSi01MLdO8DctyCa\n37u/lcUAABIWSURBVG3Coo8A6FE/nTYV4M+xkzNyhJfSxuJV7vEmy/sT9+SPDJmR3AZ/3HDneP2B\nlOUeVV528BhhlrmyKFpYou7Mg+4tcNEylUNGa7tG85OjoxZUvwL6K6VGA/cCr2brpJR6UCk1Rik1\nprq6OluXdkkmHUqViMLbV8Dn96WseE8yCmu/gQ1L4LmT895raW0L//74u9TrKD4eThxGPe5Y7RLa\n8CTcedqLum9D3OsW90ZVzNjI31KFnZ0VkbyBIFEMoV4cGplq91n5z497HH7x51T7eo+Rk91juWX8\nTp/7xlnuSS3uGs1PjkLEfSW43NF9zbYUSqlGpVSzeTwe8IuIu2JEB9Ey6CA717hjw5LTcr/hpWnZ\nLs3gwqe/ZmW9HToZTS1BuHdM3uJ/lBJHEWiAQ8cMI+Fzi/tnye1YqPrCbyfQvNM51Im9wOnzB0l4\nDMtdPHZedr/ftMK3Owr2vCj1/Jg5FntB1WG5+wtLkWul9tVZFzWanx6FiPtUYKiIDBKRAHAC4Ers\nIiI9xfQBiMhu5n1zh4B8D5rCcVuEHYunyoxlH+5Zzg0tN7uuOTn6J95M7JFxL5WIuWTcEtRJ3t1c\n/YZ7lnNk5A3iysMfYudwPlfSozxEJM0tkxpXr9GUjrud/j3t+c0XCLLQPwyANp+dgtcfSHOxXD4H\nrliEMhdDPWZeGK/Dci/ULTNy9K4AjBoxoqD+Go2m89ButIxSKi4iFwITMPZdPqqUmi0i55rnHwCO\nBc4TkTjQBpygNlOIRmM4RtRamHSEPapELMcV8FVyKJ8mR3FL7GQ+D12Uah/JIm70P556nRQfS247\njDUNB/LHNw8mXLuMu+vOA6CYMA0U80JifwZUGm6XQJe+sAFagj0o2e8irnvd7WryOlLteoOlPFVx\nDs+u3JlTu61mh6aPAPClL46WG4m9rN2llrhbJekAvAW6Zfz7XAL9d8UzaJ+C+ms0ms5DQekHTFfL\n+LS2BxzH9wH3dezQstPYFidiDlvF21KWt8pT2MIqGL0Od9GKqxvdm3Hi5n17VoT468l7U1dbY6wg\nmLSaCQi6FBviuuvOO8MiwyfPnhdR//pbrvv5HGXp/EWlXH3EKC59LkF1ZROYwTcZlruJlS/G8rl7\nHWl7vYECfe4eL2hh12h+kmx1O1SbwjGiZtSJq15p3qpFxhSQwF2DtCTpzugYF7cvu7LCXTC6TRmi\n2sUMqvdWDTRO5NhY5XNkSPR7vewyoIpPrvg5QYdQ+/z5XSyWO0bK7R2u/nau0Wg0mq0ucZiI7duO\nRyNYNqxKGD73aclhjPHMd13z2Bm7snBdM9VlQVccjw/3hJAU98dhlYGzsL4BdCkxn1o5wHX+7H0G\nsXN/e0IIFttFLyqLnekD7GNfDss93S3j83k5PnItB3unMc671c3JGo1mC7PVifvPh/cgvOcwmAaJ\nNueGJsPnfnz0OhaFTnFds+/QavbftjsAba8EKJLseWQSkv/jsMR9r8HmQqlVSWjvywG4+rCRrv6h\nElvcA46i1OKxvyEEc7pl3IU2Aj4PU9QIpsRHcGyOdLsajUZjsVWqhLW4mGi2A3LErJSUJDPsz+Ox\n246O3sSLiSxb9SEVqujitDdZULSD8YxAMe//bj+O2cV0kYgY+VTGXp/1fkUl5VnbnbtN/Tl2myrr\nV2P63LuX2ZOAT4c2ajSadtgqxd1rWruq1Rb3ZLSVmPICwqnRKzk/enHWa2897wQer7ok6zmVzXIf\ntA+ry4zsi3FPiMHVhRejKMkh7s5NSN4cLhZl7i5V5oTjd/TzerS4azSa/GyV4m5Z7nGH5a5irSTM\nt/NJcjQfJnfMeu3O/bvwwoUH0KIy3SHJbJY7kCgyNiMFPMms53NRWm4Ux0hfyC1kh2nKcs/yTcSv\nfe4ajaYdtkqVsGLDEy2OfVKxMHG8fHvTwRy7S19O2dvIbZ4sztwoG/J7afYawvtmck8e40ijbw5x\nx7xHiBw533MQKDJ87h7vpoi7dWBPKFYlJW25azSa9tjqFlQBPObmINVsJ/OSeBsJvJQGjKpDAAx9\nDk+PkdluQSxQCeF1lFd0IeLrDhtwVVNyPa/EEveNLOFnFq4wsibbeAvI6lgSDEAcqortX9Ez5+zB\n8roslaY0Go0mja1T3EOGL9vfvMpui4dTG39SbHtIznvEgl0gDBFfKbGAYREXqXDWvr4yY+dpSG2k\nuFu1Tge6qy0VYrn3rCyGFujhWEgtDfoY0Su7H1+j0WicbJVuGX8gRFx5KG1bkWrzJcIpn3shhEuN\nXGjNlJAIGjtXQ6ota99QmRHyGNhYcfeH4JyP4bjHXM2F5IaRcffDyKOg1w4b90yNRqNhK7XcA34v\nLYSoiNtx7r5kmESa+yMfi3b8A88vKWJt4ECO6t4Cy8AXb8naN9TFSF8/s8fRHLCxg+2dubBbiFuG\n7sMdBbA1Go1m49g6LXev0ELI3ZaMZESl5KNLl278J3Eoy+IVDO7fHwBPLLs/u6y8kkHhJ5k54PRN\nHrOTgsRdo9FovgdbpbgHfR5alDvdbkBtnLh3M33ZzeE4A/sZlnmlL3s0TGWJH4/HS2VxYXnU20OL\nu0aj2dxsnW4Zr5c63H7rIonSJvlL1jmpLLKF2ltsxrHncMuUh/w8/38/Y3jPsqznNxa/P9R+J41G\no/kebJXiXhz0sty03GuopJp6QkRoUoWLb3VZkEvHDuXw0b3sqJbdzs7Zf5cBXXKe21i05a7RaDY3\nW6W4dykOpHKrr/N0pzpZT7m0sUoV7mUSES4dO8xuuHY9eLbMx5FRoEOj0Wg6mK3S5+71COU+Iwvk\nSr+ddvf/27vXGKnKO47j319XWAVMvbAaAyhQSSxt0dIVjdJW2mjBtqKpRrxQbU2ISTW1faFYW9tq\n39gX1Tc0SFpSGrH0olRibbxQozXWyqqIoKCIJi6xrlV7ofEG/vviPEuP28E57M7OMM/+Pslmz3nO\nw8zzn8AvhzNz/nMAtT+nXu1BRxWNwJpgv9G+LGNmw6stwx3gCL0BwCv7T909NmYo4d5Ee+oEaWbW\nKJXCXdJcSVskbZW0+APmHS9pp6SzG7fE2g6N1wF4fdy03WPtEu4d/iYlMxtmdcNdRWOUJcA8YDpw\nnqT/a9iS5t0A3NPoRdZyx8EXAxAHT9k9NlZ7eQdpq+ypQZmZWYNUOXOfBWyNiG0R8Q6wCphfY97l\nwG1AXwPXt0d/6TqHyW/dyhkzJzfj6RrL36RkZsOsysdDJgAvlfZ7gRPKEyRNAM4C5gDH7+mBJC0C\nFgEcme4KHawffHk65x0/iamHRf3JZmYjTKNOIW8CroqID/w2i4hYFhHdEdHd1dU1pCc8dFwnJx09\nHsZ18eaJ3xrSY5mZ5abKmft2YFJpf2IaK+sGVqn4KOF44HRJOyPi9w1ZZR2a8x145MZmPFXjnPYj\nmNDd6lWYWaaqhPs6YJqkKRShvgA4vzwhIna/qynpF8CdzQp2gM7RbfgG5UmXt3oFZpaxuuEeETsl\nXQbcDXQAyyNik6RL0/Glw7zGutSkm4/MzNpFpfvtI+Iu4K4BYzVDPSIuHvqy9t517y6kN8azrBVP\nbma2j2nL3jK1LN81j879/BFDMzPIKNz/fOUcxoyu3s/dzCxn2YT7pEPGtHoJZmb7DF/HMDPLkMPd\nzCxDDnczsww53M3MMuRwNzPLkMPdzCxDDnczsww53M3MMuRwNzPLkMPdzCxDDnczsww53M3MMuRw\nNzPLkMPdzCxDDnczswxVCndJcyVtkbRV0uIax+dL2iBpvaQeSbMbv1QzM6uq7pd1SOoAlgCnAr3A\nOklrIuLp0rS1wJqICEkzgN8AxwzHgs3MrL4qZ+6zgK0RsS0i3gFWAfPLEyJiR0RE2h0LBGZm1jJV\nwn0C8FJpvzeNvY+ksyRtBv4AfL0xyzMzs8Fo2BuqEbE6Io4BzgSurzVH0qJ0Tb7n1VdfbdRTm5nZ\nAFXCfTswqbQ/MY3VFBEPAlMlja9xbFlEdEdEd1dX114v1szMqqkS7uuAaZKmSBoNLADWlCdIOlqS\n0vZMoBN4rdGLNTOzaup+WiYidkq6DLgb6ACWR8QmSZem40uBrwBflfQu8CZwbukNVjMzazK1KoO7\nu7ujp6enJc9tZtauJD0WEd315vkOVTOzDDnczcwy5HA3M8uQw93MLEMOdzOzDDnczcwy5HA3M8uQ\nw93MLEMOdzOzDDnczcwy5HA3M8uQw93MLEMOdzOzDDnczcwy5HA3M8uQw93MLEMOdzOzDDnczcwy\n5HA3M8tQpXCXNFfSFklbJS2ucfwCSRskPSXpYUnHNn6pZmZWVd1wl9QBLAHmAdOB8yRNHzDtBeCz\nEfEJ4HpgWaMXamZm1VU5c58FbI2IbRHxDrAKmF+eEBEPR8QbafcRYGJjl2lmZntjvwpzJgAvlfZ7\ngRM+YP4lwB9rHZC0CFiUdndI2lJlkTWMB/4+yD/brlzzyOCaR4ah1HxUlUlVwr0ySXMown12reMR\nsYwGXLKR1BMR3UN9nHbimkcG1zwyNKPmKuG+HZhU2p+Yxt5H0gzgZ8C8iHitMcszM7PBqHLNfR0w\nTdIUSaOBBcCa8gRJRwK3Awsj4tnGL9PMzPZG3TP3iNgp6TLgbqADWB4RmyRdmo4vBa4FDgV+Kglg\n5zD/l2MkfhrHNY8MrnlkGPaaFRHD/RxmZtZkvkPVzCxDDnczswy1XbjXa4XQriQtl9QnaWNp7BBJ\n90p6Lv0+uHTs6vQabJH0hdasemgkTZJ0v6SnJW2S9M00nm3dkvaX9KikJ1PNP0zj2dYMxZ3ukp6Q\ndGfaz7peAEkvppYs6yX1pLHm1R0RbfND8Ybu88BUYDTwJDC91etqUG2fAWYCG0tjPwYWp+3FwA1p\ne3qqvROYkl6TjlbXMIiajwBmpu0DgWdTbdnWDQgYl7ZHAX8FTsy55lTHt4FbgTvTftb1plpeBMYP\nGGta3e125l63FUK7iogHgdcHDM8HVqTtFcCZpfFVEfF2RLwAbKV4bdpKRLwcEY+n7X8Dz1DcEZ1t\n3VHYkXZHpZ8g45olTQS+SHEfTL9s662jaXW3W7jXaoUwoUVraYbDI+LltP034PC0nd3rIGky8EmK\nM9ms606XKNYDfcC9EZF7zTcBVwLvlcZyrrdfAPdJeiy1XoEm1t3Q9gM2fCIiJGX5uVVJ44DbgCsi\n4l/pXgkgz7ojYhdwnKSDgNWSPj7geDY1S/oS0BcRj0k6pdacnOodYHZEbJd0GHCvpM3lg8Ndd7ud\nuVdqhZCRVyQdAZB+96XxbF4HSaMogn1lRNyehrOvGyAi/gHcD8wl35pPBs6Q9CLFZdTPSbqFfOvd\nLSK2p999wGqKyyxNq7vdwr1uK4TMrAEuStsXAXeUxhdI6pQ0BZgGPNqC9Q2JilP0nwPPRMRPSoey\nrVtSVzpjR9IBwKnAZjKtOSKujoiJETGZ4t/rnyLiQjKtt5+ksZIO7N8GTgM20sy6W/2O8iDegT6d\n4lMVzwPXtHo9DazrV8DLwLsU19suoWjpsBZ4DrgPOKQ0/5r0GmyhaNbW8hoGUfNsiuuSG4D16ef0\nnOsGZgBPpJo3Atem8WxrLtVxCv/7tEzW9VJ8ou/J9LOpP6uaWbfbD5iZZajdLsuYmVkFDnczsww5\n3M3MMuRwNzPLkMPdzCxDDnezQZB0Sn+HQ7N9kcPdzCxDDnfLmqQLU//09ZJuTk27dki6MfVTXyup\nK809TtIjkjZIWt3fa1vS0ZLuSz3YH5f0kfTw4yT9TtJmSStVbopj1mIOd8uWpI8C5wInR8RxwC7g\nAmAs0BMRHwMeAL6f/sgvgasiYgbwVGl8JbAkIo4FTqK4kxiKLpZXUPTinkrRR8Vsn+CukJazzwOf\nAtalk+oDKBo1vQf8Os25Bbhd0oeBgyLigTS+Avht6g8yISJWA0TEWwDp8R6NiN60vx6YDDw0/GWZ\n1edwt5wJWBERV79vUPregHmD7cHxdml7F/73ZPsQX5axnK0Fzk79tPu/v/Ioir/3Z6c55wMPRcQ/\ngTckfTqNLwQeiOIbonolnZkeo1PSmKZWYTYIPtOwbEXE05K+C9wj6UMUHTe/AfwHmJWO9VFcl4ei\nBevSFN7bgK+l8YXAzZKuS49xThPLMBsUd4W0EUfSjogY1+p1mA0nX5YxM8uQz9zNzDLkM3czsww5\n3M3MMuRwNzPLkMPdzCxDDnczswz9F3D+i4Qy/a+kAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f8f40f24ac8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(train_accuracies, label=\"train\")\n",
    "plt.plot(validation_accuracies, label=\"validation\")\n",
    "plt.title(\"train & validation accuracy\")\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.ylim([0.2,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./tmp/one_layer/model_image64_adam_reg01.ckpt\n",
      "Model restored.\n",
      "final test accuracy =  0.712739641311\n"
     ]
    }
   ],
   "source": [
    "def test_accuracy(model_path):\n",
    "    with tf.Session() as sess:\n",
    "        # Restore variables from disk.\n",
    "        saver.restore(sess, model_path)\n",
    "        print(\"Model restored.\")\n",
    "        N = len(test_data)    \n",
    "        batch_size = 128\n",
    "        batch_number = int(N/batch_size) + 1\n",
    "        sum_correct = 0.\n",
    "        for i in range(batch_number):\n",
    "            batch_X, batch_Y = get_batch(test_data, test_label_onehot, i)\n",
    "            n = batch_X.shape[0]\n",
    "            a = sess.run(accuracy, feed_dict={X: batch_X, Y_: batch_Y})\n",
    "            sum_correct += a*n\n",
    "        \n",
    "        test_accuracy = sum_correct/N\n",
    "        print(\"final test accuracy = \", test_accuracy)\n",
    "        \n",
    "test_accuracy(\"./tmp/one_layer/model_image64_adam_reg01.ckpt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Result\n",
    "- 1 layer\n",
    "- regularlization = 0.5\n",
    "- batch 128\n",
    "- 500 epoches\n",
    "\n",
    "### Test accuracy = 0.71"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
